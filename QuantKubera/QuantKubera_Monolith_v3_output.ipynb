{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a4044cf",
   "metadata": {
    "papermill": {
     "duration": 0.006966,
     "end_time": "2026-02-14T16:36:14.210566",
     "exception": false,
     "start_time": "2026-02-14T16:36:14.203600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# QuantKubera Monolith v3 — Magnum Opus\n",
    "\n",
    "## Institutional-Grade Momentum Transformer for Indian Derivatives\n",
    "\n",
    "**Self-contained** | **Zero Look-Ahead Bias** | **Walk-Forward OOS Validation**\n",
    "**World Monitor Intelligence** | **Cutting-Edge ML** | **RL Meta-Learning**\n",
    "\n",
    "| Component | Reference | Features |\n",
    "|-----------|-----------|----------|\n",
    "| Temporal Fusion Transformer | Lim et al. (2021) | VSN, Interpretable MHA, GRN |\n",
    "| EMAT Attention | Entropy 2025 | Temporal Decay + Trend + Volatility heads |\n",
    "| AFML Event Pipeline | Lopez de Prado (2018) | CUSUM, Triple Barrier, Meta-Labeling |\n",
    "| Fractional Differentiation | Hosking (1981) | Memory-preserving stationarity |\n",
    "| Ramanujan Sum Filter Bank | Planat (2002) | Integer-period cycle detection |\n",
    "| NIG Changepoint Detection | Adams & MacKay (2007) | Regime shift scoring |\n",
    "| Market Microstructure | Easley et al. (2012), Kyle (1985) | VPIN, Lambda, Amihud |\n",
    "| Wavelet Decomposition | Daubechies (1992) | Multi-resolution trend/noise separation |\n",
    "| Hidden Markov Model | Baum-Welch (1970) | 3-state regime detection |\n",
    "| Persistent Homology (TDA) | Edelsbrunner (2000) | Topological crash detection |\n",
    "| Transfer Entropy | Schreiber (2000) | Cross-asset causal information flow |\n",
    "| Multifractal DFA | Kantelhardt (2002) | Fractal spectrum width |\n",
    "| KL Divergence Regime | Kullback-Leibler (1951) | Distribution shift detection |\n",
    "| World Monitor Signals | koala73 (2025) | Macro regime, CII, anomaly detection |\n",
    "| Thompson Sampling | Thompson (1933) | RL strategy selection |\n",
    "| Sharpe+DD Loss | Multi-objective | Sharpe + drawdown penalty |\n",
    "\n",
    "### Pipeline\n",
    "```\n",
    "Zerodha Kite API → 31 Base Features (10 groups) → Variable Selection Network\n",
    "GDELT News → FinBERT → 9 Sentiment Features ↗\n",
    "India VIX → 3 VIX Features ↗\n",
    "Yahoo Finance → 5 Macro Regime Features ↗     (World Monitor)\n",
    "Welford Anomaly → 4 Anomaly Features ↗         (World Monitor)\n",
    "HMM Regime → 3 Regime Features ↗               (NEW)\n",
    "Wavelet DWT → 4 Wavelet Features ↗             (NEW)\n",
    "Info Theory → 4 Entropy Features ↗              (NEW)\n",
    "Multifractal → 3 MF-DFA Features ↗             (NEW)\n",
    "TDA → 2 Topological Features ↗                  (NEW)\n",
    "→ EMAT (3-Head Financial Attention) → Momentum Signal\n",
    "→ Walk-Forward OOS (purge gaps) → Meta-Labeling → Probit Bet Sizing\n",
    "→ RL Thompson Sampling Meta-Learner → Regime-Aware Ensemble\n",
    "→ VectorBTPro Tearsheet\n",
    "```\n",
    "\n",
    "### Feature Groups (31 per-ticker + 37 cross-asset & advanced = 68 total)\n",
    "**Base (31):**\n",
    "1. Normalized Returns (5) | 2. MACD (3) | 3. Volatility (4)\n",
    "4. Changepoint Detection (2) | 5. Fractional Calculus (3) | 6. Ramanujan (4)\n",
    "7. Microstructure (4) | 8. Entropy (1) | 9. Momentum Quality (3) | 10. Volume (2)\n",
    "\n",
    "**Cross-Asset (12):**\n",
    "11. News Sentiment (9) | 12. India VIX (3)\n",
    "\n",
    "**World Monitor Intelligence (9):**\n",
    "13. Macro Regime (5) | 14. Welford Anomaly Detection (4)\n",
    "\n",
    "**Cutting-Edge ML (16):**\n",
    "15. HMM Regime (3) | 16. Wavelet Decomposition (4)\n",
    "17. Information Theory Advanced (4) | 18. Multifractal DFA (3) | 19. TDA Persistent Homology (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44735d20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:36:14.222985Z",
     "iopub.status.busy": "2026-02-14T16:36:14.222782Z",
     "iopub.status.idle": "2026-02-14T16:36:18.753454Z",
     "shell.execute_reply": "2026-02-14T16:36:18.752610Z"
    },
    "papermill": {
     "duration": 4.537792,
     "end_time": "2026-02-14T16:36:18.754027",
     "exception": false,
     "start_time": "2026-02-14T16:36:14.216235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-14 16:36:14.821409: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-14 16:36:14.888459: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-14 16:36:16.282898: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "QuantKubera Monolith v3 — Magnum Opus\n",
      "======================================================================\n",
      "  GPU Available : True (/device:GPU:0)\n",
      "  TensorFlow    : 2.20.0\n",
      "  NumPy         : 1.26.4\n",
      "  Pandas        : 3.0.0\n",
      "  Seed          : 42\n",
      "  Quick Mode    : False\n",
      "  Tickers       : ['NIFTY', 'BANKNIFTY', 'FINNIFTY', 'GOLD', 'SILVER', 'CRUDEOIL', 'NATURALGAS', 'COPPER', 'RELIANCE', 'HDFCBANK', 'ICICIBANK', 'INFY', 'TCS', 'SBIN']\n",
      "  Hidden Size   : 128\n",
      "  Num Heads     : 4\n",
      "  Epochs        : 50\n",
      "  Batch Size    : 32\n",
      "  Walk-forward  : train>=504d, test=63d, purge=5d\n",
      "  Cost Model    : 10 bps per side\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1771086978.747460 3754101 gpu_device.cc:2020] Created device /device:GPU:0 with 13775 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Setup & Configuration\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.special import gammaln, gamma as gamma_fn\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from kiteconnect import KiteConnect\n",
    "from dotenv import load_dotenv\n",
    "import pyotp\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "# --- v3 additional imports ---\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    HAS_YFINANCE = True\n",
    "except ImportError:\n",
    "    HAS_YFINANCE = False\n",
    "\n",
    "try:\n",
    "    from fredapi import Fred\n",
    "    HAS_FREDAPI = True\n",
    "except ImportError:\n",
    "    HAS_FREDAPI = False\n",
    "\n",
    "try:\n",
    "    from hmmlearn.hmm import GaussianHMM\n",
    "    HAS_HMM = True\n",
    "except ImportError:\n",
    "    HAS_HMM = False\n",
    "\n",
    "try:\n",
    "    import pywt\n",
    "    HAS_WAVELET = True\n",
    "except ImportError:\n",
    "    HAS_WAVELET = False\n",
    "\n",
    "try:\n",
    "    from gtda.homology import VietorisRipsPersistence\n",
    "    from gtda.diagrams import PersistenceEntropy, Amplitude\n",
    "    HAS_TDA = True\n",
    "except ImportError:\n",
    "    HAS_TDA = False\n",
    "\n",
    "from scipy.stats import entropy as sp_entropy\n",
    "from sklearn.neighbors import KDTree\n",
    "from scipy.special import digamma\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Suppress warnings\n",
    "# ---------------------------------------------------------------------------\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "# Suppress Keras masking warnings — we use fixed-length sequences (window_size=21),\n",
    "# no padding, no variable-length inputs. Masking is irrelevant.\n",
    "warnings.filterwarnings('ignore', message='.*does not support masking.*')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "logger = logging.getLogger('QuantKubera')\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# GPU detection & memory growth\n",
    "# ---------------------------------------------------------------------------\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            logger.warning(f\"GPU memory growth setting failed: {e}\")\n",
    "    GPU_AVAILABLE = True\n",
    "    GPU_NAME = tf.test.gpu_device_name() or gpus[0].name\n",
    "else:\n",
    "    GPU_AVAILABLE = False\n",
    "    GPU_NAME = \"None\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Seed everything\n",
    "# ---------------------------------------------------------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class MonolithConfig:\n",
    "    # Data\n",
    "    tickers: list = field(default_factory=lambda: [\n",
    "        # NSE Index Futures\n",
    "        'NIFTY', 'BANKNIFTY', 'FINNIFTY',\n",
    "        # MCX Commodity Futures\n",
    "        'GOLD', 'SILVER', 'CRUDEOIL', 'NATURALGAS', 'COPPER',\n",
    "        # NSE Stock Futures (top FnO by volume)\n",
    "        'RELIANCE', 'HDFCBANK', 'ICICIBANK', 'INFY', 'TCS', 'SBIN',\n",
    "    ])\n",
    "    exchanges: dict = field(default_factory=lambda: {\n",
    "        'NIFTY': 'NSE', 'BANKNIFTY': 'NSE', 'FINNIFTY': 'NSE',\n",
    "        'GOLD': 'MCX', 'SILVER': 'MCX', 'CRUDEOIL': 'MCX',\n",
    "        'NATURALGAS': 'MCX', 'COPPER': 'MCX',\n",
    "        'RELIANCE': 'NFO', 'HDFCBANK': 'NFO', 'ICICIBANK': 'NFO',\n",
    "        'INFY': 'NFO', 'TCS': 'NFO', 'SBIN': 'NFO',\n",
    "    })\n",
    "    lookback_days: int = 2500\n",
    "    window_size: int = 21\n",
    "    # Model\n",
    "    hidden_size: int = 128\n",
    "    num_heads: int = 4\n",
    "    dropout_rate: float = 0.2\n",
    "    # Training\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 50\n",
    "    learning_rate: float = 1e-3\n",
    "    early_stop_patience: int = 10\n",
    "    lr_reduce_patience: int = 5\n",
    "    lr_reduce_factor: float = 0.5\n",
    "    min_lr: float = 1e-6\n",
    "    clipnorm: float = 1.0\n",
    "    # Walk-forward\n",
    "    min_train_days: int = 504   # 2 years minimum\n",
    "    test_days: int = 63         # 3 months\n",
    "    purge_gap: int = 5\n",
    "    # Costs\n",
    "    bps_cost: float = 0.0010   # 10 bps per side\n",
    "    # Quick mode\n",
    "    quick_mode: bool = False    # True: 1 ticker, fewer epochs; False: full universe\n",
    "\n",
    "CFG = MonolithConfig()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Version tag — verify after Kernel Restart that you see this version\n",
    "# ---------------------------------------------------------------------------\n",
    "NOTEBOOK_VERSION = \"v3.0-2026-02-14\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Summary\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(f\"QuantKubera Monolith v3 — Magnum Opus\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  GPU Available : {GPU_AVAILABLE} ({GPU_NAME})\")\n",
    "print(f\"  TensorFlow    : {tf.__version__}\")\n",
    "print(f\"  NumPy         : {np.__version__}\")\n",
    "print(f\"  Pandas        : {pd.__version__}\")\n",
    "print(f\"  Seed          : {SEED}\")\n",
    "print(f\"  Quick Mode    : {CFG.quick_mode}\")\n",
    "print(f\"  Tickers       : {CFG.tickers}\")\n",
    "print(f\"  Hidden Size   : {CFG.hidden_size}\")\n",
    "print(f\"  Num Heads     : {CFG.num_heads}\")\n",
    "print(f\"  Epochs        : {CFG.epochs}\")\n",
    "print(f\"  Batch Size    : {CFG.batch_size}\")\n",
    "print(f\"  Walk-forward  : train>={CFG.min_train_days}d, test={CFG.test_days}d, purge={CFG.purge_gap}d\")\n",
    "print(f\"  Cost Model    : {CFG.bps_cost * 10000:.0f} bps per side\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c265875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:36:18.767227Z",
     "iopub.status.busy": "2026-02-14T16:36:18.766833Z",
     "iopub.status.idle": "2026-02-14T16:36:18.793403Z",
     "shell.execute_reply": "2026-02-14T16:36:18.792702Z"
    },
    "papermill": {
     "duration": 0.033863,
     "end_time": "2026-02-14T16:36:18.793985",
     "exception": false,
     "start_time": "2026-02-14T16:36:18.760122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Data Engine — KiteAuth + KiteFetcher\n",
    "# ============================================================================\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class KiteAuth:\n",
    "    \"\"\"TOTP-based Zerodha authentication with token caching.\"\"\"\n",
    "\n",
    "    TOKEN_PATH = Path.home() / '.zerodha_access_token'\n",
    "    LOGIN_URL = 'https://kite.zerodha.com/api/login'\n",
    "    TWOFA_URL = 'https://kite.zerodha.com/api/twofa'\n",
    "    CONNECT_URL = 'https://kite.zerodha.com/connect/login'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.api_key = os.getenv('ZERODHA_API_KEY', '')\n",
    "        self.api_secret = os.getenv('ZERODHA_API_SECRET', '')\n",
    "        self.totp_secret = os.getenv('ZERODHA_TOTP_SECRET', '')\n",
    "        self.user_id = os.getenv('ZERODHA_USER_ID', '')\n",
    "        self.password = os.getenv('ZERODHA_PASSWORD', '')\n",
    "\n",
    "    def _load_cached_token(self) -> Optional[str]:\n",
    "        \"\"\"Load cached access token if it exists and was created today.\"\"\"\n",
    "        if not self.TOKEN_PATH.exists():\n",
    "            return None\n",
    "        stat = self.TOKEN_PATH.stat()\n",
    "        modified = datetime.fromtimestamp(stat.st_mtime).date()\n",
    "        if modified != datetime.now().date():\n",
    "            return None\n",
    "        try:\n",
    "            token_data = json.loads(self.TOKEN_PATH.read_text())\n",
    "            return token_data.get('access_token')\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            return None\n",
    "\n",
    "    def _save_token(self, access_token: str):\n",
    "        \"\"\"Cache the access token to disk.\"\"\"\n",
    "        self.TOKEN_PATH.write_text(json.dumps({\n",
    "            'access_token': access_token,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'user_id': self.user_id,\n",
    "        }))\n",
    "\n",
    "    def _auto_login(self) -> str:\n",
    "        \"\"\"Perform full auto-login flow and return access token.\"\"\"\n",
    "        session = requests.Session()\n",
    "\n",
    "        # Step 1: POST /api/login\n",
    "        logger.info(\"KiteAuth: Step 1 — login\")\n",
    "        resp = session.post(self.LOGIN_URL, data={\n",
    "            'user_id': self.user_id,\n",
    "            'password': self.password,\n",
    "        })\n",
    "        resp.raise_for_status()\n",
    "        login_data = resp.json()\n",
    "        if login_data.get('status') != 'success':\n",
    "            raise RuntimeError(f\"Login failed: {login_data}\")\n",
    "        request_id = login_data['data']['request_id']\n",
    "\n",
    "        # Step 2: POST /api/twofa with TOTP\n",
    "        logger.info(\"KiteAuth: Step 2 — TOTP 2FA\")\n",
    "        totp = pyotp.TOTP(self.totp_secret)\n",
    "        twofa_value = totp.now()\n",
    "        resp = session.post(self.TWOFA_URL, data={\n",
    "            'user_id': self.user_id,\n",
    "            'request_id': request_id,\n",
    "            'twofa_value': twofa_value,\n",
    "            'twofa_type': 'totp',\n",
    "        })\n",
    "        resp.raise_for_status()\n",
    "        twofa_data = resp.json()\n",
    "        if twofa_data.get('status') != 'success':\n",
    "            raise RuntimeError(f\"2FA failed: {twofa_data}\")\n",
    "\n",
    "        # Step 3: GET /connect/login to extract request_token\n",
    "        # Follow the full redirect chain — request_token appears in the\n",
    "        # final redirect to the registered callback URL.\n",
    "        logger.info(\"KiteAuth: Step 3 — extract request_token\")\n",
    "        from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "        resp = session.get(self.CONNECT_URL, params={\n",
    "            'api_key': self.api_key,\n",
    "            'v': '3',\n",
    "        }, allow_redirects=True)\n",
    "\n",
    "        # Search for request_token in the final URL and all redirect history\n",
    "        candidate_urls = [resp.url] + [r.headers.get('Location', '') for r in resp.history]\n",
    "        request_token = None\n",
    "        for url in candidate_urls:\n",
    "            parsed = urlparse(url)\n",
    "            params = parse_qs(parsed.query)\n",
    "            if 'request_token' in params:\n",
    "                request_token = params['request_token'][0]\n",
    "                break\n",
    "\n",
    "        if request_token is None:\n",
    "            raise RuntimeError(\n",
    "                f\"Could not extract request_token. \"\n",
    "                f\"Final URL: {resp.url}, \"\n",
    "                f\"History: {[r.url for r in resp.history]}\"\n",
    "            )\n",
    "\n",
    "        # Step 4: Generate session\n",
    "        logger.info(\"KiteAuth: Step 4 — generate session\")\n",
    "        kite = KiteConnect(api_key=self.api_key)\n",
    "        data = kite.generate_session(request_token, api_secret=self.api_secret)\n",
    "        access_token = data['access_token']\n",
    "\n",
    "        self._save_token(access_token)\n",
    "        logger.info(\"KiteAuth: login complete, token cached\")\n",
    "        return access_token\n",
    "\n",
    "    def get_session(self) -> KiteConnect:\n",
    "        \"\"\"Return an authenticated KiteConnect instance.\"\"\"\n",
    "        # Try cached token first\n",
    "        cached = self._load_cached_token()\n",
    "        if cached:\n",
    "            kite = KiteConnect(api_key=self.api_key)\n",
    "            kite.set_access_token(cached)\n",
    "            try:\n",
    "                kite.profile()\n",
    "                logger.info(\"KiteAuth: using cached token\")\n",
    "                return kite\n",
    "            except Exception:\n",
    "                logger.info(\"KiteAuth: cached token expired, re-logging in\")\n",
    "\n",
    "        # Full login\n",
    "        access_token = self._auto_login()\n",
    "        kite = KiteConnect(api_key=self.api_key)\n",
    "        kite.set_access_token(access_token)\n",
    "        return kite\n",
    "\n",
    "\n",
    "class KiteFetcher:\n",
    "    \"\"\"Fetches daily OHLCV data from Zerodha Kite.\"\"\"\n",
    "\n",
    "    MAX_CHUNK_DAYS = 1900  # API limit is 2000, use 1900 for safety\n",
    "\n",
    "    def __init__(self, kite: KiteConnect):\n",
    "        self.kite = kite\n",
    "        self._instruments_cache: Optional[pd.DataFrame] = None\n",
    "\n",
    "    def _get_instruments(self, exchange: str) -> pd.DataFrame:\n",
    "        \"\"\"Fetch and cache instruments list for the given exchange.\"\"\"\n",
    "        if self._instruments_cache is None:\n",
    "            all_instruments = self.kite.instruments()\n",
    "            self._instruments_cache = pd.DataFrame(all_instruments)\n",
    "        return self._instruments_cache[\n",
    "            self._instruments_cache['exchange'] == exchange\n",
    "        ].copy()\n",
    "\n",
    "    def _resolve_instrument(self, symbol: str, exchange: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Resolve symbol to instrument_token.\n",
    "        Try exact match first, then fuzzy match for derivatives (FUT, nearest expiry).\n",
    "        Returns (instrument_token, resolved_tradingsymbol).\n",
    "        \"\"\"\n",
    "        instruments = self._get_instruments(exchange)\n",
    "\n",
    "        # Exact match on tradingsymbol\n",
    "        exact = instruments[instruments['tradingsymbol'] == symbol]\n",
    "        if len(exact) > 0:\n",
    "            row = exact.iloc[0]\n",
    "            return str(row['instrument_token']), row['tradingsymbol']\n",
    "\n",
    "        # Try NFO/MCX-FUT: look for symbol + FUT with nearest expiry\n",
    "        # For index derivatives, check NFO exchange\n",
    "        fut_exchange = exchange\n",
    "        if exchange == 'NSE':\n",
    "            fut_exchange = 'NFO'\n",
    "            instruments = self._get_instruments(fut_exchange)\n",
    "        elif exchange == 'MCX':\n",
    "            # MCX futures are on MCX itself\n",
    "            pass\n",
    "\n",
    "        # Fuzzy match: tradingsymbol starts with symbol, segment contains FUT\n",
    "        fuzzy = instruments[\n",
    "            instruments['tradingsymbol'].str.startswith(symbol) &\n",
    "            (instruments['instrument_type'] == 'FUT')\n",
    "        ].copy()\n",
    "\n",
    "        if len(fuzzy) == 0:\n",
    "            # Try with exchange-specific naming\n",
    "            fuzzy = instruments[\n",
    "                instruments['tradingsymbol'].str.contains(symbol, case=False, na=False) &\n",
    "                (instruments['instrument_type'] == 'FUT')\n",
    "            ].copy()\n",
    "\n",
    "        if len(fuzzy) == 0:\n",
    "            raise ValueError(\n",
    "                f\"Could not resolve instrument: {symbol} on {exchange}/{fut_exchange}. \"\n",
    "                f\"Check symbol name and exchange.\"\n",
    "            )\n",
    "\n",
    "        # Pick the nearest expiry\n",
    "        if 'expiry' in fuzzy.columns:\n",
    "            fuzzy['expiry'] = pd.to_datetime(fuzzy['expiry'], errors='coerce')\n",
    "            fuzzy = fuzzy.dropna(subset=['expiry'])\n",
    "            fuzzy = fuzzy.sort_values('expiry')\n",
    "            # Pick the nearest future expiry (>= today)\n",
    "            today = pd.Timestamp.now().normalize()\n",
    "            future_expiries = fuzzy[fuzzy['expiry'] >= today]\n",
    "            if len(future_expiries) > 0:\n",
    "                row = future_expiries.iloc[0]\n",
    "            else:\n",
    "                row = fuzzy.iloc[-1]  # Fallback to latest\n",
    "        else:\n",
    "            row = fuzzy.iloc[0]\n",
    "\n",
    "        resolved = row['tradingsymbol']\n",
    "        token = str(row['instrument_token'])\n",
    "        print(f\"  Resolved {symbol} -> {resolved} (expiry: {row.get('expiry', 'N/A')})\")\n",
    "        return token, resolved\n",
    "\n",
    "    def fetch_daily(self, symbol: str, exchange: str, days: int = 2500) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch daily OHLCV data for the given symbol.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        symbol : str\n",
    "            Trading symbol (e.g., 'NIFTY', 'BANKNIFTY', 'SILVER')\n",
    "        exchange : str\n",
    "            Exchange (e.g., 'NSE', 'MCX')\n",
    "        days : int\n",
    "            Number of calendar days to look back\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with index=date (tz-naive), columns: open, high, low, close, volume\n",
    "        \"\"\"\n",
    "        instrument_token, resolved_name = self._resolve_instrument(symbol, exchange)\n",
    "\n",
    "        end_date = datetime.now().date()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "\n",
    "        print(f\"  Fetching {resolved_name}: {start_date} to {end_date} ({days} calendar days)\")\n",
    "\n",
    "        all_records = []\n",
    "        chunk_start = start_date\n",
    "\n",
    "        while chunk_start < end_date:\n",
    "            chunk_end = min(chunk_start + timedelta(days=self.MAX_CHUNK_DAYS), end_date)\n",
    "\n",
    "            try:\n",
    "                records = self.kite.historical_data(\n",
    "                    instrument_token=int(instrument_token),\n",
    "                    from_date=chunk_start,\n",
    "                    to_date=chunk_end,\n",
    "                    interval='day',\n",
    "                    continuous=True,\n",
    "                )\n",
    "                all_records.extend(records)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"  Chunk {chunk_start}-{chunk_end} failed: {e}\")\n",
    "\n",
    "            chunk_start = chunk_end + timedelta(days=1)\n",
    "\n",
    "        if not all_records:\n",
    "            raise ValueError(f\"No data returned for {symbol} ({resolved_name})\")\n",
    "\n",
    "        df = pd.DataFrame(all_records)\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "        df = df.set_index('date').sort_index()\n",
    "\n",
    "        # Standardize column names to lowercase\n",
    "        df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "        # Keep only OHLCV\n",
    "        keep_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "        for col in keep_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = np.nan\n",
    "\n",
    "        df = df[keep_cols]\n",
    "        df = df[~df.index.duplicated(keep='last')]\n",
    "\n",
    "        print(f\"  {resolved_name}: {len(df)} bars, {df.index[0].date()} to {df.index[-1].date()}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d907716e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:36:18.806757Z",
     "iopub.status.busy": "2026-02-14T16:36:18.806598Z",
     "iopub.status.idle": "2026-02-14T16:36:22.643544Z",
     "shell.execute_reply": "2026-02-14T16:36:22.642655Z"
    },
    "papermill": {
     "duration": 3.8443,
     "end_time": "2026-02-14T16:36:22.644206",
     "exception": false,
     "start_time": "2026-02-14T16:36:18.799906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2b: Cross-Asset Data — News Sentiment + India VIX\n",
    "# ============================================================================\n",
    "#\n",
    "# Cross-asset features: COMMON across all tickers (one value per day).\n",
    "# Captures market-wide regime through:\n",
    "#   1. FinBERT sentiment of GDELT financial news headlines (9 features)\n",
    "#   2. India VIX fear gauge (3 features)\n",
    "#\n",
    "# Architecture:\n",
    "#   - Headlines fetched from GDELT DOC 2.0 API (free, no API key)\n",
    "#   - Scored with ProsusAI/finbert on GPU (~5ms/headline)\n",
    "#   - All data cached to disk for instant re-runs\n",
    "#   - T+1 causality: headlines from day D -> features for day D+1\n",
    "#   - India VIX fetched via Kite API (already authenticated)\n",
    "# ============================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# FinBERT availability check\n",
    "# ---------------------------------------------------------------------------\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    HAS_FINBERT = True\n",
    "except ImportError:\n",
    "    HAS_FINBERT = False\n",
    "    logger.warning(\"torch/transformers not installed — sentiment features disabled\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Cache directories\n",
    "# ---------------------------------------------------------------------------\n",
    "_QK_CACHE_DIR = Path.home() / '.quantkubera'\n",
    "_HEADLINE_CACHE_DIR = _QK_CACHE_DIR / 'headlines'\n",
    "_SCORE_CACHE_DIR = _QK_CACHE_DIR / 'sentiment_scores'\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# GDELT DOC 2.0 API\n",
    "# ---------------------------------------------------------------------------\n",
    "_GDELT_ENDPOINT = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "_GDELT_DELAY = 2.0  # seconds between requests\n",
    "\n",
    "# Focused queries: India markets + commodities + global macro + crypto\n",
    "_GDELT_QUERIES = [\n",
    "    # India markets — covers NIFTY, BANKNIFTY, Sensex, broad market\n",
    "    '\"NIFTY\" OR \"Sensex\" OR \"BSE\" OR \"NSE\" OR \"Indian stock market\" '\n",
    "    'sourceCountry:IN sourcelang:eng',\n",
    "    # India FnO stocks\n",
    "    '\"Reliance\" OR \"TCS\" OR \"HDFC\" OR \"Infosys\" OR \"ICICI\" OR \"SBI\" '\n",
    "    'sourceCountry:IN sourcelang:eng',\n",
    "    # Commodities — gold, oil, metals (relevant for MCX tickers)\n",
    "    '\"gold price\" OR \"crude oil\" OR \"silver price\" OR \"copper\" OR \"natural gas\" '\n",
    "    'sourcelang:eng',\n",
    "    # Global macro — Fed, rates, trade (affects all markets)\n",
    "    '\"Federal Reserve\" OR \"interest rate\" OR \"global markets\" OR \"trade war\" '\n",
    "    'OR \"tariffs\" sourcelang:eng',\n",
    "    # Crypto\n",
    "    '\"bitcoin\" OR \"ethereum\" OR \"cryptocurrency\" sourcelang:eng',\n",
    "]\n",
    "\n",
    "\n",
    "def _gdelt_fetch_articles(query: str, start_dt: datetime,\n",
    "                           end_dt: datetime) -> list:\n",
    "    \"\"\"Fetch articles from GDELT DOC 2.0 API (single request, max 250).\"\"\"\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'mode': 'ArtList',\n",
    "        'format': 'json',\n",
    "        'maxrecords': 250,\n",
    "        'startdatetime': start_dt.strftime('%Y%m%d%H%M%S'),\n",
    "        'enddatetime': end_dt.strftime('%Y%m%d%H%M%S'),\n",
    "        'sort': 'DateDesc',\n",
    "    }\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            resp = requests.get(_GDELT_ENDPOINT, params=params, timeout=30,\n",
    "                                headers={'User-Agent': 'QuantKubera/2.1'})\n",
    "            if resp.status_code == 429:\n",
    "                wait = 5 * (attempt + 1)\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            return data.get('articles', [])\n",
    "        except requests.exceptions.JSONDecodeError:\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"GDELT fetch failed (attempt {attempt+1}/3): {e}\")\n",
    "            if attempt < 2:\n",
    "                time.sleep(3)\n",
    "    return []\n",
    "\n",
    "\n",
    "def _parse_gdelt_date(seendate: str) -> Optional[datetime]:\n",
    "    \"\"\"Parse GDELT seendate like '20260210T083000Z'.\"\"\"\n",
    "    try:\n",
    "        clean = seendate.replace('T', '').replace('Z', '')\n",
    "        return datetime.strptime(clean, '%Y%m%d%H%M%S')\n",
    "    except (ValueError, AttributeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_gdelt_headlines(start_date: str, end_date: str,\n",
    "                          chunk_days: int = 14) -> Dict[str, list]:\n",
    "    \"\"\"Fetch financial news headlines from GDELT with disk caching.\n",
    "\n",
    "    Args:\n",
    "        start_date, end_date: 'YYYY-MM-DD'\n",
    "        chunk_days: calendar days per API request\n",
    "\n",
    "    Returns: {date_str: [{'title': str, 'source': str}]}\n",
    "    \"\"\"\n",
    "    _HEADLINE_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Check disk cache\n",
    "    cache_file = _HEADLINE_CACHE_DIR / f\"gdelt_{start_date}_{end_date}.json\"\n",
    "    if cache_file.exists():\n",
    "        try:\n",
    "            with open(cache_file) as f:\n",
    "                cached = json.load(f)\n",
    "            n_total = sum(len(v) for v in cached.values())\n",
    "            print(f\"  GDELT cache hit: {n_total} headlines across \"\n",
    "                  f\"{len(cached)} days\")\n",
    "            return cached\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            pass\n",
    "\n",
    "    start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_dt = datetime.strptime(end_date, '%Y-%m-%d').replace(\n",
    "        hour=23, minute=59, second=59)\n",
    "\n",
    "    seen_titles = set()\n",
    "    daily_headlines: Dict[str, list] = {}\n",
    "    total = 0\n",
    "\n",
    "    n_queries = len(_GDELT_QUERIES)\n",
    "    n_chunks_per_query = max(1, (end_dt - start_dt).days // chunk_days + 1)\n",
    "    total_requests = n_queries * n_chunks_per_query\n",
    "\n",
    "    pbar = tqdm(total=total_requests, desc=\"GDELT headlines\",\n",
    "                unit=\"req\", leave=False)\n",
    "\n",
    "    for q_idx, query in enumerate(_GDELT_QUERIES):\n",
    "        chunk_start = start_dt\n",
    "        while chunk_start < end_dt:\n",
    "            chunk_end = min(\n",
    "                chunk_start + timedelta(days=chunk_days), end_dt)\n",
    "\n",
    "            articles = _gdelt_fetch_articles(query, chunk_start, chunk_end)\n",
    "\n",
    "            for art in articles:\n",
    "                title = art.get('title', '').strip()\n",
    "                if not title or len(title) < 15:\n",
    "                    continue\n",
    "\n",
    "                norm = title.lower().strip()\n",
    "                if norm in seen_titles:\n",
    "                    continue\n",
    "                seen_titles.add(norm)\n",
    "\n",
    "                dt = _parse_gdelt_date(art.get('seendate', ''))\n",
    "                if dt is None:\n",
    "                    continue\n",
    "\n",
    "                date_str = dt.strftime('%Y-%m-%d')\n",
    "                if date_str not in daily_headlines:\n",
    "                    daily_headlines[date_str] = []\n",
    "\n",
    "                daily_headlines[date_str].append({\n",
    "                    'title': title,\n",
    "                    'source': art.get('domain', 'unknown'),\n",
    "                })\n",
    "                total += 1\n",
    "\n",
    "            chunk_start = chunk_end + timedelta(days=1)\n",
    "            time.sleep(_GDELT_DELAY)\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"{total} headlines\")\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Save cache\n",
    "    with open(cache_file, 'w') as f:\n",
    "        json.dump(daily_headlines, f)\n",
    "\n",
    "    print(f\"  GDELT: {total} unique headlines across {len(daily_headlines)} days\")\n",
    "    return daily_headlines\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# FinBERT Sentiment Scorer with disk caching\n",
    "# ---------------------------------------------------------------------------\n",
    "_finbert_model = None\n",
    "_finbert_tokenizer = None\n",
    "_finbert_device = None\n",
    "\n",
    "\n",
    "def _load_finbert():\n",
    "    \"\"\"Load ProsusAI/finbert model (singleton, GPU if available).\"\"\"\n",
    "    global _finbert_model, _finbert_tokenizer, _finbert_device\n",
    "    if _finbert_model is not None:\n",
    "        return\n",
    "\n",
    "    if not HAS_FINBERT:\n",
    "        raise RuntimeError(\"torch/transformers not installed\")\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"  Loading ProsusAI/finbert on {device}...\")\n",
    "\n",
    "    _finbert_tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "    _finbert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'ProsusAI/finbert')\n",
    "    _finbert_model.to(device)\n",
    "    _finbert_model.eval()\n",
    "    _finbert_device = device\n",
    "    print(f\"  FinBERT loaded on {device}\")\n",
    "\n",
    "\n",
    "def _finbert_score_batch(texts: list, batch_size: int = 64) -> list:\n",
    "    \"\"\"Score texts with FinBERT. Returns [(score, confidence, label), ...]\n",
    "\n",
    "    score: float in [-1, +1] (positive - negative probability)\n",
    "    confidence: float in [0, 1] (max class probability)\n",
    "    label: str \"positive\" | \"negative\" | \"neutral\"\n",
    "    \"\"\"\n",
    "    _load_finbert()\n",
    "    label_map = {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
    "    results = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = _finbert_tokenizer(\n",
    "            batch, padding=True, truncation=True,\n",
    "            max_length=128, return_tensors='pt'\n",
    "        ).to(_finbert_device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = _finbert_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "        for j in range(len(batch)):\n",
    "            p = probs[j]\n",
    "            pos, neg = p[0].item(), p[1].item()\n",
    "            score = pos - neg\n",
    "            confidence = p.max().item()\n",
    "            label = label_map[p.argmax().item()]\n",
    "            results.append((score, confidence, label))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _title_hash(title: str) -> str:\n",
    "    \"\"\"Deterministic hash for cache key.\"\"\"\n",
    "    return hashlib.sha256(title.strip().lower().encode()).hexdigest()[:16]\n",
    "\n",
    "\n",
    "def score_headlines_with_cache(\n",
    "    daily_headlines: Dict[str, list],\n",
    ") -> Dict[str, list]:\n",
    "    \"\"\"Score all headlines with FinBERT, using disk cache.\n",
    "\n",
    "    Args:\n",
    "        daily_headlines: {date_str: [{'title': str, 'source': str}]}\n",
    "\n",
    "    Returns: {date_str: [(score, confidence, label), ...]}\n",
    "    \"\"\"\n",
    "    if not HAS_FINBERT:\n",
    "        return {}\n",
    "\n",
    "    _SCORE_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    cache_file = _SCORE_CACHE_DIR / 'finbert_scores.json'\n",
    "\n",
    "    # Load existing cache {title_hash: [score, confidence, label]}\n",
    "    score_cache: Dict[str, list] = {}\n",
    "    if cache_file.exists():\n",
    "        try:\n",
    "            with open(cache_file) as f:\n",
    "                score_cache = json.load(f)\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            pass\n",
    "\n",
    "    # Collect uncached titles\n",
    "    uncached = []  # (date_str, idx, title, hash)\n",
    "    for date_str, headlines in daily_headlines.items():\n",
    "        for idx, h in enumerate(headlines):\n",
    "            th = _title_hash(h['title'])\n",
    "            if th not in score_cache:\n",
    "                uncached.append((date_str, idx, h['title'], th))\n",
    "\n",
    "    if uncached:\n",
    "        print(f\"  Scoring {len(uncached)} uncached headlines with FinBERT...\")\n",
    "        titles = [u[2] for u in uncached]\n",
    "        scores = _finbert_score_batch(titles)\n",
    "\n",
    "        for (date_str, idx, title, th), (score, conf, label) in zip(\n",
    "                uncached, scores):\n",
    "            score_cache[th] = [score, conf, label]\n",
    "\n",
    "        # Persist cache\n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(score_cache, f)\n",
    "        print(f\"  Scored {len(uncached)} headlines, cache now \"\n",
    "              f\"{len(score_cache)} entries\")\n",
    "    else:\n",
    "        print(f\"  All {sum(len(v) for v in daily_headlines.values())} \"\n",
    "              f\"headlines already cached\")\n",
    "\n",
    "    # Build result\n",
    "    result: Dict[str, list] = {}\n",
    "    for date_str, headlines in daily_headlines.items():\n",
    "        day_scores = []\n",
    "        for h in headlines:\n",
    "            th = _title_hash(h['title'])\n",
    "            if th in score_cache:\n",
    "                day_scores.append(tuple(score_cache[th]))\n",
    "            else:\n",
    "                day_scores.append((0.0, 0.5, 'neutral'))\n",
    "        result[date_str] = day_scores\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Sentiment Feature Builder (9 features, T+1 lag)\n",
    "# ---------------------------------------------------------------------------\n",
    "SENTIMENT_COLUMNS = [\n",
    "    'ns_sent_mean', 'ns_sent_std', 'ns_pos_ratio', 'ns_neg_ratio',\n",
    "    'ns_confidence_mean', 'ns_news_count',\n",
    "    'ns_sent_5d_ma', 'ns_news_count_5d', 'ns_sent_momentum',\n",
    "]\n",
    "\n",
    "\n",
    "def build_sentiment_features(\n",
    "    daily_scores: Dict[str, list],\n",
    "    date_index: pd.DatetimeIndex,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Build 9 daily sentiment features with T+1 causal lag.\n",
    "\n",
    "    Headlines from day D -> features for day D+1 (zero look-ahead).\n",
    "\n",
    "    Args:\n",
    "        daily_scores: {date_str: [(score, confidence, label), ...]}\n",
    "        date_index: DatetimeIndex to align features to\n",
    "\n",
    "    Returns: DataFrame with SENTIMENT_COLUMNS, indexed by date\n",
    "    \"\"\"\n",
    "    if not daily_scores:\n",
    "        return pd.DataFrame(columns=SENTIMENT_COLUMNS,\n",
    "                            index=date_index, dtype=float)\n",
    "\n",
    "    rows = []\n",
    "    for date_str, scores in sorted(daily_scores.items()):\n",
    "        if not scores:\n",
    "            continue\n",
    "        scores_arr = np.array([s[0] for s in scores])\n",
    "        confs_arr = np.array([s[1] for s in scores])\n",
    "        labels = [s[2] for s in scores]\n",
    "        n = len(scores_arr)\n",
    "        n_pos = sum(1 for l in labels if l == 'positive')\n",
    "        n_neg = sum(1 for l in labels if l == 'negative')\n",
    "\n",
    "        rows.append({\n",
    "            'date': pd.Timestamp(date_str),\n",
    "            'ns_sent_mean': float(np.mean(scores_arr)),\n",
    "            'ns_sent_std': float(np.std(scores_arr, ddof=1)) if n > 1 else 0.0,\n",
    "            'ns_pos_ratio': n_pos / n if n > 0 else 0.0,\n",
    "            'ns_neg_ratio': n_neg / n if n > 0 else 0.0,\n",
    "            'ns_confidence_mean': float(np.mean(confs_arr)),\n",
    "            'ns_news_count': float(n),\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=SENTIMENT_COLUMNS,\n",
    "                            index=date_index, dtype=float)\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index('date').sort_index()\n",
    "\n",
    "    # Rolling features (causal — use only past data)\n",
    "    df['ns_sent_5d_ma'] = df['ns_sent_mean'].rolling(5, min_periods=1).mean()\n",
    "    df['ns_news_count_5d'] = df['ns_news_count'].rolling(5, min_periods=1).sum()\n",
    "    df['ns_sent_momentum'] = df['ns_sent_mean'] - df['ns_sent_5d_ma']\n",
    "\n",
    "    # T+1 lag: headlines from day D become features for day D+1\n",
    "    df = df.shift(1)\n",
    "\n",
    "    # Align to target date index, forward-fill short gaps (weekends)\n",
    "    df = df.reindex(date_index)\n",
    "    df = df.ffill(limit=3)\n",
    "\n",
    "    return df[SENTIMENT_COLUMNS]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# India VIX Fetcher (via Kite API)\n",
    "# ---------------------------------------------------------------------------\n",
    "def fetch_india_vix_kite(kite: KiteConnect, lookback_days: int = 2500\n",
    "                         ) -> pd.DataFrame:\n",
    "    \"\"\"Fetch India VIX daily OHLCV via Kite API.\n",
    "\n",
    "    Uses the authenticated KiteConnect instance to fetch INDIA VIX from NSE.\n",
    "\n",
    "    Args:\n",
    "        kite: authenticated KiteConnect instance\n",
    "        lookback_days: calendar days of history\n",
    "\n",
    "    Returns: DataFrame with [vix_close, vix_open, vix_high, vix_low],\n",
    "             index=date (tz-naive DatetimeIndex)\n",
    "    \"\"\"\n",
    "    empty = pd.DataFrame(\n",
    "        columns=['vix_open', 'vix_high', 'vix_low', 'vix_close'])\n",
    "\n",
    "    # Resolve INDIA VIX instrument token\n",
    "    try:\n",
    "        instruments = pd.DataFrame(kite.instruments('NSE'))\n",
    "        vix_rows = instruments[\n",
    "            instruments['tradingsymbol'] == 'INDIA VIX']\n",
    "        if vix_rows.empty:\n",
    "            # Try partial match\n",
    "            vix_rows = instruments[\n",
    "                instruments['tradingsymbol'].str.contains(\n",
    "                    'VIX', case=False, na=False)]\n",
    "        if vix_rows.empty:\n",
    "            logger.warning(\"INDIA VIX instrument not found on NSE\")\n",
    "            return empty\n",
    "        token = int(vix_rows.iloc[0]['instrument_token'])\n",
    "        symbol = vix_rows.iloc[0]['tradingsymbol']\n",
    "        print(f\"  Resolved VIX: {symbol} (token={token})\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"VIX instrument resolution failed: {e}\")\n",
    "        return empty\n",
    "\n",
    "    end_date = datetime.now().date()\n",
    "    start_date = end_date - timedelta(days=lookback_days)\n",
    "\n",
    "    all_records = []\n",
    "    chunk_start = start_date\n",
    "    max_chunk = 1900\n",
    "\n",
    "    while chunk_start < end_date:\n",
    "        chunk_end = min(chunk_start + timedelta(days=max_chunk), end_date)\n",
    "        try:\n",
    "            records = kite.historical_data(\n",
    "                instrument_token=token,\n",
    "                from_date=chunk_start,\n",
    "                to_date=chunk_end,\n",
    "                interval='day',\n",
    "                continuous=False,  # VIX is an index, not futures\n",
    "            )\n",
    "            all_records.extend(records)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"VIX chunk {chunk_start}-{chunk_end} failed: {e}\")\n",
    "        chunk_start = chunk_end + timedelta(days=1)\n",
    "\n",
    "    if not all_records:\n",
    "        return empty\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "    df = df.set_index('date').sort_index()\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "    # Rename to vix_* prefix\n",
    "    rename_map = {'close': 'vix_close', 'open': 'vix_open',\n",
    "                  'high': 'vix_high', 'low': 'vix_low'}\n",
    "    df = df.rename(columns=rename_map)\n",
    "    keep = [c for c in ['vix_open', 'vix_high', 'vix_low', 'vix_close']\n",
    "            if c in df.columns]\n",
    "    df = df[keep]\n",
    "    df = df[~df.index.duplicated(keep='last')]\n",
    "\n",
    "    print(f\"  India VIX: {len(df)} days \"\n",
    "          f\"({df.index[0].date()} to {df.index[-1].date()})\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# VIX Feature Builder (3 features)\n",
    "# ---------------------------------------------------------------------------\n",
    "VIX_COLUMNS = ['vix_level', 'vix_change_5d', 'vix_mean_reversion']\n",
    "\n",
    "\n",
    "def build_vix_features(vix_df: pd.DataFrame,\n",
    "                       date_index: pd.DatetimeIndex) -> pd.DataFrame:\n",
    "    \"\"\"Build 3 VIX features.\n",
    "\n",
    "    Args:\n",
    "        vix_df: DataFrame with vix_close column\n",
    "        date_index: DatetimeIndex to align to\n",
    "\n",
    "    Returns: DataFrame with VIX_COLUMNS\n",
    "    \"\"\"\n",
    "    if vix_df.empty or 'vix_close' not in vix_df.columns:\n",
    "        return pd.DataFrame(columns=VIX_COLUMNS,\n",
    "                            index=date_index, dtype=float)\n",
    "\n",
    "    vix = vix_df['vix_close'].copy()\n",
    "    result = pd.DataFrame(index=vix.index)\n",
    "\n",
    "    # VIX z-score: (VIX - 60d mean) / 60d std\n",
    "    vix_mean = vix.rolling(60, min_periods=20).mean()\n",
    "    vix_std = vix.rolling(60, min_periods=20).std(ddof=1)\n",
    "    vix_std = vix_std.replace(0, np.nan)\n",
    "    result['vix_level'] = (vix - vix_mean) / vix_std\n",
    "\n",
    "    # 5-day VIX change (percent)\n",
    "    result['vix_change_5d'] = vix.pct_change(5)\n",
    "\n",
    "    # VIX mean reversion: (VIX - 20d MA) / 20d std\n",
    "    vix_ma20 = vix.rolling(20, min_periods=10).mean()\n",
    "    vix_std20 = vix.rolling(20, min_periods=10).std(ddof=1)\n",
    "    vix_std20 = vix_std20.replace(0, np.nan)\n",
    "    result['vix_mean_reversion'] = (vix - vix_ma20) / vix_std20\n",
    "\n",
    "    # Align to target date index, forward-fill short gaps\n",
    "    result = result.reindex(date_index)\n",
    "    result = result.ffill(limit=3)\n",
    "\n",
    "    return result[VIX_COLUMNS]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Master Cross-Asset Function\n",
    "# ---------------------------------------------------------------------------\n",
    "CROSS_ASSET_COLUMNS: List[str] = []  # populated at runtime\n",
    "\n",
    "\n",
    "def fetch_cross_asset_features(\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    date_index: pd.DatetimeIndex,\n",
    "    kite: Optional[KiteConnect] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Fetch all cross-asset data and build features.\n",
    "\n",
    "    Populates the global CROSS_ASSET_COLUMNS with available feature names.\n",
    "\n",
    "    Args:\n",
    "        start_date, end_date: 'YYYY-MM-DD'\n",
    "        date_index: DatetimeIndex to align all features to\n",
    "        kite: authenticated KiteConnect (for VIX)\n",
    "\n",
    "    Returns: DataFrame with available cross-asset features\n",
    "    \"\"\"\n",
    "    global CROSS_ASSET_COLUMNS\n",
    "\n",
    "    all_features = pd.DataFrame(index=date_index)\n",
    "    available_cols: List[str] = []\n",
    "\n",
    "    # --- News Sentiment (GDELT + FinBERT) ---\n",
    "    print(\"\\n  [1/2] News Sentiment (GDELT + FinBERT)\")\n",
    "    try:\n",
    "        daily_headlines = fetch_gdelt_headlines(start_date, end_date)\n",
    "\n",
    "        if daily_headlines and HAS_FINBERT:\n",
    "            daily_scores = score_headlines_with_cache(daily_headlines)\n",
    "            sent_df = build_sentiment_features(daily_scores, date_index)\n",
    "\n",
    "            for col in SENTIMENT_COLUMNS:\n",
    "                if col in sent_df.columns:\n",
    "                    all_features[col] = sent_df[col]\n",
    "                    available_cols.append(col)\n",
    "\n",
    "            n_days = sent_df.notna().any(axis=1).sum()\n",
    "            print(f\"    -> {len(available_cols)} features, \"\n",
    "                  f\"{n_days} days with data\")\n",
    "        elif not HAS_FINBERT:\n",
    "            print(\"    SKIP: FinBERT not available \"\n",
    "                  \"(pip install torch transformers)\")\n",
    "        else:\n",
    "            print(\"    SKIP: No headlines fetched from GDELT\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Sentiment features failed: {e}\")\n",
    "        print(f\"    SKIP: Failed ({e})\")\n",
    "\n",
    "    # --- India VIX ---\n",
    "    print(\"  [2/2] India VIX\")\n",
    "    try:\n",
    "        if kite is not None:\n",
    "            vix_df = fetch_india_vix_kite(kite)\n",
    "            if not vix_df.empty:\n",
    "                vix_feats = build_vix_features(vix_df, date_index)\n",
    "                for col in VIX_COLUMNS:\n",
    "                    if col in vix_feats.columns:\n",
    "                        all_features[col] = vix_feats[col]\n",
    "                        available_cols.append(col)\n",
    "                n_days = vix_feats.notna().any(axis=1).sum()\n",
    "                print(f\"    -> {len(VIX_COLUMNS)} features, \"\n",
    "                      f\"{n_days} days with data\")\n",
    "            else:\n",
    "                print(\"    SKIP: No VIX data from Kite\")\n",
    "        else:\n",
    "            print(\"    SKIP: No Kite instance provided\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"VIX features failed: {e}\")\n",
    "        print(f\"    SKIP: Failed ({e})\")\n",
    "\n",
    "    CROSS_ASSET_COLUMNS = available_cols\n",
    "\n",
    "    print(f\"\\n  Cross-Asset Summary: {len(available_cols)} features available\")\n",
    "    if available_cols:\n",
    "        print(f\"    Columns: {available_cols}\")\n",
    "\n",
    "    if available_cols:\n",
    "        return all_features[available_cols]\n",
    "    return pd.DataFrame(index=date_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc5cca45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:36:22.657540Z",
     "iopub.status.busy": "2026-02-14T16:36:22.657120Z",
     "iopub.status.idle": "2026-02-14T16:36:22.679730Z",
     "shell.execute_reply": "2026-02-14T16:36:22.678993Z"
    },
    "papermill": {
     "duration": 0.02993,
     "end_time": "2026-02-14T16:36:22.680289",
     "exception": false,
     "start_time": "2026-02-14T16:36:22.650359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3b: World Monitor Intelligence Engine\n",
    "# ============================================================================\n",
    "# Integrates macro signals from the World Monitor geopolitical intelligence\n",
    "# platform (koala73/worldmonitor). Adds global macro regime detection,\n",
    "# anomaly detection (Welford's algorithm), and cross-market signals.\n",
    "#\n",
    "# Data Sources:\n",
    "#   - Yahoo Finance: JPY/USD, QQQ, XLP, DXY, Gold, BTC (free, no API key)\n",
    "#   - alternative.me: Fear & Greed Index (free)\n",
    "#   - CoinGecko: Stablecoin peg health (free)\n",
    "#\n",
    "# All features are CAUSAL: use data available at time t to predict t+1.\n",
    "# ============================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Group 13: Macro Regime Signals (World Monitor port)\n",
    "# ---------------------------------------------------------------------------\n",
    "# Ported from worldmonitor/api/macro-signals.js\n",
    "# 7-signal composite: JPY liquidity, risk-on/off, flow alignment,\n",
    "# technical trend, hash rate, mining cost, Fear & Greed.\n",
    "# We implement the 4 most relevant for India FnO:\n",
    "#   1. JPY carry (yen strengthening kills EM)\n",
    "#   2. Risk-on/off (QQQ vs XLP consumer staples)\n",
    "#   3. BTC/QQQ flow alignment (cross-asset momentum coherence)\n",
    "#   4. Fear & Greed (sentiment baseline)\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "MACRO_COLUMNS = [\n",
    "    'macro_jpy_carry',       # JPY/USD 30d ROC (negative = squeeze)\n",
    "    'macro_risk_regime',     # QQQ/XLP ratio z-score (risk-on/off)\n",
    "    'macro_flow_align',      # |BTC 5d - QQQ 5d| (cross-asset coherence)\n",
    "    'macro_fear_greed_z',    # Fear & Greed Index z-scored\n",
    "    'macro_composite',       # Weighted composite (0-100)\n",
    "]\n",
    "\n",
    "def fetch_yahoo_macro(start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Fetch macro signals from Yahoo Finance (free, no API key).\n",
    "\n",
    "    Fetches: JPY=X (USD/JPY), QQQ, XLP, BTC-USD, GC=F (Gold), DX-Y.NYB (DXY)\n",
    "    Returns: DataFrame indexed by date with macro features.\n",
    "    \"\"\"\n",
    "    if not HAS_YFINANCE:\n",
    "        logger.warning(\"yfinance not installed — macro signals disabled\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    tickers = {\n",
    "        'JPY=X': 'jpy',       # USD/JPY (invert for JPY strength)\n",
    "        'QQQ': 'qqq',         # Nasdaq 100 ETF\n",
    "        'XLP': 'xlp',         # Consumer Staples (defensive)\n",
    "        'BTC-USD': 'btc',     # Bitcoin\n",
    "    }\n",
    "\n",
    "    print(\"  Fetching Yahoo Finance macro data...\")\n",
    "    try:\n",
    "        data = yf.download(\n",
    "            list(tickers.keys()),\n",
    "            start=start_date,\n",
    "            end=end_date,\n",
    "            auto_adjust=True,\n",
    "            progress=False,\n",
    "        )\n",
    "        if data.empty:\n",
    "            print(\"    SKIP: No data from Yahoo Finance\")\n",
    "            return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Yahoo Finance fetch failed: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Extract close prices\n",
    "    closes = pd.DataFrame(index=data.index)\n",
    "    if isinstance(data.columns, pd.MultiIndex):\n",
    "        for yahoo_tk, col_name in tickers.items():\n",
    "            if yahoo_tk in data['Close'].columns:\n",
    "                closes[col_name] = data['Close'][yahoo_tk]\n",
    "    else:\n",
    "        # Single ticker case\n",
    "        for yahoo_tk, col_name in tickers.items():\n",
    "            if 'Close' in data.columns:\n",
    "                closes[col_name] = data['Close']\n",
    "\n",
    "    if closes.empty or closes.dropna(how='all').empty:\n",
    "        print(\"    SKIP: Yahoo Finance returned empty data\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    closes = closes.ffill().dropna()\n",
    "\n",
    "    result = pd.DataFrame(index=closes.index)\n",
    "\n",
    "    # Signal 1: JPY Carry — 30d rate of change of USD/JPY\n",
    "    # Negative ROC = JPY strengthening = EM risk (carry unwind)\n",
    "    if 'jpy' in closes.columns:\n",
    "        jpy_roc_30 = closes['jpy'].pct_change(30) * 100\n",
    "        result['macro_jpy_carry'] = jpy_roc_30\n",
    "\n",
    "    # Signal 2: Risk-On/Off — QQQ vs XLP 20d ROC ratio\n",
    "    # QQQ outperforming XLP = risk-on; z-scored for stationarity\n",
    "    if 'qqq' in closes.columns and 'xlp' in closes.columns:\n",
    "        qqq_roc = closes['qqq'].pct_change(20)\n",
    "        xlp_roc = closes['xlp'].pct_change(20)\n",
    "        ratio = qqq_roc - xlp_roc\n",
    "        result['macro_risk_regime'] = (\n",
    "            (ratio - ratio.rolling(126).mean()) /\n",
    "            (ratio.rolling(126).std() + 1e-8)\n",
    "        )\n",
    "\n",
    "    # Signal 3: Flow Alignment — |BTC 5d - QQQ 5d| gap\n",
    "    # Small gap = markets aligned = momentum persistence\n",
    "    if 'btc' in closes.columns and 'qqq' in closes.columns:\n",
    "        btc_5d = closes['btc'].pct_change(5) * 100\n",
    "        qqq_5d = closes['qqq'].pct_change(5) * 100\n",
    "        gap = (btc_5d - qqq_5d).abs()\n",
    "        # Invert and z-score: low gap = high alignment = positive signal\n",
    "        result['macro_flow_align'] = -(\n",
    "            (gap - gap.rolling(63).mean()) /\n",
    "            (gap.rolling(63).std() + 1e-8)\n",
    "        )\n",
    "\n",
    "    n_signals = result.notna().any().sum()\n",
    "    n_days = len(result.dropna(how='all'))\n",
    "    print(f\"    -> {n_signals} macro signals, {n_days} days\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def fetch_fear_greed() -> pd.DataFrame:\n",
    "    \"\"\"Fetch Fear & Greed Index from alternative.me (free, no API key).\n",
    "\n",
    "    Returns: DataFrame with 'macro_fear_greed_z' column.\n",
    "    The index ranges 0-100. We z-score it over 126-day window.\n",
    "    \"\"\"\n",
    "    url = \"https://api.alternative.me/fng/?limit=365&format=json\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json().get('data', [])\n",
    "        if not data:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        records = []\n",
    "        for item in data:\n",
    "            dt = pd.to_datetime(int(item['timestamp']), unit='s')\n",
    "            records.append({'date': dt, 'fear_greed': int(item['value'])})\n",
    "\n",
    "        df = pd.DataFrame(records).set_index('date').sort_index()\n",
    "        # Z-score over 126-day rolling window\n",
    "        fg = df['fear_greed'].astype(float)\n",
    "        df['macro_fear_greed_z'] = (\n",
    "            (fg - fg.rolling(126, min_periods=30).mean()) /\n",
    "            (fg.rolling(126, min_periods=30).std() + 1e-8)\n",
    "        )\n",
    "\n",
    "        print(f\"    -> Fear & Greed Index: {len(df)} days, \"\n",
    "              f\"latest={df['fear_greed'].iloc[-1]}\")\n",
    "        return df[['macro_fear_greed_z']]\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Fear & Greed fetch failed: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def compute_macro_composite(macro_df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"Compute weighted macro composite score (0-100).\n",
    "\n",
    "    Ported from worldmonitor/api/macro-signals.js verdict logic.\n",
    "    Each signal contributes to a bullish/bearish score.\n",
    "    \"\"\"\n",
    "    score = pd.Series(50.0, index=macro_df.index)  # neutral baseline\n",
    "\n",
    "    if 'macro_jpy_carry' in macro_df.columns:\n",
    "        # JPY carry > 0 (USD strengthening vs JPY) = bullish for EM\n",
    "        score += np.where(macro_df['macro_jpy_carry'] > 0, 10, -10)\n",
    "\n",
    "    if 'macro_risk_regime' in macro_df.columns:\n",
    "        # Risk-on (positive z) = bullish\n",
    "        score += np.clip(macro_df['macro_risk_regime'] * 5, -15, 15)\n",
    "\n",
    "    if 'macro_flow_align' in macro_df.columns:\n",
    "        # Aligned flows (positive) = bullish\n",
    "        score += np.clip(macro_df['macro_flow_align'] * 5, -10, 10)\n",
    "\n",
    "    if 'macro_fear_greed_z' in macro_df.columns:\n",
    "        score += np.clip(macro_df['macro_fear_greed_z'] * 5, -15, 15)\n",
    "\n",
    "    return np.clip(score, 0, 100)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Group 14: Welford Anomaly Detection (World Monitor port)\n",
    "# ---------------------------------------------------------------------------\n",
    "# Ported from worldmonitor/api/temporal-baseline.js\n",
    "# Uses Welford's online algorithm for numerically stable streaming\n",
    "# mean/variance computation. Detects z-score deviations from learned\n",
    "# baselines per weekday × month (seasonal patterns).\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "WELFORD_COLUMNS = [\n",
    "    'anomaly_volume_z',    # Volume vs weekday×month baseline\n",
    "    'anomaly_range_z',     # High-low range vs baseline\n",
    "    'anomaly_gap_z',       # Overnight gap vs baseline\n",
    "    'anomaly_composite',   # Max of above (worst anomaly)\n",
    "]\n",
    "\n",
    "\n",
    "class WelfordBaseline:\n",
    "    \"\"\"Welford's online algorithm for streaming mean/variance.\n",
    "\n",
    "    Ported from worldmonitor/api/temporal-baseline.js.\n",
    "    Numerically stable single-pass computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.baselines = {}  # key -> {mean, m2, n}\n",
    "\n",
    "    def _key(self, weekday: int, month: int, metric: str) -> str:\n",
    "        return f\"{metric}:{weekday}:{month}\"\n",
    "\n",
    "    def update(self, weekday: int, month: int, metric: str, value: float):\n",
    "        \"\"\"Update baseline with new observation (Welford's algorithm).\"\"\"\n",
    "        key = self._key(weekday, month, metric)\n",
    "        if key not in self.baselines:\n",
    "            self.baselines[key] = {'mean': 0.0, 'm2': 0.0, 'n': 0}\n",
    "\n",
    "        bl = self.baselines[key]\n",
    "        bl['n'] += 1\n",
    "        delta = value - bl['mean']\n",
    "        bl['mean'] += delta / bl['n']\n",
    "        delta2 = value - bl['mean']\n",
    "        bl['m2'] += delta * delta2\n",
    "\n",
    "    def zscore(self, weekday: int, month: int, metric: str,\n",
    "               value: float, min_samples: int = 10) -> float:\n",
    "        \"\"\"Compute z-score of value against baseline.\"\"\"\n",
    "        key = self._key(weekday, month, metric)\n",
    "        if key not in self.baselines:\n",
    "            return 0.0\n",
    "\n",
    "        bl = self.baselines[key]\n",
    "        if bl['n'] < min_samples:\n",
    "            return 0.0\n",
    "\n",
    "        variance = max(0, bl['m2'] / (bl['n'] - 1))  # ddof=1\n",
    "        std = np.sqrt(variance)\n",
    "        if std < 1e-10:\n",
    "            return 0.0\n",
    "\n",
    "        return (value - bl['mean']) / std\n",
    "\n",
    "\n",
    "def compute_welford_anomalies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Compute anomaly z-scores using Welford's online algorithm.\n",
    "\n",
    "    Detects unusual volume, range, and gap sizes relative to\n",
    "    weekday×month seasonal baselines. FULLY CAUSAL: only uses\n",
    "    data from before time t to compute baselines at time t.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with OHLCV columns and DatetimeIndex\n",
    "\n",
    "    Returns: DataFrame with anomaly z-score columns\n",
    "    \"\"\"\n",
    "    baseline = WelfordBaseline()\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "\n",
    "    vol_z = np.zeros(len(df))\n",
    "    range_z = np.zeros(len(df))\n",
    "    gap_z = np.zeros(len(df))\n",
    "\n",
    "    closes = df['close'].values\n",
    "    volumes = df['volume'].values\n",
    "    highs = df['high'].values\n",
    "    lows = df['low'].values\n",
    "    opens = df['open'].values\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        dt = df.index[i]\n",
    "        wd = dt.weekday()\n",
    "        mo = dt.month\n",
    "\n",
    "        # Current values\n",
    "        vol_val = float(volumes[i]) if volumes[i] > 0 else 0.0\n",
    "        range_val = float((highs[i] - lows[i]) / closes[i-1]) if closes[i-1] > 0 else 0.0\n",
    "        gap_val = float(abs(opens[i] - closes[i-1]) / closes[i-1]) if closes[i-1] > 0 else 0.0\n",
    "\n",
    "        # Compute z-scores BEFORE updating (causal)\n",
    "        vol_z[i] = baseline.zscore(wd, mo, 'volume', vol_val)\n",
    "        range_z[i] = baseline.zscore(wd, mo, 'range', range_val)\n",
    "        gap_z[i] = baseline.zscore(wd, mo, 'gap', gap_val)\n",
    "\n",
    "        # Update baselines AFTER scoring (causal)\n",
    "        baseline.update(wd, mo, 'volume', vol_val)\n",
    "        baseline.update(wd, mo, 'range', range_val)\n",
    "        baseline.update(wd, mo, 'gap', gap_val)\n",
    "\n",
    "    result['anomaly_volume_z'] = vol_z\n",
    "    result['anomaly_range_z'] = range_z\n",
    "    result['anomaly_gap_z'] = gap_z\n",
    "    result['anomaly_composite'] = np.maximum.reduce([\n",
    "        np.abs(vol_z), np.abs(range_z), np.abs(gap_z)\n",
    "    ])\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b1ffd7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:36:22.693039Z",
     "iopub.status.busy": "2026-02-14T16:36:22.692881Z",
     "iopub.status.idle": "2026-02-14T16:36:22.742603Z",
     "shell.execute_reply": "2026-02-14T16:36:22.741886Z"
    },
    "papermill": {
     "duration": 0.057143,
     "end_time": "2026-02-14T16:36:22.743207",
     "exception": false,
     "start_time": "2026-02-14T16:36:22.686064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Advanced Feature Engineering — 31 Features in 10 Groups\n",
    "# ============================================================================\n",
    "\n",
    "FEATURE_COLUMNS = [\n",
    "    'norm_ret_1d', 'norm_ret_21d', 'norm_ret_63d', 'norm_ret_126d', 'norm_ret_252d',\n",
    "    'macd_8_24', 'macd_16_48', 'macd_32_96',\n",
    "    'rvol_20d', 'rvol_60d', 'gk_vol_20d', 'parkinson_vol_20d',\n",
    "    'cp_rl_21', 'cp_score_21',\n",
    "    'frac_diff_03', 'frac_diff_05', 'hurst_exp',\n",
    "    'ram_5', 'ram_10', 'ram_21', 'ram_63',\n",
    "    'vpin', 'kyles_lambda', 'amihud_illiq', 'hl_spread',\n",
    "    'entropy',\n",
    "    'trend_strength', 'momentum_consistency', 'mr_zscore',\n",
    "    'vol_zscore', 'vol_momentum',\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 1: Normalized Returns (5 features)\n",
    "# ============================================================================\n",
    "def compute_returns(close: pd.Series, horizons: List[int] = [1, 21, 63, 126, 252]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute horizon returns normalized by EWM volatility.\n",
    "    norm_ret_h = log(close / close.shift(h)) / (vol * sqrt(h))\n",
    "    where vol = ewm(span=60).std() of daily log returns.\n",
    "    \"\"\"\n",
    "    log_ret = np.log(close / close.shift(1))\n",
    "    vol = log_ret.ewm(span=60, min_periods=20).std()\n",
    "\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "    for h in horizons:\n",
    "        raw_ret = np.log(close / close.shift(h))\n",
    "        denom = vol * np.sqrt(h)\n",
    "        # Avoid division by zero\n",
    "        denom = denom.replace(0, np.nan)\n",
    "        result[f'norm_ret_{h}d'] = raw_ret / denom\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 2: MACD (3 features)\n",
    "# ============================================================================\n",
    "def compute_macd(close: pd.Series, pairs: List[Tuple[int, int]] = [(8, 24), (16, 48), (32, 96)]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute MACD z-scores for stationarity.\n",
    "    Raw MACD = EWM(fast) - EWM(slow), then z-scored over 126-day rolling window.\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "    for fast, slow in pairs:\n",
    "        ema_fast = close.ewm(span=fast, min_periods=fast).mean()\n",
    "        ema_slow = close.ewm(span=slow, min_periods=slow).mean()\n",
    "        raw_macd = ema_fast - ema_slow\n",
    "\n",
    "        roll_mean = raw_macd.rolling(window=126, min_periods=63).mean()\n",
    "        roll_std = raw_macd.rolling(window=126, min_periods=63).std(ddof=1)\n",
    "        roll_std = roll_std.replace(0, np.nan)\n",
    "\n",
    "        result[f'macd_{fast}_{slow}'] = (raw_macd - roll_mean) / roll_std\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 3: Volatility (4 features)\n",
    "# ============================================================================\n",
    "def compute_volatility(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute 4 volatility estimators:\n",
    "      - rvol_20d, rvol_60d: realized vol (rolling std of log returns, annualized)\n",
    "      - gk_vol_20d: Garman-Klass volatility\n",
    "      - parkinson_vol_20d: Parkinson high-low volatility\n",
    "    \"\"\"\n",
    "    close = df['close']\n",
    "    high = df['high']\n",
    "    low = df['low']\n",
    "    opn = df['open']\n",
    "\n",
    "    log_ret = np.log(close / close.shift(1))\n",
    "\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Realized vol\n",
    "    result['rvol_20d'] = log_ret.rolling(window=20, min_periods=15).std(ddof=1) * np.sqrt(252)\n",
    "    result['rvol_60d'] = log_ret.rolling(window=60, min_periods=40).std(ddof=1) * np.sqrt(252)\n",
    "\n",
    "    # Garman-Klass: sqrt(mean(0.5*ln(H/L)^2 - (2*ln2 - 1)*ln(C/O)^2) * 252)\n",
    "    log_hl = np.log(high / low)\n",
    "    log_co = np.log(close / opn)\n",
    "    gk_term = 0.5 * log_hl ** 2 - (2.0 * np.log(2.0) - 1.0) * log_co ** 2\n",
    "    gk_mean = gk_term.rolling(window=20, min_periods=15).mean()\n",
    "    # Clamp to non-negative before sqrt\n",
    "    gk_mean = gk_mean.clip(lower=0.0)\n",
    "    result['gk_vol_20d'] = np.sqrt(gk_mean * 252)\n",
    "\n",
    "    # Parkinson: sqrt(mean(ln(H/L)^2 / (4*ln2)) * 252)\n",
    "    park_term = log_hl ** 2 / (4.0 * np.log(2.0))\n",
    "    park_mean = park_term.rolling(window=20, min_periods=15).mean()\n",
    "    park_mean = park_mean.clip(lower=0.0)\n",
    "    result['parkinson_vol_20d'] = np.sqrt(park_mean * 252)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 4: Changepoint Detection (2 features)\n",
    "# ============================================================================\n",
    "def nig_log_marginal(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Normal-Inverse-Gamma log marginal likelihood P(x | NIG prior).\n",
    "    Prior: mu0=0, kappa0=1, alpha0=1, beta0=1\n",
    "    Posterior update:\n",
    "      n = len(x), x_bar = mean(x), s2 = var(x)\n",
    "      kappa_n = kappa0 + n\n",
    "      alpha_n = alpha0 + n/2\n",
    "      beta_n = beta0 + 0.5*n*s2 + 0.5*kappa0*n*x_bar^2 / kappa_n\n",
    "    Log marginal = gammaln(alpha_n) - gammaln(alpha0) + alpha0*log(beta0) - alpha_n*log(beta_n)\n",
    "                   + 0.5*log(kappa0/kappa_n) - (n/2)*log(2*pi)\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    if n < 2:\n",
    "        return -np.inf\n",
    "\n",
    "    mu0, kappa0, alpha0, beta0 = 0.0, 1.0, 1.0, 1.0\n",
    "\n",
    "    x_bar = np.mean(x)\n",
    "    s2 = np.var(x, ddof=1) if n > 1 else 0.0\n",
    "\n",
    "    kappa_n = kappa0 + n\n",
    "    alpha_n = alpha0 + n / 2.0\n",
    "    beta_n = beta0 + 0.5 * (n - 1) * s2 + 0.5 * kappa0 * n * x_bar ** 2 / kappa_n\n",
    "\n",
    "    # Protect against non-positive beta_n\n",
    "    if beta_n <= 0:\n",
    "        beta_n = 1e-300\n",
    "\n",
    "    log_ml = (\n",
    "        gammaln(alpha_n) - gammaln(alpha0)\n",
    "        + alpha0 * np.log(beta0) - alpha_n * np.log(beta_n)\n",
    "        + 0.5 * np.log(kappa0 / kappa_n)\n",
    "        - (n / 2.0) * np.log(2.0 * np.pi)\n",
    "    )\n",
    "    return log_ml\n",
    "\n",
    "\n",
    "def compute_cpd(close: pd.Series, lookback: int = 21, min_seg: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Changepoint detection via NIG Bayesian model comparison.\n",
    "    For each position, take lookback-window of log returns.\n",
    "    Try all split points; best split maximizes sum of two-segment likelihoods.\n",
    "    cp_rl = best_split_position / lookback (relative location in [0, 1])\n",
    "    cp_score = sigmoid(best_split_ll - full_ll) (severity in [0, 1])\n",
    "    \"\"\"\n",
    "    log_ret = np.log(close / close.shift(1)).values\n",
    "    n = len(log_ret)\n",
    "\n",
    "    cp_rl = np.full(n, np.nan)\n",
    "    cp_score = np.full(n, np.nan)\n",
    "\n",
    "    for i in range(lookback, n):\n",
    "        window = log_ret[i - lookback + 1: i + 1]  # lookback values ending at i\n",
    "\n",
    "        # Skip if any NaN\n",
    "        if np.any(np.isnan(window)):\n",
    "            continue\n",
    "\n",
    "        full_ll = nig_log_marginal(window)\n",
    "\n",
    "        best_split_ll = -np.inf\n",
    "        best_split_pos = lookback // 2  # default to middle\n",
    "\n",
    "        for s in range(min_seg, lookback - min_seg + 1):\n",
    "            left = window[:s]\n",
    "            right = window[s:]\n",
    "            split_ll = nig_log_marginal(left) + nig_log_marginal(right)\n",
    "\n",
    "            if split_ll > best_split_ll:\n",
    "                best_split_ll = split_ll\n",
    "                best_split_pos = s\n",
    "\n",
    "        cp_rl[i] = best_split_pos / lookback\n",
    "\n",
    "        # Severity: sigmoid of log-likelihood ratio\n",
    "        delta = best_split_ll - full_ll\n",
    "        # Clamp to avoid overflow in exp\n",
    "        delta_clamped = np.clip(delta, -500, 500)\n",
    "        cp_score[i] = 1.0 / (1.0 + np.exp(-delta_clamped))\n",
    "\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "    result['cp_rl_21'] = cp_rl\n",
    "    result['cp_score_21'] = cp_score\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 5: Fractional Calculus (3 features)\n",
    "# ============================================================================\n",
    "def frac_diff_weights(d: float, thresh: float = 1e-5, max_width: int = 500) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Hosking (1981) fractional differencing weights.\n",
    "    w[0] = 1, w[k] = -w[k-1] * (d - k + 1) / k\n",
    "    Iterate until |w[k]| < thresh or max_width reached.\n",
    "\n",
    "    Note: For d=0.3 with thresh=1e-5, weights decay as k^{-1.3} requiring\n",
    "    ~7000 terms. max_width caps this to keep the warmup period practical\n",
    "    while preserving >99.9% of the filter energy.\n",
    "\n",
    "    Returns weights array from oldest (index 0) to newest (index -1).\n",
    "    \"\"\"\n",
    "    weights = [1.0]\n",
    "    k = 1\n",
    "    while True:\n",
    "        w_k = -weights[-1] * (d - k + 1) / k\n",
    "        if abs(w_k) < thresh:\n",
    "            break\n",
    "        weights.append(w_k)\n",
    "        k += 1\n",
    "        if k >= max_width:\n",
    "            break\n",
    "    # Reverse so index 0 = oldest weight\n",
    "    return np.array(weights[::-1])\n",
    "\n",
    "\n",
    "def compute_frac_diff(close: pd.Series, d: float) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply fractional differencing of order d to log(close).\n",
    "    Convolve log prices with frac_diff weights.\n",
    "    \"\"\"\n",
    "    log_price = np.log(close.values)\n",
    "    w = frac_diff_weights(d)\n",
    "    width = len(w)\n",
    "\n",
    "    n = len(log_price)\n",
    "    result = np.full(n, np.nan)\n",
    "\n",
    "    for i in range(width - 1, n):\n",
    "        segment = log_price[i - width + 1: i + 1]\n",
    "        result[i] = np.dot(w, segment)\n",
    "\n",
    "    return pd.Series(result, index=close.index)\n",
    "\n",
    "\n",
    "def _compute_hurst_single(returns: np.ndarray, max_lag: int = 50) -> float:\n",
    "    \"\"\"\n",
    "    Compute Hurst exponent from returns using MSD (Mean Squared Displacement).\n",
    "    MSD(tau) = E[(X(t+tau) - X(t))^2] where X = cumsum(returns)\n",
    "    Regress log(MSD) on log(tau) -> slope / 2 = H\n",
    "    \"\"\"\n",
    "    if len(returns) < max_lag + 10:\n",
    "        return np.nan\n",
    "\n",
    "    X = np.cumsum(returns)\n",
    "    taus = np.arange(1, max_lag + 1)\n",
    "    msd = np.full(max_lag, np.nan)\n",
    "\n",
    "    for idx, tau in enumerate(taus):\n",
    "        diffs = X[tau:] - X[:-tau]\n",
    "        if len(diffs) < 5:\n",
    "            continue\n",
    "        msd[idx] = np.mean(diffs ** 2)\n",
    "\n",
    "    # Filter valid MSD values (positive and finite)\n",
    "    valid = np.isfinite(msd) & (msd > 0)\n",
    "    if valid.sum() < 5:\n",
    "        return np.nan\n",
    "\n",
    "    log_tau = np.log(taus[valid])\n",
    "    log_msd = np.log(msd[valid])\n",
    "\n",
    "    # Linear regression: log_msd = slope * log_tau + intercept\n",
    "    n_valid = len(log_tau)\n",
    "    sum_x = np.sum(log_tau)\n",
    "    sum_y = np.sum(log_msd)\n",
    "    sum_xy = np.sum(log_tau * log_msd)\n",
    "    sum_xx = np.sum(log_tau ** 2)\n",
    "\n",
    "    denom = n_valid * sum_xx - sum_x ** 2\n",
    "    if abs(denom) < 1e-15:\n",
    "        return np.nan\n",
    "\n",
    "    slope = (n_valid * sum_xy - sum_x * sum_y) / denom\n",
    "    hurst = slope / 2.0\n",
    "\n",
    "    # Clamp to reasonable range\n",
    "    return np.clip(hurst, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def compute_hurst(close: pd.Series, window: int = 252, max_lag: int = 50) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Rolling Hurst exponent computed over a window of returns.\n",
    "    \"\"\"\n",
    "    log_ret = np.log(close / close.shift(1)).values\n",
    "    n = len(log_ret)\n",
    "    hurst_vals = np.full(n, np.nan)\n",
    "\n",
    "    for i in range(window, n):\n",
    "        segment = log_ret[i - window + 1: i + 1]\n",
    "        if np.any(np.isnan(segment)):\n",
    "            continue\n",
    "        hurst_vals[i] = _compute_hurst_single(segment, max_lag=max_lag)\n",
    "\n",
    "    return pd.Series(hurst_vals, index=close.index, name='hurst_exp')\n",
    "\n",
    "\n",
    "def compute_fractional_features(close: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Compute all fractional calculus features.\"\"\"\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "    result['frac_diff_03'] = compute_frac_diff(close, d=0.3)\n",
    "    result['frac_diff_05'] = compute_frac_diff(close, d=0.5)\n",
    "    result['hurst_exp'] = compute_hurst(close)\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 6: Ramanujan Sum Filter Bank (4 features)\n",
    "# ============================================================================\n",
    "def euler_phi(n: int) -> int:\n",
    "    \"\"\"Euler's totient function: count of integers in [1, n] coprime to n.\"\"\"\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    result = n\n",
    "    p = 2\n",
    "    temp = n\n",
    "    while p * p <= temp:\n",
    "        if temp % p == 0:\n",
    "            while temp % p == 0:\n",
    "                temp //= p\n",
    "            result -= result // p\n",
    "        p += 1\n",
    "    if temp > 1:\n",
    "        result -= result // temp\n",
    "    return result\n",
    "\n",
    "\n",
    "def mobius(n: int) -> int:\n",
    "    \"\"\"\n",
    "    Mobius function:\n",
    "      mu(1) = 1\n",
    "      mu(n) = 0 if n has a squared prime factor\n",
    "      mu(n) = (-1)^k if n is a product of k distinct primes\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    if n == 1:\n",
    "        return 1\n",
    "\n",
    "    num_factors = 0\n",
    "    temp = n\n",
    "    p = 2\n",
    "\n",
    "    while p * p <= temp:\n",
    "        if temp % p == 0:\n",
    "            temp //= p\n",
    "            num_factors += 1\n",
    "            if temp % p == 0:\n",
    "                return 0  # Squared factor\n",
    "        p += 1\n",
    "\n",
    "    if temp > 1:\n",
    "        num_factors += 1\n",
    "\n",
    "    return 1 if num_factors % 2 == 0 else -1\n",
    "\n",
    "\n",
    "def ramanujan_sum(q: int, n: int) -> float:\n",
    "    \"\"\"\n",
    "    Ramanujan sum c_q(n) = sum over d|gcd(n,q) of mu(q/d) * phi(q) / phi(q/d)\n",
    "    Simplified: c_q(n) = mu(q/g) * phi(q) / phi(q/g) where g = gcd(n, q)\n",
    "    Actually the full definition sums over all d dividing gcd(n,q).\n",
    "    \"\"\"\n",
    "    g = math.gcd(n, q)\n",
    "    # Sum over divisors d of g: mu(q/d) * d * (phi(q/d) != 0 check)\n",
    "    # More standard: c_q(n) = sum_{d | gcd(n,q)} d * mu(q/d)\n",
    "    total = 0.0\n",
    "    for d in range(1, g + 1):\n",
    "        if g % d == 0:\n",
    "            qd = q // d\n",
    "            total += d * mobius(qd)\n",
    "    return total\n",
    "\n",
    "\n",
    "def compute_ramanujan(close: pd.Series, periods: List[int] = [5, 10, 21, 63],\n",
    "                      window: int = 252) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ramanujan Sum Filter Bank: convolve log-returns with Ramanujan sum kernels\n",
    "    to extract energy at specific trading cycle periods.\n",
    "    \"\"\"\n",
    "    log_ret = np.log(close / close.shift(1)).values\n",
    "    n = len(log_ret)\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "\n",
    "    for q in periods:\n",
    "        # Pre-compute kernel: kernel[j] = c_q(j+1) for j in [0, window)\n",
    "        kernel = np.array([ramanujan_sum(q, j + 1) for j in range(window)])\n",
    "        kernel = kernel / window  # Normalize\n",
    "\n",
    "        # Convolve (causal: only use past data)\n",
    "        filtered = np.full(n, np.nan)\n",
    "        for i in range(window, n):\n",
    "            segment = log_ret[i - window + 1: i + 1]\n",
    "            if np.any(np.isnan(segment)):\n",
    "                continue\n",
    "            # Kernel is applied: newest data * kernel[0], oldest * kernel[-1]\n",
    "            # Reverse kernel for convolution alignment (kernel[0] applies to most recent)\n",
    "            filtered[i] = np.dot(segment, kernel[::-1][:len(segment)])\n",
    "\n",
    "        result[f'ram_{q}'] = filtered\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 7: Microstructure (4 features)\n",
    "# ============================================================================\n",
    "def compute_microstructure(df: pd.DataFrame, window: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute 4 microstructure features:\n",
    "      - VPIN: Volume-Synchronized Probability of Informed Trading\n",
    "      - Kyle's Lambda: price impact coefficient\n",
    "      - Amihud Illiquidity: |return| / dollar volume\n",
    "      - HL Spread: high-low spread proxy (Corwin-Schultz simplified)\n",
    "    \"\"\"\n",
    "    close = df['close']\n",
    "    high = df['high']\n",
    "    low = df['low']\n",
    "    volume = df['volume'].astype(float)\n",
    "\n",
    "    log_ret = np.log(close / close.shift(1))\n",
    "\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # --- VPIN ---\n",
    "    sigma = log_ret.rolling(window=20, min_periods=10).std()\n",
    "    sigma = sigma.replace(0, np.nan)\n",
    "    # Bulk volume classification: buy probability = Phi(ret / sigma)\n",
    "    z = log_ret / sigma\n",
    "    buy_prob = pd.Series(norm.cdf(z.values), index=close.index)\n",
    "    buy_vol = volume * buy_prob\n",
    "    sell_vol = volume * (1.0 - buy_prob)\n",
    "    abs_imbalance = (buy_vol - sell_vol).abs()\n",
    "    total_vol = volume.rolling(window=window, min_periods=window // 2).sum()\n",
    "    total_vol = total_vol.replace(0, np.nan)\n",
    "    vpin = abs_imbalance.rolling(window=window, min_periods=window // 2).sum() / total_vol\n",
    "    result['vpin'] = vpin\n",
    "\n",
    "    # --- Kyle's Lambda ---\n",
    "    abs_ret = log_ret.abs()\n",
    "    signed_vol = np.sign(log_ret) * volume\n",
    "    abs_signed_vol = signed_vol.abs()\n",
    "\n",
    "    # Rolling covariance / variance\n",
    "    cov_rv = abs_ret.rolling(window=window, min_periods=window // 2).cov(abs_signed_vol)\n",
    "    var_sv = abs_signed_vol.rolling(window=window, min_periods=window // 2).var(ddof=1)\n",
    "    var_sv = var_sv.replace(0, np.nan)\n",
    "    result['kyles_lambda'] = cov_rv / var_sv\n",
    "\n",
    "    # --- Amihud Illiquidity ---\n",
    "    dollar_vol = close * volume\n",
    "    dollar_vol = dollar_vol.replace(0, np.nan)\n",
    "    daily_illiq = abs_ret / dollar_vol\n",
    "    result['amihud_illiq'] = daily_illiq.rolling(window=window, min_periods=window // 2).mean()\n",
    "\n",
    "    # --- HL Spread (Corwin-Schultz simplified) ---\n",
    "    # Use rolling average of log(H/L) as spread proxy\n",
    "    log_hl = np.log(high / low)\n",
    "    # Corwin-Schultz: alpha derived from 2-day high-low ratio\n",
    "    # Simplified version: spread = 2*(exp(alpha) - 1) / (1 + exp(alpha))\n",
    "    # where alpha = (sqrt(2*beta) - sqrt(beta)) / (3 - 2*sqrt(2))\n",
    "    # beta = E[ln(H_t/L_t)^2]\n",
    "    beta = (log_hl ** 2).rolling(window=window, min_periods=window // 2).mean()\n",
    "    # Also compute gamma from 2-day range\n",
    "    high_2d = high.rolling(window=2).max()\n",
    "    low_2d = low.rolling(window=2).min()\n",
    "    gamma = np.log(high_2d / low_2d) ** 2\n",
    "\n",
    "    # alpha = (sqrt(2) - 1) * sqrt(beta) / (3 - 2*sqrt(2)) when gamma term is small\n",
    "    # Full: alpha = (sqrt(2*beta) - sqrt(beta)) / (3 - 2*sqrt(2)) - sqrt(gamma / (3 - 2*sqrt(2)))\n",
    "    sqrt2 = np.sqrt(2.0)\n",
    "    denom_cs = 3.0 - 2.0 * sqrt2\n",
    "\n",
    "    # Ensure beta is non-negative\n",
    "    beta_safe = beta.clip(lower=0.0)\n",
    "    gamma_safe = gamma.clip(lower=0.0)\n",
    "\n",
    "    alpha = (np.sqrt(2.0 * beta_safe) - np.sqrt(beta_safe)) / denom_cs\n",
    "    # Adjust with gamma correction\n",
    "    gamma_correction = np.sqrt(gamma_safe / denom_cs)\n",
    "    alpha = alpha - gamma_correction\n",
    "    # Clamp alpha to reasonable range to avoid extreme spread values\n",
    "    alpha = alpha.clip(lower=-1.0, upper=2.0)\n",
    "\n",
    "    spread = 2.0 * (np.exp(alpha) - 1.0) / (1.0 + np.exp(alpha))\n",
    "    # Clamp negative spreads to 0\n",
    "    spread = spread.clip(lower=0.0)\n",
    "    result['hl_spread'] = spread\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 8: Information Theory (1 feature)\n",
    "# ============================================================================\n",
    "def compute_entropy(close: pd.Series, word_len: int = 3, window: int = 252) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Shannon entropy of binary price movement patterns.\n",
    "    Encode: 1 if price up, 0 if down.\n",
    "    Form words of word_len consecutive bits.\n",
    "    Compute normalized Shannon entropy over rolling window.\n",
    "    \"\"\"\n",
    "    # Binary encoding: 1 if close > prev_close, 0 otherwise\n",
    "    direction = (close.diff() > 0).astype(int).values\n",
    "    n = len(direction)\n",
    "    n_words = 2 ** word_len\n",
    "    max_entropy = np.log2(n_words) if n_words > 1 else 1.0\n",
    "\n",
    "    entropy_vals = np.full(n, np.nan)\n",
    "\n",
    "    for i in range(window + word_len - 1, n):\n",
    "        # Extract the window of directions\n",
    "        seg = direction[i - window + 1: i + 1]\n",
    "\n",
    "        # Build words\n",
    "        words = []\n",
    "        for j in range(word_len - 1, len(seg)):\n",
    "            word = 0\n",
    "            for k in range(word_len):\n",
    "                word = (word << 1) | seg[j - word_len + 1 + k]\n",
    "            words.append(word)\n",
    "\n",
    "        if len(words) == 0:\n",
    "            continue\n",
    "\n",
    "        # Histogram\n",
    "        counts = np.bincount(words, minlength=n_words).astype(float)\n",
    "        probs = counts / counts.sum()\n",
    "\n",
    "        # Shannon entropy (base 2)\n",
    "        probs_pos = probs[probs > 0]\n",
    "        H = -np.sum(probs_pos * np.log2(probs_pos))\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        entropy_vals[i] = H / max_entropy if max_entropy > 0 else 0.0\n",
    "\n",
    "    return pd.Series(entropy_vals, index=close.index, name='entropy')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 9: Momentum Quality (3 features)\n",
    "# ============================================================================\n",
    "def compute_momentum_quality(close: pd.Series, window: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute momentum quality metrics:\n",
    "      - trend_strength: |avg_up - avg_down| / (avg_up + avg_down)\n",
    "      - momentum_consistency: fraction of positive returns in rolling window\n",
    "      - mr_zscore: (close - EMA) / rolling_std  (mean-reversion z-score)\n",
    "    \"\"\"\n",
    "    ret = close.pct_change()\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "\n",
    "    # Trend strength\n",
    "    up_ret = ret.clip(lower=0)\n",
    "    down_ret = (-ret).clip(lower=0)  # magnitude of down moves\n",
    "\n",
    "    avg_up = up_ret.rolling(window=window, min_periods=window // 2).mean()\n",
    "    avg_down = down_ret.rolling(window=window, min_periods=window // 2).mean()\n",
    "\n",
    "    denom_ts = avg_up + avg_down\n",
    "    denom_ts = denom_ts.replace(0, np.nan)\n",
    "    result['trend_strength'] = (avg_up - avg_down).abs() / denom_ts\n",
    "\n",
    "    # Momentum consistency: fraction of positive returns\n",
    "    pos_indicator = (ret > 0).astype(float)\n",
    "    result['momentum_consistency'] = pos_indicator.rolling(\n",
    "        window=window, min_periods=window // 2\n",
    "    ).mean()\n",
    "\n",
    "    # Mean reversion z-score: (close - EMA) / rolling_std\n",
    "    ema = close.ewm(span=window, min_periods=window // 2).mean()\n",
    "    rolling_std = close.rolling(window=window, min_periods=window // 2).std(ddof=1)\n",
    "    rolling_std = rolling_std.replace(0, np.nan)\n",
    "    result['mr_zscore'] = (close - ema) / rolling_std\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 10: Volume Features (2 features)\n",
    "# ============================================================================\n",
    "def compute_volume_features(df: pd.DataFrame, window: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute volume-based features:\n",
    "      - vol_zscore: (volume - rolling_mean) / rolling_std\n",
    "      - vol_momentum: volume.pct_change(5)  (5-day volume momentum)\n",
    "    \"\"\"\n",
    "    volume = df['volume'].astype(float)\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "\n",
    "    roll_mean = volume.rolling(window=window, min_periods=window // 2).mean()\n",
    "    roll_std = volume.rolling(window=window, min_periods=window // 2).std(ddof=1)\n",
    "    roll_std = roll_std.replace(0, np.nan)\n",
    "\n",
    "    result['vol_zscore'] = (volume - roll_mean) / roll_std\n",
    "    result['vol_momentum'] = volume.pct_change(periods=5)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Master Function: Build All Features\n",
    "# ============================================================================\n",
    "def build_all_features(df: pd.DataFrame, cfg: Optional[MonolithConfig] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build all 31 engineered features from 10 research groups.\n",
    "    Adds forward target: target_ret = close.pct_change(1).shift(-1)\n",
    "    Drops NaN warmup rows.\n",
    "    Returns df with FEATURE_COLUMNS + 'target_ret' + original OHLCV.\n",
    "    \"\"\"\n",
    "    if cfg is None:\n",
    "        cfg = MonolithConfig()\n",
    "\n",
    "    close = df['close']\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(\"Building features...\")\n",
    "\n",
    "    # Group 1: Normalized Returns\n",
    "    print(\"  [1/10] Normalized Returns (5 features)\")\n",
    "    feat_ret = compute_returns(close)\n",
    "\n",
    "    # Group 2: MACD\n",
    "    print(\"  [2/10] MACD Z-scores (3 features)\")\n",
    "    feat_macd = compute_macd(close)\n",
    "\n",
    "    # Group 3: Volatility\n",
    "    print(\"  [3/10] Volatility Estimators (4 features)\")\n",
    "    feat_vol = compute_volatility(df)\n",
    "\n",
    "    # Group 4: Changepoint Detection\n",
    "    print(\"  [4/10] NIG Changepoint Detection (2 features)\")\n",
    "    feat_cpd = compute_cpd(close)\n",
    "\n",
    "    # Group 5: Fractional Calculus\n",
    "    print(\"  [5/10] Fractional Differentiation + Hurst (3 features)\")\n",
    "    feat_frac = compute_fractional_features(close)\n",
    "\n",
    "    # Group 6: Ramanujan Filter Bank\n",
    "    print(\"  [6/10] Ramanujan Sum Filter Bank (4 features)\")\n",
    "    feat_ram = compute_ramanujan(close)\n",
    "\n",
    "    # Group 7: Microstructure\n",
    "    print(\"  [7/10] Market Microstructure (4 features)\")\n",
    "    feat_micro = compute_microstructure(df)\n",
    "\n",
    "    # Group 8: Entropy\n",
    "    print(\"  [8/10] Information-Theoretic Entropy (1 feature)\")\n",
    "    feat_entropy = compute_entropy(close)\n",
    "\n",
    "    # Group 9: Momentum Quality\n",
    "    print(\"  [9/10] Momentum Quality (3 features)\")\n",
    "    feat_mq = compute_momentum_quality(close)\n",
    "\n",
    "    # Group 10: Volume Features\n",
    "    print(\"  [10/10] Volume Features (2 features)\")\n",
    "    feat_vf = compute_volume_features(df)\n",
    "\n",
    "    # Assemble all features into the dataframe\n",
    "    out = df.copy()\n",
    "\n",
    "    for col in feat_ret.columns:\n",
    "        out[col] = feat_ret[col]\n",
    "    for col in feat_macd.columns:\n",
    "        out[col] = feat_macd[col]\n",
    "    for col in feat_vol.columns:\n",
    "        out[col] = feat_vol[col]\n",
    "    for col in feat_cpd.columns:\n",
    "        out[col] = feat_cpd[col]\n",
    "    for col in feat_frac.columns:\n",
    "        out[col] = feat_frac[col]\n",
    "    for col in feat_ram.columns:\n",
    "        out[col] = feat_ram[col]\n",
    "    for col in feat_micro.columns:\n",
    "        out[col] = feat_micro[col]\n",
    "    out['entropy'] = feat_entropy\n",
    "    for col in feat_mq.columns:\n",
    "        out[col] = feat_mq[col]\n",
    "    for col in feat_vf.columns:\n",
    "        out[col] = feat_vf[col]\n",
    "\n",
    "    # Target: next-day return (shift -1 is the ONLY forward-looking value, used as label)\n",
    "    out['target_ret'] = close.pct_change(1).shift(-1)\n",
    "\n",
    "    # Vol-scaled training target: raw_fwd_return / realized_vol\n",
    "    # Vol computed on CURRENT (unshifted) returns — no leakage\n",
    "    log_ret = np.log(close / close.shift(1))\n",
    "    vol_20 = log_ret.rolling(20, min_periods=10).std()\n",
    "    vol_20 = vol_20.replace(0, np.nan)\n",
    "    out['target_train'] = out['target_ret'] / vol_20\n",
    "\n",
    "    # Verify all expected feature columns exist\n",
    "    missing = [c for c in FEATURE_COLUMNS if c not in out.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing feature columns: {missing}\")\n",
    "\n",
    "    # Drop rows where ANY feature or target is NaN (warmup period)\n",
    "    n_before = len(out)\n",
    "    out_pre_drop = out  # keep reference for diagnostics\n",
    "    out = out.dropna(subset=FEATURE_COLUMNS + ['target_ret', 'target_train'])\n",
    "    n_after = len(out)\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"\\nFeature engineering complete in {elapsed:.1f}s\")\n",
    "    print(f\"  Rows: {n_before} -> {n_after} (dropped {n_before - n_after} warmup rows)\")\n",
    "    print(f\"  Features: {len(FEATURE_COLUMNS)}\")\n",
    "\n",
    "    if n_after == 0:\n",
    "        # Diagnose which features are all-NaN\n",
    "        nan_cols = [c for c in FEATURE_COLUMNS if out_pre_drop[c].isna().all()]\n",
    "        raise ValueError(\n",
    "            f\"All rows dropped after NaN removal. \"\n",
    "            f\"Features that are entirely NaN: {nan_cols}. \"\n",
    "            f\"Data has {n_before} rows but longest warmup exceeds this.\"\n",
    "        )\n",
    "\n",
    "    print(f\"  Date range: {out.index[0].date()} to {out.index[-1].date()}\")\n",
    "    print(f\"  Columns: {list(out.columns)}\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73549ab2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:36:22.755953Z",
     "iopub.status.busy": "2026-02-14T16:36:22.755796Z",
     "iopub.status.idle": "2026-02-14T16:36:22.800114Z",
     "shell.execute_reply": "2026-02-14T16:36:22.799443Z"
    },
    "papermill": {
     "duration": 0.05165,
     "end_time": "2026-02-14T16:36:22.800705",
     "exception": false,
     "start_time": "2026-02-14T16:36:22.749055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Cutting-Edge Feature Engineering — Groups 15-19\n",
    "# ============================================================================\n",
    "# Implements 5 advanced feature groups from cutting-edge quant research:\n",
    "#\n",
    "#   Group 15: HMM Regime Detection (3 features)\n",
    "#             - Baum-Welch EM on returns + volatility\n",
    "#             - 3-state: bull, bear, sideways\n",
    "#\n",
    "#   Group 16: Wavelet Decomposition (4 features)\n",
    "#             - DWT with Daubechies-4 wavelet\n",
    "#             - Multi-resolution trend/noise separation\n",
    "#             Ref: Springer Computational Economics 2025\n",
    "#\n",
    "#   Group 17: Information Theory Advanced (4 features)\n",
    "#             - KL divergence regime shift detector\n",
    "#             - Sample entropy (regularity)\n",
    "#             - NMI predictability window\n",
    "#             - Spectral entropy\n",
    "#             Ref: arXiv:2511.16339 (Financial Information Theory)\n",
    "#\n",
    "#   Group 18: Multifractal DFA (3 features)\n",
    "#             - Kantelhardt (2002) MF-DFA\n",
    "#             - Spectrum width, H(2), asymmetry\n",
    "#\n",
    "#   Group 19: TDA Persistent Homology (2 features)\n",
    "#             - Vietoris-Rips persistence on return point clouds\n",
    "#             - Crash early warning from topological complexity\n",
    "#             Ref: MDPI Computers 2025\n",
    "#\n",
    "# ALL features are CAUSAL (rolling windows, no future data).\n",
    "# ============================================================================\n",
    "\n",
    "ADVANCED_FEATURE_COLUMNS = [\n",
    "    # Group 15: HMM Regime\n",
    "    'hmm_regime',           # Decoded regime label (0=bear, 1=sideways, 2=bull)\n",
    "    'hmm_regime_prob',      # Probability of current regime\n",
    "    'hmm_regime_duration',  # Days in current regime\n",
    "    # Group 16: Wavelet\n",
    "    'wavelet_trend_energy',   # Energy in approximation coefficients (low-freq)\n",
    "    'wavelet_detail_energy',  # Energy in detail coefficients (high-freq)\n",
    "    'wavelet_snr',            # Signal-to-noise ratio (trend/detail)\n",
    "    'wavelet_dominant_scale', # Dominant wavelet scale\n",
    "    # Group 17: Info Theory\n",
    "    'kl_regime_shift',      # KL divergence between recent vs prior returns\n",
    "    'sample_entropy',       # Sample entropy (regularity measure)\n",
    "    'nmi_predictability',   # Normalized Mutual Information (market efficiency)\n",
    "    'spectral_entropy',     # Spectral entropy of return power spectrum\n",
    "    # Group 18: Multifractal\n",
    "    'mf_delta_alpha',       # Multifractal spectrum width\n",
    "    'mf_hurst2',            # Generalized Hurst exponent H(2)\n",
    "    'mf_asymmetry',         # Left-right asymmetry of MF spectrum\n",
    "    # Group 19: TDA\n",
    "    'tda_persistence_norm',  # L2-norm of persistence landscape (H1)\n",
    "    'tda_betti_ratio',       # Betti-1 / Betti-0 (loop/component ratio)\n",
    "]\n",
    "\n",
    "\n",
    "# ─── Group 15: HMM Regime Detection ─────────────────────────────────────────\n",
    "def compute_hmm_regime(close: pd.Series, n_states: int = 3,\n",
    "                       window: int = 252) -> pd.DataFrame:\n",
    "    \"\"\"Fit rolling 3-state Gaussian HMM on returns + volatility.\n",
    "\n",
    "    States are sorted by mean return: 0=bear, 1=sideways, 2=bull.\n",
    "    CAUSAL: fits on data up to time t only.\n",
    "\n",
    "    Args:\n",
    "        close: price series\n",
    "        n_states: number of hidden states (default 3)\n",
    "        window: training window in days\n",
    "\n",
    "    Returns: DataFrame with hmm_regime, hmm_regime_prob, hmm_regime_duration\n",
    "    \"\"\"\n",
    "    if not HAS_HMM:\n",
    "        logger.warning(\"hmmlearn not installed — HMM features disabled\")\n",
    "        return pd.DataFrame(\n",
    "            np.nan, index=close.index,\n",
    "            columns=['hmm_regime', 'hmm_regime_prob', 'hmm_regime_duration']\n",
    "        )\n",
    "\n",
    "    log_ret = np.log(close / close.shift(1)).dropna()\n",
    "    vol_20 = log_ret.rolling(20, min_periods=10).std()\n",
    "\n",
    "    # Observation matrix: [return, volatility]\n",
    "    obs = pd.DataFrame({\n",
    "        'ret': log_ret,\n",
    "        'vol': vol_20,\n",
    "    }).dropna()\n",
    "\n",
    "    regime = np.full(len(close), np.nan)\n",
    "    regime_prob = np.full(len(close), np.nan)\n",
    "    regime_dur = np.full(len(close), np.nan)\n",
    "\n",
    "    refit_interval = 63  # Refit every quarter\n",
    "    model = None\n",
    "    state_order = None\n",
    "\n",
    "    for t in range(window, len(obs)):\n",
    "        # Refit model periodically\n",
    "        if model is None or (t - window) % refit_interval == 0:\n",
    "            train_data = obs.iloc[t - window:t].values\n",
    "            try:\n",
    "                model = GaussianHMM(\n",
    "                    n_components=n_states, covariance_type='full',\n",
    "                    n_iter=100, random_state=42, verbose=False,\n",
    "                )\n",
    "                model.fit(train_data)\n",
    "                # Sort states by mean return: bear < sideways < bull\n",
    "                state_order = np.argsort(model.means_[:, 0])\n",
    "            except Exception:\n",
    "                model = None\n",
    "                continue\n",
    "\n",
    "        if model is None:\n",
    "            continue\n",
    "\n",
    "        # Predict current state (causal: uses data up to t)\n",
    "        try:\n",
    "            recent = obs.iloc[t - window:t + 1].values\n",
    "            states = model.predict(recent)\n",
    "            probs = model.predict_proba(recent)\n",
    "\n",
    "            current_state_raw = states[-1]\n",
    "            # Map to sorted order\n",
    "            current_state = int(np.where(state_order == current_state_raw)[0][0])\n",
    "            current_prob = float(probs[-1, current_state_raw])\n",
    "\n",
    "            # Map back to close index\n",
    "            obs_date = obs.index[t]\n",
    "            close_idx = close.index.get_loc(obs_date)\n",
    "            regime[close_idx] = current_state\n",
    "            regime_prob[close_idx] = current_prob\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Compute regime duration (consecutive days in same regime)\n",
    "    dur = np.zeros(len(close))\n",
    "    count = 0\n",
    "    prev = np.nan\n",
    "    for i in range(len(close)):\n",
    "        if np.isnan(regime[i]):\n",
    "            count = 0\n",
    "        elif regime[i] == prev:\n",
    "            count += 1\n",
    "        else:\n",
    "            count = 1\n",
    "        dur[i] = count\n",
    "        prev = regime[i]\n",
    "\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "    result['hmm_regime'] = regime\n",
    "    result['hmm_regime_prob'] = regime_prob\n",
    "    result['hmm_regime_duration'] = dur\n",
    "    return result\n",
    "\n",
    "\n",
    "# ─── Group 16: Wavelet Decomposition ────────────────────────────────────────\n",
    "def compute_wavelet_features(close: pd.Series, wavelet: str = 'db4',\n",
    "                             level: int = 4, window: int = 63) -> pd.DataFrame:\n",
    "    \"\"\"Multi-resolution wavelet decomposition features.\n",
    "\n",
    "    Uses Discrete Wavelet Transform (DWT) with Daubechies-4 wavelet.\n",
    "    Decomposes price into approximation (trend) and detail (noise)\n",
    "    coefficients at multiple scales.\n",
    "\n",
    "    Ref: \"Leveraging Wavelet Transform & Deep Learning for Option Price\n",
    "    Prediction: Insights from the Indian Derivative Market\" (2025)\n",
    "\n",
    "    Args:\n",
    "        close: price series\n",
    "        wavelet: wavelet family (default 'db4')\n",
    "        level: decomposition level (default 4)\n",
    "        window: rolling window for energy computation\n",
    "\n",
    "    Returns: DataFrame with wavelet energy features\n",
    "    \"\"\"\n",
    "    if not HAS_WAVELET:\n",
    "        logger.warning(\"PyWavelets not installed — wavelet features disabled\")\n",
    "        return pd.DataFrame(\n",
    "            np.nan, index=close.index,\n",
    "            columns=['wavelet_trend_energy', 'wavelet_detail_energy',\n",
    "                     'wavelet_snr', 'wavelet_dominant_scale']\n",
    "        )\n",
    "\n",
    "    log_ret = np.log(close / close.shift(1)).fillna(0).values\n",
    "    n = len(log_ret)\n",
    "\n",
    "    trend_energy = np.full(n, np.nan)\n",
    "    detail_energy = np.full(n, np.nan)\n",
    "    snr = np.full(n, np.nan)\n",
    "    dominant_scale = np.full(n, np.nan)\n",
    "\n",
    "    for t in range(window, n):\n",
    "        segment = log_ret[t - window:t]\n",
    "        try:\n",
    "            coeffs = pywt.wavedec(segment, wavelet, level=level)\n",
    "            # coeffs[0] = approximation (trend), coeffs[1:] = details (noise)\n",
    "            approx_energy = float(np.sum(coeffs[0] ** 2))\n",
    "            detail_energies = [float(np.sum(c ** 2)) for c in coeffs[1:]]\n",
    "            total_detail = sum(detail_energies)\n",
    "\n",
    "            trend_energy[t] = approx_energy\n",
    "            detail_energy[t] = total_detail\n",
    "            snr[t] = float(np.log1p(approx_energy / (total_detail + 1e-10)))\n",
    "\n",
    "            # Dominant scale: which detail level has most energy\n",
    "            if detail_energies:\n",
    "                dominant_scale[t] = float(np.argmax(detail_energies))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "    result['wavelet_trend_energy'] = _rolling_zscore(trend_energy, 126)\n",
    "    result['wavelet_detail_energy'] = _rolling_zscore(detail_energy, 126)\n",
    "    result['wavelet_snr'] = snr\n",
    "    result['wavelet_dominant_scale'] = dominant_scale\n",
    "    return result\n",
    "\n",
    "\n",
    "def _rolling_zscore(arr, window):\n",
    "    \"\"\"Z-score an array over a rolling window.\"\"\"\n",
    "    s = pd.Series(arr)\n",
    "    return ((s - s.rolling(window, min_periods=20).mean()) /\n",
    "            (s.rolling(window, min_periods=20).std() + 1e-8)).values\n",
    "\n",
    "\n",
    "# ─── Group 17: Information Theory Advanced ───────────────────────────────────\n",
    "def compute_kl_regime_shift(returns: np.ndarray, window: int = 252,\n",
    "                            n_bins: int = 50) -> np.ndarray:\n",
    "    \"\"\"KL divergence between recent and prior return distributions.\n",
    "\n",
    "    High KL = distribution has shifted = regime change.\n",
    "    Ref: arXiv:2511.16339 (Financial Information Theory)\n",
    "\n",
    "    CAUSAL: compares [t-W, t] vs [t-2W, t-W].\n",
    "    \"\"\"\n",
    "    kl = np.full(len(returns), np.nan)\n",
    "    for t in range(2 * window, len(returns)):\n",
    "        recent = returns[t - window:t]\n",
    "        prior = returns[t - 2 * window:t - window]\n",
    "\n",
    "        all_r = np.concatenate([recent, prior])\n",
    "        bins = np.linspace(all_r.min() - 1e-8, all_r.max() + 1e-8, n_bins + 1)\n",
    "\n",
    "        p_recent = np.histogram(recent, bins=bins)[0].astype(float) + 1e-10\n",
    "        p_prior = np.histogram(prior, bins=bins)[0].astype(float) + 1e-10\n",
    "        p_recent /= p_recent.sum()\n",
    "        p_prior /= p_prior.sum()\n",
    "\n",
    "        kl[t] = float(sp_entropy(p_recent, p_prior))\n",
    "\n",
    "    # Z-score for stationarity\n",
    "    s = pd.Series(kl)\n",
    "    z = (s - s.rolling(252, min_periods=50).mean()) / (s.rolling(252, min_periods=50).std() + 1e-8)\n",
    "    return z.values\n",
    "\n",
    "\n",
    "def compute_sample_entropy(series: np.ndarray, m: int = 2,\n",
    "                           r_mult: float = 0.2,\n",
    "                           window: int = 100) -> np.ndarray:\n",
    "    \"\"\"Rolling sample entropy (SampEn) — measures regularity.\n",
    "\n",
    "    Low SampEn = regular/predictable, High SampEn = random.\n",
    "    Markets become MORE predictable (lower SampEn) during crises.\n",
    "\n",
    "    CAUSAL: uses [t-W, t] window only.\n",
    "    \"\"\"\n",
    "    se = np.full(len(series), np.nan)\n",
    "    for t in range(window, len(series)):\n",
    "        x = series[t - window:t]\n",
    "        r = r_mult * np.std(x, ddof=1)\n",
    "        if r < 1e-10:\n",
    "            se[t] = 0.0\n",
    "            continue\n",
    "\n",
    "        N = len(x)\n",
    "        # Count template matches\n",
    "        def _count(m_len):\n",
    "            if N - m_len < 1:\n",
    "                return 0\n",
    "            templates = np.array([x[i:i + m_len] for i in range(N - m_len)])\n",
    "            count = 0\n",
    "            for i in range(len(templates)):\n",
    "                dists = np.max(np.abs(templates[i] - templates[i+1:]), axis=1)\n",
    "                count += np.sum(dists < r)\n",
    "            return count\n",
    "\n",
    "        B = _count(m)\n",
    "        A = _count(m + 1)\n",
    "\n",
    "        if B > 0 and A > 0:\n",
    "            se[t] = -np.log(A / B)\n",
    "        elif B > 0:\n",
    "            se[t] = -np.log(1.0 / B)\n",
    "        else:\n",
    "            se[t] = 0.0\n",
    "\n",
    "    return se\n",
    "\n",
    "\n",
    "def compute_nmi_predictability(returns: np.ndarray, lag: int = 1,\n",
    "                                window: int = 252,\n",
    "                                n_bins: int = 20) -> np.ndarray:\n",
    "    \"\"\"Normalized Mutual Information between lagged and current returns.\n",
    "\n",
    "    High NMI = market temporarily inefficient (exploitable).\n",
    "    ~78% of the time NMI < 0.05 (EMH holds).\n",
    "\n",
    "    Ref: arXiv:2511.16339\n",
    "    \"\"\"\n",
    "    nmi = np.full(len(returns), np.nan)\n",
    "    for t in range(window + lag, len(returns)):\n",
    "        past = returns[t - window - lag:t - lag]\n",
    "        future = returns[t - window:t]\n",
    "\n",
    "        bins = np.linspace(\n",
    "            min(past.min(), future.min()) - 1e-8,\n",
    "            max(past.max(), future.max()) + 1e-8,\n",
    "            n_bins + 1\n",
    "        )\n",
    "        past_binned = np.digitize(past, bins)\n",
    "        future_binned = np.digitize(future, bins)\n",
    "\n",
    "        # Joint and marginal entropies\n",
    "        joint_hist = np.histogram2d(past_binned, future_binned,\n",
    "                                     bins=n_bins)[0] + 1e-10\n",
    "        joint_hist /= joint_hist.sum()\n",
    "\n",
    "        p_past = joint_hist.sum(axis=1)\n",
    "        p_future = joint_hist.sum(axis=0)\n",
    "\n",
    "        h_past = -np.sum(p_past * np.log(p_past + 1e-10))\n",
    "        h_future = -np.sum(p_future * np.log(p_future + 1e-10))\n",
    "        h_joint = -np.sum(joint_hist * np.log(joint_hist + 1e-10))\n",
    "\n",
    "        mi = h_past + h_future - h_joint\n",
    "        norm = np.sqrt(h_past * h_future) if h_past > 0 and h_future > 0 else 1.0\n",
    "        nmi[t] = mi / (norm + 1e-10)\n",
    "\n",
    "    return nmi\n",
    "\n",
    "\n",
    "def compute_spectral_entropy(series: np.ndarray,\n",
    "                              window: int = 63) -> np.ndarray:\n",
    "    \"\"\"Spectral entropy of return power spectrum.\n",
    "\n",
    "    Uniform spectrum (white noise) -> high entropy.\n",
    "    Concentrated spectrum (strong periodicity) -> low entropy.\n",
    "    \"\"\"\n",
    "    se = np.full(len(series), np.nan)\n",
    "    for t in range(window, len(series)):\n",
    "        segment = series[t - window:t]\n",
    "        # FFT power spectrum\n",
    "        fft_vals = np.fft.rfft(segment - segment.mean())\n",
    "        psd = np.abs(fft_vals) ** 2\n",
    "        psd = psd / (psd.sum() + 1e-10)\n",
    "        psd = psd[psd > 0]\n",
    "        se[t] = -np.sum(psd * np.log(psd + 1e-10)) / np.log(len(psd) + 1)\n",
    "    return se\n",
    "\n",
    "\n",
    "def compute_info_theory_features(close: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Compute all Group 17 information-theoretic features.\"\"\"\n",
    "    log_ret = np.log(close / close.shift(1)).fillna(0).values\n",
    "\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "\n",
    "    print(\"    [17a] KL divergence regime shift...\")\n",
    "    result['kl_regime_shift'] = compute_kl_regime_shift(log_ret, window=126)\n",
    "\n",
    "    print(\"    [17b] Sample entropy...\")\n",
    "    result['sample_entropy'] = compute_sample_entropy(log_ret, window=60)\n",
    "\n",
    "    print(\"    [17c] NMI predictability...\")\n",
    "    result['nmi_predictability'] = compute_nmi_predictability(log_ret, window=126)\n",
    "\n",
    "    print(\"    [17d] Spectral entropy...\")\n",
    "    result['spectral_entropy'] = compute_spectral_entropy(log_ret, window=63)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ─── Group 18: Multifractal DFA ──────────────────────────────────────────────\n",
    "def compute_mfdfa_features(close: pd.Series,\n",
    "                           window: int = 252) -> pd.DataFrame:\n",
    "    \"\"\"Multifractal Detrended Fluctuation Analysis features.\n",
    "\n",
    "    Ref: Kantelhardt et al. (2002), PMC 8392555 (2021)\n",
    "\n",
    "    Computes:\n",
    "      - delta_alpha: spectrum width (large = complex multifractal)\n",
    "      - H(2): standard Hurst exponent from q=2\n",
    "      - asymmetry: left vs right spectrum skew\n",
    "\n",
    "    CAUSAL: rolling window computation.\n",
    "    \"\"\"\n",
    "    log_ret = np.log(close / close.shift(1)).fillna(0).values\n",
    "    n = len(log_ret)\n",
    "\n",
    "    delta_alpha = np.full(n, np.nan)\n",
    "    hurst2 = np.full(n, np.nan)\n",
    "    asymmetry = np.full(n, np.nan)\n",
    "\n",
    "    q_list = np.array([-3, -2, -1, 1, 2, 3, 4, 5], dtype=float)\n",
    "\n",
    "    for t in range(window, n):\n",
    "        series = log_ret[t - window:t]\n",
    "        try:\n",
    "            result = _mfdfa_single(series, q_list)\n",
    "            if result is not None:\n",
    "                delta_alpha[t] = result['delta_alpha']\n",
    "                hurst2[t] = result['hurst2']\n",
    "                asymmetry[t] = result['asymmetry']\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    out = pd.DataFrame(index=close.index)\n",
    "    out['mf_delta_alpha'] = _rolling_zscore(delta_alpha, 126)\n",
    "    out['mf_hurst2'] = hurst2\n",
    "    out['mf_asymmetry'] = asymmetry\n",
    "    return out\n",
    "\n",
    "\n",
    "def _mfdfa_single(series, q_list, order=1):\n",
    "    \"\"\"MF-DFA for a single window. Returns dict or None.\"\"\"\n",
    "    N = len(series)\n",
    "    if N < 30:\n",
    "        return None\n",
    "\n",
    "    # Profile (cumulative sum of mean-subtracted series)\n",
    "    Y = np.cumsum(series - series.mean())\n",
    "\n",
    "    # Scales: log-spaced from 8 to N/4\n",
    "    scales = np.unique(np.logspace(\n",
    "        np.log10(8), np.log10(max(10, N // 4)), 10\n",
    "    ).astype(int))\n",
    "    scales = scales[scales >= 4]\n",
    "\n",
    "    if len(scales) < 3:\n",
    "        return None\n",
    "\n",
    "    Fq = np.zeros((len(q_list), len(scales)))\n",
    "\n",
    "    for si, s in enumerate(scales):\n",
    "        n_seg = N // s\n",
    "        if n_seg < 1:\n",
    "            continue\n",
    "\n",
    "        fluct = np.zeros(n_seg)\n",
    "        t_vec = np.arange(s)\n",
    "\n",
    "        for v in range(n_seg):\n",
    "            seg = Y[v * s:(v + 1) * s]\n",
    "            if len(seg) < s:\n",
    "                continue\n",
    "            coeff = np.polyfit(t_vec[:len(seg)], seg, order)\n",
    "            trend = np.polyval(coeff, t_vec[:len(seg)])\n",
    "            fluct[v] = np.mean((seg - trend) ** 2)\n",
    "\n",
    "        fluct = fluct[fluct > 0]\n",
    "        if len(fluct) == 0:\n",
    "            continue\n",
    "\n",
    "        for qi, q in enumerate(q_list):\n",
    "            if q == 0:\n",
    "                Fq[qi, si] = np.exp(0.5 * np.mean(np.log(fluct + 1e-20)))\n",
    "            else:\n",
    "                Fq[qi, si] = np.mean(fluct ** (q / 2)) ** (1 / q)\n",
    "\n",
    "    # Generalized Hurst exponents h(q)\n",
    "    log_s = np.log(scales)\n",
    "    hq = np.zeros(len(q_list))\n",
    "    for qi in range(len(q_list)):\n",
    "        valid = Fq[qi] > 0\n",
    "        if valid.sum() >= 3:\n",
    "            hq[qi] = np.polyfit(log_s[valid], np.log(Fq[qi][valid] + 1e-20), 1)[0]\n",
    "\n",
    "    # Multifractal spectrum\n",
    "    tau_q = q_list * hq - 1\n",
    "    alpha_holder = np.gradient(tau_q, q_list)\n",
    "\n",
    "    delta_alpha = float(alpha_holder.max() - alpha_holder.min())\n",
    "\n",
    "    # H(2) = standard Hurst\n",
    "    idx_q2 = np.argmin(np.abs(q_list - 2))\n",
    "    hurst2_val = float(hq[idx_q2])\n",
    "\n",
    "    # Asymmetry: left vs right tail\n",
    "    mid = len(alpha_holder) // 2\n",
    "    left_width = alpha_holder[mid] - alpha_holder[0] if mid > 0 else 0\n",
    "    right_width = alpha_holder[-1] - alpha_holder[mid] if mid < len(alpha_holder) - 1 else 0\n",
    "    asym = float((left_width - right_width) / (delta_alpha + 1e-10))\n",
    "\n",
    "    return {'delta_alpha': delta_alpha, 'hurst2': hurst2_val, 'asymmetry': asym}\n",
    "\n",
    "\n",
    "# ─── Group 19: TDA Persistent Homology ──────────────────────────────────────\n",
    "def compute_tda_features(close: pd.Series, window: int = 50,\n",
    "                         embedding_dim: int = 3,\n",
    "                         embedding_lag: int = 1) -> pd.DataFrame:\n",
    "    \"\"\"Topological Data Analysis features via persistent homology.\n",
    "\n",
    "    Embeds a sliding window of returns into a point cloud using\n",
    "    Takens' time-delay embedding, then computes Vietoris-Rips\n",
    "    persistence. Tracks the lifetime of 1-dimensional topological\n",
    "    features (loops) as crash indicators.\n",
    "\n",
    "    Ref: MDPI Computers 2025, ACM AMMIC 2025\n",
    "\n",
    "    CAUSAL: uses [t-W, t] only.\n",
    "    \"\"\"\n",
    "    log_ret = np.log(close / close.shift(1)).fillna(0).values\n",
    "    n = len(log_ret)\n",
    "\n",
    "    pers_norm = np.full(n, np.nan)\n",
    "    betti_ratio = np.full(n, np.nan)\n",
    "\n",
    "    embed_size = embedding_dim + (embedding_dim - 1) * embedding_lag\n",
    "\n",
    "    for t in range(max(window, embed_size + 10), n):\n",
    "        segment = log_ret[t - window:t]\n",
    "\n",
    "        try:\n",
    "            # Takens time-delay embedding\n",
    "            rows = []\n",
    "            for i in range(len(segment) - embed_size + 1):\n",
    "                row = [segment[i + j * embedding_lag]\n",
    "                       for j in range(embedding_dim)]\n",
    "                rows.append(row)\n",
    "            cloud = np.array(rows)\n",
    "\n",
    "            if len(cloud) < 5:\n",
    "                continue\n",
    "\n",
    "            if HAS_TDA:\n",
    "                # Use giotto-tda for persistence\n",
    "                vrp = VietorisRipsPersistence(\n",
    "                    homology_dimensions=[0, 1], max_edge_length=2.0,\n",
    "                    n_jobs=1\n",
    "                )\n",
    "                diagrams = vrp.fit_transform(cloud[np.newaxis, :, :])\n",
    "                dgm = diagrams[0]\n",
    "\n",
    "                # H0: connected components, H1: loops\n",
    "                h0 = dgm[dgm[:, 2] == 0]\n",
    "                h1 = dgm[dgm[:, 2] == 1]\n",
    "\n",
    "                # Filter out infinite persistence\n",
    "                h0_finite = h0[np.isfinite(h0[:, 1])]\n",
    "                h1_finite = h1[np.isfinite(h1[:, 1])]\n",
    "\n",
    "                # L2 norm of H1 persistence (loop lifetimes)\n",
    "                if len(h1_finite) > 0:\n",
    "                    lifetimes = h1_finite[:, 1] - h1_finite[:, 0]\n",
    "                    pers_norm[t] = float(np.sqrt(np.sum(lifetimes ** 2)))\n",
    "                else:\n",
    "                    pers_norm[t] = 0.0\n",
    "\n",
    "                # Betti ratio: number of loops / number of components\n",
    "                n_h0 = max(1, len(h0_finite))\n",
    "                n_h1 = len(h1_finite)\n",
    "                betti_ratio[t] = float(n_h1 / n_h0)\n",
    "            else:\n",
    "                # Fallback: simple distance-based proxy\n",
    "                from scipy.spatial.distance import pdist\n",
    "                dists = pdist(cloud)\n",
    "                # Approximate topological complexity from distance distribution\n",
    "                pers_norm[t] = float(np.std(dists))\n",
    "                betti_ratio[t] = float(np.mean(dists < np.median(dists)))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "    result['tda_persistence_norm'] = _rolling_zscore(pers_norm, 126)\n",
    "    result['tda_betti_ratio'] = betti_ratio\n",
    "    return result\n",
    "\n",
    "\n",
    "# ─── Master: Build All Advanced Features ─────────────────────────────────────\n",
    "def build_advanced_features(df: pd.DataFrame, close: pd.Series,\n",
    "                            cfg) -> pd.DataFrame:\n",
    "    \"\"\"Build all 16 advanced features (Groups 15-19).\n",
    "\n",
    "    Args:\n",
    "        df: OHLCV DataFrame\n",
    "        close: close price Series\n",
    "        cfg: MonolithConfig\n",
    "\n",
    "    Returns: DataFrame with ADVANCED_FEATURE_COLUMNS\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    print(f\"\\n  Building {len(ADVANCED_FEATURE_COLUMNS)} advanced features...\")\n",
    "\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Group 15: HMM Regime (3 features)\n",
    "    print(\"  [15/19] HMM Regime Detection (3 features)\")\n",
    "    hmm_df = compute_hmm_regime(close, n_states=3, window=252)\n",
    "    for col in ['hmm_regime', 'hmm_regime_prob', 'hmm_regime_duration']:\n",
    "        result[col] = hmm_df[col] if col in hmm_df.columns else np.nan\n",
    "\n",
    "    # Group 16: Wavelet Decomposition (4 features)\n",
    "    print(\"  [16/19] Wavelet Decomposition (4 features)\")\n",
    "    wav_df = compute_wavelet_features(close, wavelet='db4', level=4, window=63)\n",
    "    for col in ['wavelet_trend_energy', 'wavelet_detail_energy',\n",
    "                'wavelet_snr', 'wavelet_dominant_scale']:\n",
    "        result[col] = wav_df[col] if col in wav_df.columns else np.nan\n",
    "\n",
    "    # Group 17: Information Theory (4 features)\n",
    "    print(\"  [17/19] Information Theory Advanced (4 features)\")\n",
    "    info_df = compute_info_theory_features(close)\n",
    "    for col in ['kl_regime_shift', 'sample_entropy',\n",
    "                'nmi_predictability', 'spectral_entropy']:\n",
    "        result[col] = info_df[col] if col in info_df.columns else np.nan\n",
    "\n",
    "    # Group 18: Multifractal DFA (3 features)\n",
    "    print(\"  [18/19] Multifractal DFA (3 features)\")\n",
    "    mf_df = compute_mfdfa_features(close, window=252)\n",
    "    for col in ['mf_delta_alpha', 'mf_hurst2', 'mf_asymmetry']:\n",
    "        result[col] = mf_df[col] if col in mf_df.columns else np.nan\n",
    "\n",
    "    # Group 19: TDA Persistent Homology (2 features)\n",
    "    print(\"  [19/19] TDA Persistent Homology (2 features)\")\n",
    "    tda_df = compute_tda_features(close, window=50, embedding_dim=3)\n",
    "    for col in ['tda_persistence_norm', 'tda_betti_ratio']:\n",
    "        result[col] = tda_df[col] if col in tda_df.columns else np.nan\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    n_valid = result.notna().any(axis=1).sum()\n",
    "    print(f\"\\n  Advanced features: {len(ADVANCED_FEATURE_COLUMNS)} features, \"\n",
    "          f\"{n_valid} valid days ({elapsed:.1f}s)\")\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9f1121",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:36:22.813203Z",
     "iopub.status.busy": "2026-02-14T16:36:22.813050Z",
     "iopub.status.idle": "2026-02-14T16:36:22.827395Z",
     "shell.execute_reply": "2026-02-14T16:36:22.826734Z"
    },
    "papermill": {
     "duration": 0.021354,
     "end_time": "2026-02-14T16:36:22.827944",
     "exception": false,
     "start_time": "2026-02-14T16:36:22.806590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AFML EVENT-DRIVEN PIPELINE\n",
      "  Functions defined: get_daily_vol, cusum_filter, triple_barrier_labels,\n",
      "                     meta_labeling, bet_size\n",
      "  These will be applied during walk-forward training in Cell 6.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: AFML EVENT-DRIVEN PIPELINE (Lopez de Prado, 2018)\n",
    "# ============================================================================\n",
    "# Implements: CUSUM filter, Triple Barrier Labels, Meta-Labeling, Bet Sizing\n",
    "# Reference: Advances in Financial Machine Learning, Chapters 2-3, 5\n",
    "# ============================================================================\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "def get_daily_vol(close, span=50):\n",
    "    \"\"\"EWM standard deviation of log returns.\n",
    "    \n",
    "    Args:\n",
    "        close: pd.Series of prices indexed by datetime.\n",
    "        span: int, span for exponential weighted moving average.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series of daily volatility estimates.\n",
    "    \"\"\"\n",
    "    log_ret = np.log(close).diff().dropna()\n",
    "    return log_ret.ewm(span=span, min_periods=max(1, span // 2)).std()\n",
    "\n",
    "\n",
    "def cusum_filter(close, threshold):\n",
    "    \"\"\"Symmetric CUSUM filter for event detection.\n",
    "    \n",
    "    Detects structural breaks by tracking positive and negative cumulative\n",
    "    sums of log returns. When either sum exceeds the threshold, an event\n",
    "    is recorded and the cumulative sum resets to zero.\n",
    "    \n",
    "    Args:\n",
    "        close: pd.Series of prices indexed by datetime.\n",
    "        threshold: float or pd.Series. If Series, must share close's index.\n",
    "                   Typical usage: daily_vol * multiplier (e.g., 2.0).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DatetimeIndex of event timestamps where structural breaks detected.\n",
    "    \"\"\"\n",
    "    log_ret = np.log(close).diff().dropna()\n",
    "    events = []\n",
    "    s_pos = 0.0\n",
    "    s_neg = 0.0\n",
    "    \n",
    "    # Convert scalar threshold to Series for uniform handling\n",
    "    if isinstance(threshold, (int, float)):\n",
    "        thresh_series = pd.Series(threshold, index=log_ret.index)\n",
    "    else:\n",
    "        thresh_series = threshold.reindex(log_ret.index, method='ffill')\n",
    "    \n",
    "    for t, r in log_ret.items():\n",
    "        h = thresh_series.loc[t]\n",
    "        if np.isnan(h) or np.isnan(r):\n",
    "            continue\n",
    "        \n",
    "        # Update cumulative sums (reset floor at zero)\n",
    "        s_pos = max(0.0, s_pos + r)\n",
    "        s_neg = min(0.0, s_neg + r)\n",
    "        \n",
    "        # Check if either sum breaches threshold\n",
    "        if s_pos > h:\n",
    "            events.append(t)\n",
    "            s_pos = 0.0\n",
    "            s_neg = 0.0\n",
    "        elif s_neg < -h:\n",
    "            events.append(t)\n",
    "            s_pos = 0.0\n",
    "            s_neg = 0.0\n",
    "    \n",
    "    return pd.DatetimeIndex(events)\n",
    "\n",
    "\n",
    "def triple_barrier_labels(close, events, pt_sl=(2.0, 1.0), num_days=10, min_ret=0.0):\n",
    "    \"\"\"Triple barrier labeling with volatility-scaled barriers.\n",
    "    \n",
    "    For each event, sets three barriers:\n",
    "      - Upper (profit-take): pt_sl[0] * daily_vol above entry\n",
    "      - Lower (stop-loss):  -pt_sl[1] * daily_vol below entry\n",
    "      - Vertical:            num_days forward (max holding period)\n",
    "    \n",
    "    The label is determined by which barrier is touched first.\n",
    "    \n",
    "    Args:\n",
    "        close: pd.Series of prices indexed by datetime.\n",
    "        events: pd.DatetimeIndex of event timestamps (from cusum_filter).\n",
    "        pt_sl: tuple (profit_take_mult, stop_loss_mult) of daily vol.\n",
    "               Set either to 0 to disable that barrier.\n",
    "        num_days: int, maximum holding period in trading days.\n",
    "        min_ret: float, minimum absolute log return for a non-zero label\n",
    "                 when the vertical barrier is hit first.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame indexed by event timestamps with columns:\n",
    "            'ret':   realized log return at exit\n",
    "            'bin':   label (+1 profit-take, -1 stop-loss, 0 vertical/below min_ret)\n",
    "            't_end': timestamp when position was closed\n",
    "    \"\"\"\n",
    "    daily_vol = get_daily_vol(close, span=50)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for t0 in events:\n",
    "        if t0 not in close.index or t0 not in daily_vol.index:\n",
    "            continue\n",
    "        \n",
    "        vol = daily_vol.loc[t0]\n",
    "        if np.isnan(vol) or vol <= 0:\n",
    "            continue\n",
    "        \n",
    "        p0 = close.loc[t0]\n",
    "        \n",
    "        # Define barrier levels\n",
    "        upper_barrier = pt_sl[0] * vol if pt_sl[0] > 0 else np.inf\n",
    "        lower_barrier = -pt_sl[1] * vol if pt_sl[1] > 0 else -np.inf\n",
    "        \n",
    "        # Get the forward price path from t0 up to t0 + num_days bars\n",
    "        t0_idx = close.index.get_loc(t0)\n",
    "        t_end_idx = min(t0_idx + num_days, len(close) - 1)\n",
    "        path = close.iloc[t0_idx: t_end_idx + 1]\n",
    "        \n",
    "        if len(path) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Log returns relative to entry\n",
    "        log_returns = np.log(path / p0)\n",
    "        \n",
    "        # Find first crossing of each barrier\n",
    "        hit_upper = log_returns[log_returns >= upper_barrier].index\n",
    "        hit_lower = log_returns[log_returns <= lower_barrier].index\n",
    "        \n",
    "        # Determine first touch times (use NaT for unhit barriers)\n",
    "        t_upper = hit_upper[0] if len(hit_upper) > 0 else pd.NaT\n",
    "        t_lower = hit_lower[0] if len(hit_lower) > 0 else pd.NaT\n",
    "        t_vert = path.index[-1]  # vertical barrier always exists\n",
    "        \n",
    "        # Find earliest barrier touch\n",
    "        candidates = {}\n",
    "        if not pd.isna(t_upper):\n",
    "            candidates[t_upper] = 'upper'\n",
    "        if not pd.isna(t_lower):\n",
    "            candidates[t_lower] = 'lower'\n",
    "        candidates[t_vert] = 'vertical'\n",
    "        \n",
    "        t_end = min(candidates.keys())\n",
    "        barrier_type = candidates[t_end]\n",
    "        \n",
    "        # Realized log return at exit\n",
    "        ret = np.log(close.loc[t_end] / p0)\n",
    "        \n",
    "        # Assign label\n",
    "        if barrier_type == 'upper':\n",
    "            label = 1\n",
    "        elif barrier_type == 'lower':\n",
    "            label = -1\n",
    "        else:\n",
    "            # Vertical barrier: label based on return direction if above min_ret\n",
    "            if abs(ret) > min_ret:\n",
    "                label = int(np.sign(ret))\n",
    "            else:\n",
    "                label = 0\n",
    "        \n",
    "        results.append({'t0': t0, 'ret': ret, 'bin': label, 't_end': t_end})\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        return pd.DataFrame(columns=['ret', 'bin', 't_end'])\n",
    "    \n",
    "    df = pd.DataFrame(results).set_index('t0')\n",
    "    df.index.name = None\n",
    "    return df\n",
    "\n",
    "\n",
    "def meta_labeling(primary_preds, true_labels):\n",
    "    \"\"\"Generate meta-labels: 1 if primary model correctly predicted direction.\n",
    "    \n",
    "    Meta-labeling separates side (direction) from size (confidence).\n",
    "    The primary model decides direction; the meta-model decides whether\n",
    "    to take the trade and how large to size it.\n",
    "    \n",
    "    Args:\n",
    "        primary_preds: pd.Series of predicted directions (+1/-1).\n",
    "        true_labels: pd.Series of actual directions (+1/-1/0).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series of 0/1 meta-labels. 1 = primary model was correct.\n",
    "    \"\"\"\n",
    "    # Align indices\n",
    "    common_idx = primary_preds.index.intersection(true_labels.index)\n",
    "    preds = primary_preds.loc[common_idx]\n",
    "    actuals = true_labels.loc[common_idx]\n",
    "    \n",
    "    # Meta-label: 1 if signs agree (correct prediction), 0 otherwise\n",
    "    # A zero true label means the trade was unprofitable, so meta-label = 0\n",
    "    meta = (np.sign(preds) == np.sign(actuals)).astype(int)\n",
    "    \n",
    "    # If true label is 0, the trade had no edge -> meta = 0\n",
    "    meta[actuals == 0] = 0\n",
    "    \n",
    "    return meta\n",
    "\n",
    "\n",
    "def bet_size(meta_probs, max_leverage=1.0):\n",
    "    \"\"\"Probit-based position sizing from meta-model probabilities.\n",
    "    \n",
    "    Converts P(correct) from the meta-model into a continuous position\n",
    "    size using the inverse normal CDF (probit function). This maps\n",
    "    probabilities smoothly to position sizes with natural concavity\n",
    "    near 0 and 1.\n",
    "    \n",
    "    Args:\n",
    "        meta_probs: pd.Series of P(correct) from meta-model, in [0, 1].\n",
    "        max_leverage: float, maximum absolute position size.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series of position sizes in [-max_leverage, max_leverage].\n",
    "        Values near 0.5 produce small sizes; values near 0 or 1 produce\n",
    "        sizes approaching max_leverage.\n",
    "    \"\"\"\n",
    "    # Clip to avoid infinities from norm.ppf at 0 and 1\n",
    "    clipped = meta_probs.clip(1e-5, 1.0 - 1e-5)\n",
    "    \n",
    "    # Probit transform: map probability to z-score\n",
    "    z = pd.Series(norm.ppf(clipped.values), index=clipped.index)\n",
    "    \n",
    "    # Convert z-score to position size via CDF\n",
    "    # size = (2 * Phi(z) - 1) maps z to [-1, 1]\n",
    "    # z > 0 (prob > 0.5) -> positive size, z < 0 (prob < 0.5) -> negative size\n",
    "    size = (2.0 * norm.cdf(z) - 1.0) * max_leverage\n",
    "    \n",
    "    return size\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"AFML EVENT-DRIVEN PIPELINE\")\n",
    "print(\"  Functions defined: get_daily_vol, cusum_filter, triple_barrier_labels,\")\n",
    "print(\"                     meta_labeling, bet_size\")\n",
    "print(\"  These will be applied during walk-forward training in Cell 6.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "993bc39a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:36:22.840808Z",
     "iopub.status.busy": "2026-02-14T16:36:22.840657Z",
     "iopub.status.idle": "2026-02-14T16:36:46.804651Z",
     "shell.execute_reply": "2026-02-14T16:36:46.803721Z"
    },
    "papermill": {
     "duration": 23.971405,
     "end_time": "2026-02-14T16:36:46.805348",
     "exception": false,
     "start_time": "2026-02-14T16:36:22.833943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "MOMENTUM TRANSFORMER (TFT Architecture)\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1771086982.914809 3754101 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13775 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-14 16:36:27.623696: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model architecture verified:\n",
      "  Input shape:       (16, 20, 8)\n",
      "  Signal shape:      (1, 16, 20, 1)\n",
      "  VSN weights shape: (16, 20, 8, 1)\n",
      "  Attn weights shape:(1, 4, 16, 20, 20)\n",
      "  Signal range:      [-0.9993, 0.9996]\n",
      "\n",
      "  Causal mask check (upper triangle sum): 0.0000000000\n",
      "  Causal masking: VERIFIED\n",
      "\n",
      "  Sharpe loss value: 1.3554\n",
      "  (Negative = model has positive Sharpe)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Training step loss: 1.3554\n",
      "\n",
      "  Total parameters: 69,393\n",
      "\n",
      "  Feature importance (VSN weights, descending):\n",
      "    1. feat_6: 0.1458\n",
      "    2. feat_0: 0.1402\n",
      "    3. feat_7: 0.1305\n",
      "    4. feat_5: 0.1289\n",
      "    5. feat_3: 0.1229\n",
      "\n",
      "  Serialization config keys: ['dropout_rate', 'dtype', 'hidden_size', 'input_size', 'name', 'num_heads', 'output_size', 'time_steps', 'trainable']\n",
      "  Config: time_steps=20, input_size=8, hidden_size=32, num_heads=4\n",
      "\n",
      "======================================================================\n",
      "Momentum Transformer built, tested, and verified.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: MOMENTUM TRANSFORMER (Temporal Fusion Transformer Architecture)\n",
    "# ============================================================================\n",
    "# Implements the full TFT architecture from Lim et al. (2021) adapted for\n",
    "# momentum signal generation with Sharpe ratio loss.\n",
    "# All layers are serializable with proper get_config() methods.\n",
    "# ============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class GluLayer(layers.Layer):\n",
    "    \"\"\"Gated Linear Unit: splits transformation into value and gate streams.\n",
    "    \n",
    "    GLU(x) = Dense_value(x) * sigmoid(Dense_gate(x))\n",
    "    \n",
    "    The gate learns which components of the transformation to pass through,\n",
    "    providing a learnable skip-like mechanism at the feature level.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, dropout_rate=None, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation_name = activation\n",
    "        \n",
    "        # Value stream: optional activation (e.g., ELU for GRN intermediate)\n",
    "        self.dense_value = layers.Dense(hidden_size, activation=activation)\n",
    "        # Gate stream: always sigmoid for gating\n",
    "        self.dense_gate = layers.Dense(hidden_size, activation='sigmoid')\n",
    "        \n",
    "        self.dropout_layer = None\n",
    "        if dropout_rate is not None and dropout_rate > 0:\n",
    "            self.dropout_layer = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        value = self.dense_value(inputs)\n",
    "        gate = self.dense_gate(inputs)\n",
    "        \n",
    "        if self.dropout_layer is not None:\n",
    "            value = self.dropout_layer(value)\n",
    "        \n",
    "        glu_output = value * gate\n",
    "        return glu_output, gate\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'activation': self.activation_name,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class GatedResidualNetwork(layers.Layer):\n",
    "    \"\"\"Gated Residual Network: the core building block of TFT.\n",
    "    \n",
    "    Architecture:\n",
    "        eta_1 = Dense(hidden_size)(input)             [+ Dense(hidden_size)(context) if context]\n",
    "        eta_2 = ELU(eta_1)\n",
    "        eta_1_prime = Dense(hidden_size)(eta_2)\n",
    "        glu_output = GLU(eta_1_prime)\n",
    "        output = LayerNorm(input_skip + glu_output)\n",
    "    \n",
    "    When output_size != input_size, a skip projection is applied to the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size=None, dropout_rate=None,\n",
    "                 context_size=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size or hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        # Primary pathway\n",
    "        self.dense1 = layers.Dense(hidden_size)\n",
    "        self.dense2 = layers.Dense(self.output_size)\n",
    "        self.glu = GluLayer(self.output_size, dropout_rate=dropout_rate)\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "        \n",
    "        # Optional context injection\n",
    "        self.context_dense = None\n",
    "        if context_size is not None:\n",
    "            self.context_dense = layers.Dense(hidden_size, use_bias=False)\n",
    "        \n",
    "        # Skip projection (created dynamically if needed)\n",
    "        self._skip_layer = None\n",
    "        self._skip_built = False\n",
    "    \n",
    "    def call(self, inputs, context=None, return_gate=False):\n",
    "        # Build skip projection on first call if input dim != output_size\n",
    "        if not self._skip_built:\n",
    "            input_dim = inputs.shape[-1]\n",
    "            if input_dim is not None and input_dim != self.output_size:\n",
    "                self._skip_layer = layers.Dense(self.output_size)\n",
    "            self._skip_built = True\n",
    "        \n",
    "        # Skip connection\n",
    "        if self._skip_layer is not None:\n",
    "            skip = self._skip_layer(inputs)\n",
    "        else:\n",
    "            skip = inputs\n",
    "        \n",
    "        # Primary pathway\n",
    "        eta_1 = self.dense1(inputs)\n",
    "        \n",
    "        # Context injection (additive)\n",
    "        if context is not None and self.context_dense is not None:\n",
    "            eta_1 = eta_1 + self.context_dense(context)\n",
    "        \n",
    "        # ELU activation (using tf.nn.elu as specified)\n",
    "        eta_2 = tf.nn.elu(eta_1)\n",
    "        \n",
    "        # Second dense\n",
    "        eta_1_prime = self.dense2(eta_2)\n",
    "        \n",
    "        # GLU gating\n",
    "        glu_output, gate = self.glu(eta_1_prime)\n",
    "        \n",
    "        # Residual connection + layer norm\n",
    "        output = self.layer_norm(skip + glu_output)\n",
    "        \n",
    "        if return_gate:\n",
    "            return output, gate\n",
    "        return output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'output_size': self.output_size,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'context_size': self.context_size,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class VariableSelectionNetwork(layers.Layer):\n",
    "    \"\"\"Variable Selection Network: soft feature importance via learned weights.\n",
    "    \n",
    "    Each input feature is processed by its own GRN, then a selection GRN\n",
    "    produces softmax weights over features. The output is the weighted\n",
    "    combination of per-feature GRN outputs.\n",
    "    \n",
    "    This is the TFT's primary interpretability mechanism -- the softmax\n",
    "    weights directly indicate feature importance at each timestep.\n",
    "    \n",
    "    Input shape:  (batch, time, num_inputs, hidden_size) -- after embedding\n",
    "    Output shape: (batch, time, hidden_size)\n",
    "    Weights shape: (batch, time, num_inputs, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs, hidden_size, dropout_rate=None,\n",
    "                 context_size=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        # Per-feature GRNs: each processes one feature independently\n",
    "        self.feature_grns = [\n",
    "            GatedResidualNetwork(\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=hidden_size,\n",
    "                dropout_rate=dropout_rate,\n",
    "                name=f\"feature_grn_{i}\"\n",
    "            )\n",
    "            for i in range(num_inputs)\n",
    "        ]\n",
    "        \n",
    "        # Selection GRN: produces weights over features\n",
    "        # Input is flattened features: (batch, time, num_inputs * hidden_size)\n",
    "        # Output: (batch, time, num_inputs) -> softmax -> weights\n",
    "        self.selection_grn = GatedResidualNetwork(\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=num_inputs,\n",
    "            dropout_rate=dropout_rate,\n",
    "            context_size=context_size,\n",
    "            name=\"selection_grn\"\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs, context=None):\n",
    "        # inputs: (batch, time, num_inputs, hidden_size)\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        time_steps = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Flatten for selection GRN: (batch, time, num_inputs * hidden_size)\n",
    "        flattened = tf.reshape(inputs, [batch_size, time_steps,\n",
    "                                        self.num_inputs * self.hidden_size])\n",
    "        \n",
    "        # Selection weights via GRN + softmax\n",
    "        selection_output = self.selection_grn(flattened, context=context)\n",
    "        # selection_output: (batch, time, num_inputs)\n",
    "        weights = tf.nn.softmax(selection_output, axis=-1)\n",
    "        # weights: (batch, time, num_inputs)\n",
    "        weights_expanded = tf.expand_dims(weights, axis=-1)\n",
    "        # weights_expanded: (batch, time, num_inputs, 1)\n",
    "        \n",
    "        # Process each feature through its own GRN\n",
    "        processed_features = []\n",
    "        for i in range(self.num_inputs):\n",
    "            # Extract feature i: (batch, time, hidden_size)\n",
    "            feat_i = inputs[:, :, i, :]\n",
    "            # Apply per-feature GRN\n",
    "            grn_out = self.feature_grns[i](feat_i)\n",
    "            processed_features.append(grn_out)\n",
    "        \n",
    "        # Stack: (batch, time, num_inputs, hidden_size)\n",
    "        stacked = tf.stack(processed_features, axis=2)\n",
    "        \n",
    "        # Weighted combination: (batch, time, hidden_size)\n",
    "        selected = tf.reduce_sum(stacked * weights_expanded, axis=2)\n",
    "        \n",
    "        return selected, weights_expanded\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_inputs': self.num_inputs,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'context_size': self.context_size,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class ScaledDotProductAttention(layers.Layer):\n",
    "    \"\"\"Standard scaled dot-product attention.\n",
    "    \n",
    "    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
    "    \n",
    "    Supports optional causal masking (lower-triangular) to prevent\n",
    "    attending to future positions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dropout_rate=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout_layer = layers.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "    \n",
    "    def call(self, q, k, v, mask=None):\n",
    "        # q, k, v: (batch, ..., seq_len, d_k)\n",
    "        d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        \n",
    "        # Scaled dot product: (batch, ..., seq_q, seq_k)\n",
    "        scores = tf.matmul(q, k, transpose_b=True) / tf.sqrt(d_k)\n",
    "        \n",
    "        # Apply mask (e.g., causal mask): masked positions get -1e9\n",
    "        if mask is not None:\n",
    "            scores = scores + (1.0 - mask) * (-1e9)\n",
    "        \n",
    "        # Softmax over keys\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        \n",
    "        if self.dropout_layer is not None:\n",
    "            attention_weights = self.dropout_layer(attention_weights)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class InterpretableMultiHeadAttention(layers.Layer):\n",
    "    \"\"\"Interpretable Multi-Head Attention from TFT paper.\n",
    "    \n",
    "    Key differences from standard Transformer MHA:\n",
    "    1. All heads SHARE the same value projection (W_v)\n",
    "    2. Head outputs are AVERAGED, not concatenated\n",
    "    \n",
    "    This design enables direct interpretation of attention patterns\n",
    "    because each head attends to the same value representation.\n",
    "    The averaged attention weights can be examined per-head to understand\n",
    "    what temporal patterns the model has learned.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, d_model, dropout_rate=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.d_head = d_model // num_heads\n",
    "        \n",
    "        # Per-head Q and K projections\n",
    "        self.W_q = [layers.Dense(self.d_head, use_bias=False, name=f\"W_q_{i}\")\n",
    "                     for i in range(num_heads)]\n",
    "        self.W_k = [layers.Dense(self.d_head, use_bias=False, name=f\"W_k_{i}\")\n",
    "                     for i in range(num_heads)]\n",
    "        \n",
    "        # SHARED value projection across all heads\n",
    "        self.W_v = layers.Dense(self.d_head, use_bias=False, name=\"W_v_shared\")\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = layers.Dense(d_model, name=\"W_o\")\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        self.attention = ScaledDotProductAttention(dropout_rate=dropout_rate)\n",
    "    \n",
    "    def call(self, q, k, v, mask=None):\n",
    "        # Shared value projection: (batch, seq, d_head)\n",
    "        v_shared = self.W_v(v)\n",
    "        \n",
    "        head_outputs = []\n",
    "        head_attentions = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            # Per-head query and key projections\n",
    "            q_i = self.W_q[i](q)  # (batch, seq_q, d_head)\n",
    "            k_i = self.W_k[i](k)  # (batch, seq_k, d_head)\n",
    "            \n",
    "            # Attention with shared values\n",
    "            attn_output, attn_weights = self.attention(q_i, k_i, v_shared, mask=mask)\n",
    "            head_outputs.append(attn_output)\n",
    "            head_attentions.append(attn_weights)\n",
    "        \n",
    "        # AVERAGE head outputs (not concatenate) -- key TFT design choice\n",
    "        # Stack: (num_heads, batch, seq, d_head)\n",
    "        stacked_outputs = tf.stack(head_outputs, axis=0)\n",
    "        averaged = tf.reduce_mean(stacked_outputs, axis=0)  # (batch, seq, d_head)\n",
    "        \n",
    "        # Output projection back to d_model\n",
    "        output = self.W_o(averaged)  # (batch, seq, d_model)\n",
    "        \n",
    "        # Stack attention weights for interpretability: (batch, num_heads, seq_q, seq_k)\n",
    "        stacked_attentions = tf.stack(head_attentions, axis=1)\n",
    "        \n",
    "        return output, stacked_attentions\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_heads': self.num_heads,\n",
    "            'd_model': self.d_model,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class MomentumTransformer(Model):\n",
    "    \"\"\"Complete Temporal Fusion Transformer for momentum signal generation.\n",
    "    \n",
    "    Architecture flow:\n",
    "    1. Feature Embedding: per-feature Dense projections to hidden_size\n",
    "    2. Variable Selection: learned soft feature importance via VSN\n",
    "    3. LSTM Encoder: capture temporal dependencies\n",
    "    4. Post-LSTM: GRN + GLU gate + skip + LayerNorm\n",
    "    5. Interpretable Multi-Head Self-Attention with causal mask\n",
    "    6. Post-Attention: GRN + GLU gate + skip + LayerNorm\n",
    "    7. Output: Dense(tanh) -> signal in [-1, 1]\n",
    "    \n",
    "    The model is fully interpretable: VSN weights show feature importance,\n",
    "    attention weights show temporal dependencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, time_steps, input_size, output_size, hidden_size,\n",
    "                 num_heads, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.time_steps = time_steps\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # 1. Per-feature embedding layers\n",
    "        self.feature_embeddings = [\n",
    "            layers.Dense(hidden_size, name=f\"feat_embed_{i}\")\n",
    "            for i in range(input_size)\n",
    "        ]\n",
    "        \n",
    "        # 2. Variable Selection Network\n",
    "        self.vsn = VariableSelectionNetwork(\n",
    "            num_inputs=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=\"vsn\"\n",
    "        )\n",
    "        \n",
    "        # 3. LSTM Encoder\n",
    "        self.lstm = layers.LSTM(\n",
    "            hidden_size,\n",
    "            return_sequences=True,\n",
    "            dropout=dropout_rate if dropout_rate else 0.0,\n",
    "            name=\"lstm_encoder\"\n",
    "        )\n",
    "        \n",
    "        # 4. Post-LSTM processing\n",
    "        self.post_lstm_grn = GatedResidualNetwork(\n",
    "            hidden_size=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=\"post_lstm_grn\"\n",
    "        )\n",
    "        self.post_lstm_glu = GluLayer(\n",
    "            hidden_size=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=\"post_lstm_glu\"\n",
    "        )\n",
    "        self.post_lstm_norm = layers.LayerNormalization(name=\"post_lstm_norm\")\n",
    "        \n",
    "        # 5. Interpretable Multi-Head Self-Attention\n",
    "        self.mha = InterpretableMultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            d_model=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=\"interpretable_mha\"\n",
    "        )\n",
    "        \n",
    "        # 6. Post-attention processing\n",
    "        self.post_attn_grn = GatedResidualNetwork(\n",
    "            hidden_size=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=\"post_attn_grn\"\n",
    "        )\n",
    "        self.post_attn_glu = GluLayer(\n",
    "            hidden_size=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=\"post_attn_glu\"\n",
    "        )\n",
    "        self.post_attn_norm = layers.LayerNormalization(name=\"post_attn_norm\")\n",
    "        \n",
    "        # 7. Output layer\n",
    "        self.output_dense = layers.Dense(\n",
    "            output_size,\n",
    "            activation='tanh',\n",
    "            name=\"output_signal\"\n",
    "        )\n",
    "    \n",
    "    def call(self, x, return_weights=False):\n",
    "        # x shape: (batch, time_steps, input_size)\n",
    "        \n",
    "        # --- 1. Feature Embedding ---\n",
    "        # Project each feature to hidden_size independently\n",
    "        embedded = [self.feature_embeddings[i](x[:, :, i:i+1])\n",
    "                     for i in range(self.input_size)]\n",
    "        # Stack: (batch, time, num_features, hidden_size)\n",
    "        embedded = tf.stack(embedded, axis=2)\n",
    "        \n",
    "        # --- 2. Variable Selection ---\n",
    "        vsn_output, vsn_weights = self.vsn(embedded)\n",
    "        # vsn_output: (batch, time, hidden_size)\n",
    "        \n",
    "        # --- 3. LSTM Encoder ---\n",
    "        lstm_output = self.lstm(vsn_output)\n",
    "        # lstm_output: (batch, time, hidden_size)\n",
    "        \n",
    "        # --- 4. Post-LSTM: GRN + GLU gate + residual + LayerNorm ---\n",
    "        post_lstm = self.post_lstm_grn(lstm_output)\n",
    "        post_lstm_gated, _ = self.post_lstm_glu(post_lstm)\n",
    "        post_lstm_out = self.post_lstm_norm(vsn_output + post_lstm_gated)\n",
    "        \n",
    "        # --- 5. Causal Self-Attention ---\n",
    "        seq_len = tf.shape(post_lstm_out)[1]\n",
    "        causal_mask = tf.linalg.band_part(\n",
    "            tf.ones([seq_len, seq_len]), -1, 0\n",
    "        )  # Lower triangular: position i can attend to positions <= i\n",
    "        causal_mask = tf.expand_dims(tf.expand_dims(causal_mask, 0), 0)\n",
    "        # (1, 1, seq, seq) -- broadcasts over batch and heads\n",
    "        \n",
    "        attn_output, attn_weights = self.mha(\n",
    "            post_lstm_out, post_lstm_out, post_lstm_out, mask=causal_mask\n",
    "        )\n",
    "        \n",
    "        # --- 6. Post-Attention: GRN + GLU gate + residual + LayerNorm ---\n",
    "        post_attn = self.post_attn_grn(attn_output)\n",
    "        post_attn_gated, _ = self.post_attn_glu(post_attn)\n",
    "        post_attn_out = self.post_attn_norm(post_lstm_out + post_attn_gated)\n",
    "        \n",
    "        # --- 7. Output signal ---\n",
    "        signal = self.output_dense(post_attn_out)\n",
    "        # signal: (batch, time, output_size) with values in [-1, 1]\n",
    "        \n",
    "        if return_weights:\n",
    "            return signal, {\n",
    "                'vsn_weights': vsn_weights,   # (batch, time, num_inputs, 1)\n",
    "                'attn_weights': attn_weights,  # (batch, num_heads, seq, seq)\n",
    "            }\n",
    "        return signal\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'time_steps': self.time_steps,\n",
    "            'input_size': self.input_size,\n",
    "            'output_size': self.output_size,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class SharpeLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Negative annualized Sharpe ratio as differentiable loss function.\n",
    "    \n",
    "    loss = -(mean(signal * return) / std(signal * return)) * sqrt(252)\n",
    "    \n",
    "    Uses ddof=1 (Bessel's correction) for unbiased sample standard deviation.\n",
    "    The model learns to produce signals that maximize risk-adjusted returns\n",
    "    directly, rather than optimizing a proxy like MSE.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_size=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_true: actual returns, y_pred: predicted signals\n",
    "        # Both: (batch, time, 1) or (batch, time)\n",
    "        strategy_returns = y_pred * y_true\n",
    "        \n",
    "        # Flatten to compute portfolio-level Sharpe\n",
    "        strategy_returns = tf.reshape(strategy_returns, [-1])\n",
    "        \n",
    "        mean_ret = tf.reduce_mean(strategy_returns)\n",
    "        n = tf.cast(tf.size(strategy_returns), tf.float32)\n",
    "        \n",
    "        # Unbiased variance with ddof=1: sum((x - mean)^2) / (n - 1)\n",
    "        var = tf.reduce_sum(tf.square(strategy_returns - mean_ret)) / (n - 1.0)\n",
    "        std = tf.sqrt(var + 1e-9)  # Small epsilon for numerical stability\n",
    "        \n",
    "        # Negative annualized Sharpe (negative because we minimize loss)\n",
    "        sharpe = (mean_ret / std) * tf.sqrt(252.0)\n",
    "        return -sharpe\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'output_size': self.output_size,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# --- Build and verify the model ---\n",
    "print(\"=\" * 70)\n",
    "print(\"MOMENTUM TRANSFORMER (TFT Architecture)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration\n",
    "TIME_STEPS = 20\n",
    "INPUT_SIZE = 8    # Number of features\n",
    "OUTPUT_SIZE = 1   # Single signal output\n",
    "HIDDEN_SIZE = 32  # Hidden dimension\n",
    "NUM_HEADS = 4     # Attention heads\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "# Build model\n",
    "model = MomentumTransformer(\n",
    "    time_steps=TIME_STEPS,\n",
    "    input_size=INPUT_SIZE,\n",
    "    output_size=OUTPUT_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "# Test forward pass with dummy data\n",
    "dummy_x = tf.random.normal([16, TIME_STEPS, INPUT_SIZE])\n",
    "dummy_y = tf.random.normal([16, TIME_STEPS, OUTPUT_SIZE])\n",
    "\n",
    "# Forward pass with weights\n",
    "signal, weights = model(dummy_x, return_weights=True)\n",
    "print(f\"\\nModel architecture verified:\")\n",
    "print(f\"  Input shape:       {dummy_x.shape}\")\n",
    "print(f\"  Signal shape:      {signal.shape}\")\n",
    "print(f\"  VSN weights shape: {weights['vsn_weights'].shape}\")\n",
    "print(f\"  Attn weights shape:{weights['attn_weights'].shape}\")\n",
    "print(f\"  Signal range:      [{tf.reduce_min(signal):.4f}, {tf.reduce_max(signal):.4f}]\")\n",
    "\n",
    "# Verify causal masking: attention at position i should have zero weight for j > i\n",
    "attn = weights['attn_weights'][0, 0].numpy()  # First sample, first head\n",
    "upper_triangle_sum = np.triu(attn, k=1).sum()\n",
    "print(f\"\\n  Causal mask check (upper triangle sum): {upper_triangle_sum:.10f}\")\n",
    "assert upper_triangle_sum < 1e-6, \"Causal masking is broken!\"\n",
    "print(f\"  Causal masking: VERIFIED\")\n",
    "\n",
    "# Test Sharpe loss\n",
    "loss_fn = SharpeLoss(output_size=OUTPUT_SIZE)\n",
    "loss_val = loss_fn(dummy_y, signal)\n",
    "print(f\"\\n  Sharpe loss value: {loss_val:.4f}\")\n",
    "print(f\"  (Negative = model has positive Sharpe)\")\n",
    "\n",
    "# Compile and do one training step to verify gradients flow\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=SharpeLoss(output_size=OUTPUT_SIZE)\n",
    ")\n",
    "\n",
    "# Single training step\n",
    "history = model.fit(dummy_x, dummy_y, epochs=1, batch_size=16, verbose=0)\n",
    "print(f\"  Training step loss: {history.history['loss'][0]:.4f}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\n  Total parameters: {total_params:,}\")\n",
    "\n",
    "# VSN feature importance (interpretability demo)\n",
    "vsn_w = weights['vsn_weights'].numpy()\n",
    "mean_importance = vsn_w.mean(axis=(0, 1)).flatten()\n",
    "feature_names = [f\"feat_{i}\" for i in range(INPUT_SIZE)]\n",
    "importance_order = np.argsort(-mean_importance)\n",
    "print(f\"\\n  Feature importance (VSN weights, descending):\")\n",
    "for rank, idx in enumerate(importance_order[:5]):\n",
    "    print(f\"    {rank+1}. {feature_names[idx]}: {mean_importance[idx]:.4f}\")\n",
    "\n",
    "# Serialization verification\n",
    "config = model.get_config()\n",
    "print(f\"\\n  Serialization config keys: {sorted(config.keys())}\")\n",
    "print(f\"  Config: time_steps={config.get('time_steps')}, \"\n",
    "      f\"input_size={config.get('input_size')}, \"\n",
    "      f\"hidden_size={config.get('hidden_size')}, \"\n",
    "      f\"num_heads={config.get('num_heads')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Momentum Transformer built, tested, and verified.\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# NOTE: Enhanced Multi-Aspect Attention (EMAT) and Multi-Objective Loss\n",
    "# are defined in the next cell. The MomentumTransformer above can be used\n",
    "# as-is, or replaced with the EMAT-enhanced version for v3 training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdbeb86a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:36:46.819762Z",
     "iopub.status.busy": "2026-02-14T16:36:46.819579Z",
     "iopub.status.idle": "2026-02-14T16:36:46.832893Z",
     "shell.execute_reply": "2026-02-14T16:36:46.832067Z"
    },
    "papermill": {
     "duration": 0.021335,
     "end_time": "2026-02-14T16:36:46.833484",
     "exception": false,
     "start_time": "2026-02-14T16:36:46.812149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Enhanced Multi-Aspect Attention (EMAT) & Multi-Objective Loss\n",
    "# ============================================================================\n",
    "# Replaces standard Interpretable Multi-Head Attention with 3 specialized\n",
    "# financial attention heads:\n",
    "#   1. Temporal Decay Attention: exponential decay weighting (recent > old)\n",
    "#   2. Trend Attention: attention over differenced (momentum) features\n",
    "#   3. Volatility Attention: attention over rolling variance features\n",
    "#\n",
    "# Multi-Objective Loss: Sharpe + Volatility Consistency + Drawdown Penalty\n",
    "#\n",
    "# Ref: \"EMAT: Enhanced Multi-Aspect Attention Transformer for Financial\n",
    "#       Time Series Forecasting\" (MDPI Entropy 2025)\n",
    "# ============================================================================\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class TemporalDecayAttention(layers.Layer):\n",
    "    \"\"\"Attention with exponential temporal decay.\n",
    "\n",
    "    Applies exp(-lambda * |i-j|) weighting to attention scores,\n",
    "    so recent time steps get higher attention regardless of content.\n",
    "    lambda is learnable per head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W_q = layers.Dense(self.d_model, use_bias=False, name=\"tda_Wq\")\n",
    "        self.W_k = layers.Dense(self.d_model, use_bias=False, name=\"tda_Wk\")\n",
    "        self.W_v = layers.Dense(self.d_model, use_bias=False, name=\"tda_Wv\")\n",
    "        # Learnable decay rate (initialized to ~0.1)\n",
    "        self.decay_rate = self.add_weight(\n",
    "            name=\"decay_rate\", shape=(1,),\n",
    "            initializer=tf.keras.initializers.Constant(0.1),\n",
    "            trainable=True\n",
    "        )\n",
    "        self.dropout = layers.Dropout(dropout_rate)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None, training=None):\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "\n",
    "        # Standard scaled dot-product scores\n",
    "        d_k = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "        scores = tf.matmul(Q, K, transpose_b=True) / tf.sqrt(d_k)\n",
    "\n",
    "        # Temporal decay bias: exp(-lambda * |i - j|)\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        positions = tf.cast(tf.range(seq_len), tf.float32)\n",
    "        dist_matrix = tf.abs(positions[:, None] - positions[None, :])\n",
    "        decay_bias = -tf.abs(self.decay_rate) * dist_matrix\n",
    "        scores = scores + decay_bias[None, :, :]\n",
    "\n",
    "        if mask is not None:\n",
    "            scores += (1.0 - tf.cast(mask, tf.float32)) * -1e9\n",
    "\n",
    "        weights = tf.nn.softmax(scores, axis=-1)\n",
    "        weights = self.dropout(weights, training=training)\n",
    "        output = tf.matmul(weights, V)\n",
    "        return output, weights\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'d_model': self.d_model, 'dropout_rate': self.dropout_rate})\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class MultiObjectiveSharpeLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Multi-objective loss: Sharpe + Volatility Consistency + Drawdown.\n",
    "\n",
    "    L = -Sharpe + alpha * VolConsistency + beta * MaxDrawdown\n",
    "\n",
    "    - Sharpe: annualized (sqrt(252), ddof=1)\n",
    "    - VolConsistency: penalizes return sequences with unstable volatility\n",
    "    - MaxDrawdown: penalizes large drawdowns during training\n",
    "\n",
    "    alpha=0.1, beta=0.2 recommended (Sharpe-dominant with DD protection).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha=0.1, beta=0.2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Strategy returns: signal * actual returns\n",
    "        signal = tf.squeeze(y_pred, axis=-1) if len(y_pred.shape) > 1 else y_pred\n",
    "        actual = tf.squeeze(y_true, axis=-1) if len(y_true.shape) > 1 else y_true\n",
    "        strategy_ret = signal * actual\n",
    "\n",
    "        # Sharpe component (primary objective)\n",
    "        mean_r = tf.reduce_mean(strategy_ret)\n",
    "        std_r = tf.math.reduce_std(strategy_ret) + 1e-8\n",
    "        sharpe = tf.sqrt(252.0) * mean_r / std_r\n",
    "\n",
    "        # Volatility consistency: std of rolling vol shouldn't be too high\n",
    "        # Approximation: variance of squared returns (proxy for vol-of-vol)\n",
    "        sq_ret = strategy_ret ** 2\n",
    "        vol_consistency = tf.math.reduce_std(sq_ret) / (tf.reduce_mean(sq_ret) + 1e-8)\n",
    "\n",
    "        # Drawdown penalty: compute max drawdown from cumulative returns\n",
    "        cum_ret = tf.cumsum(strategy_ret)\n",
    "        running_max = tf.scan(lambda a, x: tf.maximum(a, x), cum_ret, initializer=cum_ret[0])\n",
    "        drawdowns = cum_ret - running_max\n",
    "        max_dd = -tf.reduce_min(drawdowns)\n",
    "\n",
    "        # Combined loss (minimize)\n",
    "        loss = -sharpe + self.alpha * vol_consistency + self.beta * max_dd\n",
    "        return loss\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({'alpha': self.alpha, 'beta': self.beta})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b581b12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:36:46.847575Z",
     "iopub.status.busy": "2026-02-14T16:36:46.847353Z",
     "iopub.status.idle": "2026-02-14T16:36:46.866426Z",
     "shell.execute_reply": "2026-02-14T16:36:46.865684Z"
    },
    "papermill": {
     "duration": 0.027041,
     "end_time": "2026-02-14T16:36:46.866910",
     "exception": false,
     "start_time": "2026-02-14T16:36:46.839869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# RL Thompson Sampling Meta-Learner\n",
    "# ============================================================================\n",
    "# Inspired by RL-book (Rao & Jelvis, Stanford CME 241) Thompson Sampling\n",
    "# implementation in rl/chapter14/ts_gaussian.py.\n",
    "#\n",
    "# The meta-learner treats each signal source (TFT, Momentum, MR) as a\n",
    "# \"bandit arm\" and uses Thompson Sampling to learn which signal performs\n",
    "# best in the CURRENT regime. The posterior distribution of each arm's\n",
    "# reward (Sharpe) is updated online as new data arrives.\n",
    "#\n",
    "# Key insight: different strategies work in different regimes.\n",
    "# Thompson Sampling automatically discovers this without manual rules.\n",
    "# ============================================================================\n",
    "\n",
    "class ThompsonSamplingMetaLearner:\n",
    "    \"\"\"Thompson Sampling strategy selector with regime conditioning.\n",
    "\n",
    "    Each strategy is modeled as a Gaussian bandit arm with unknown mean\n",
    "    and known variance. Posterior is updated via conjugate Normal-Normal:\n",
    "\n",
    "        Prior:     mu ~ N(mu_0, sigma_0^2)\n",
    "        Likelihood: X | mu ~ N(mu, sigma_obs^2)\n",
    "        Posterior:  mu | X ~ N(mu_n, sigma_n^2)\n",
    "\n",
    "    where:\n",
    "        sigma_n^2 = 1 / (1/sigma_0^2 + n/sigma_obs^2)\n",
    "        mu_n = sigma_n^2 * (mu_0/sigma_0^2 + sum(X)/sigma_obs^2)\n",
    "\n",
    "    Selection: sample from each posterior, pick highest sample.\n",
    "\n",
    "    When regime is provided, we maintain SEPARATE posteriors per regime.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, strategy_names: List[str],\n",
    "                 n_regimes: int = 3,\n",
    "                 prior_mean: float = 0.0,\n",
    "                 prior_std: float = 1.0,\n",
    "                 obs_std: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            strategy_names: names of strategies (bandit arms)\n",
    "            n_regimes: number of regime states\n",
    "            prior_mean: prior mean for each arm\n",
    "            prior_std: prior standard deviation\n",
    "            obs_std: assumed observation noise (daily Sharpe scale)\n",
    "        \"\"\"\n",
    "        self.names = strategy_names\n",
    "        self.n_arms = len(strategy_names)\n",
    "        self.n_regimes = n_regimes\n",
    "        self.obs_var = obs_std ** 2\n",
    "\n",
    "        # Per-regime, per-arm posteriors: (mean, variance, count)\n",
    "        self.posteriors = {}\n",
    "        for r in range(n_regimes):\n",
    "            self.posteriors[r] = {\n",
    "                name: {\n",
    "                    'mean': prior_mean,\n",
    "                    'var': prior_std ** 2,\n",
    "                    'n': 0,\n",
    "                    'sum_rewards': 0.0,\n",
    "                }\n",
    "                for name in strategy_names\n",
    "            }\n",
    "\n",
    "    def select(self, regime: int = 0,\n",
    "               rng: Optional[np.random.Generator] = None) -> str:\n",
    "        \"\"\"Thompson Sampling: sample from posteriors, pick best arm.\n",
    "\n",
    "        Args:\n",
    "            regime: current regime label (0, 1, or 2)\n",
    "            rng: random number generator\n",
    "\n",
    "        Returns: name of selected strategy\n",
    "        \"\"\"\n",
    "        if rng is None:\n",
    "            rng = np.random.default_rng()\n",
    "\n",
    "        regime = int(np.clip(regime, 0, self.n_regimes - 1))\n",
    "\n",
    "        samples = {}\n",
    "        for name in self.names:\n",
    "            post = self.posteriors[regime][name]\n",
    "            # Sample from posterior N(mean, var)\n",
    "            sample = rng.normal(post['mean'], np.sqrt(post['var'] + 1e-10))\n",
    "            samples[name] = sample\n",
    "\n",
    "        return max(samples, key=samples.get)\n",
    "\n",
    "    def get_weights(self, regime: int = 0, n_samples: int = 1000,\n",
    "                    rng: Optional[np.random.Generator] = None) -> Dict[str, float]:\n",
    "        \"\"\"Get selection probabilities by Monte Carlo sampling.\n",
    "\n",
    "        Returns: {strategy_name: probability_of_selection}\n",
    "        \"\"\"\n",
    "        if rng is None:\n",
    "            rng = np.random.default_rng(42)\n",
    "\n",
    "        regime = int(np.clip(regime, 0, self.n_regimes - 1))\n",
    "        counts = {name: 0 for name in self.names}\n",
    "\n",
    "        for _ in range(n_samples):\n",
    "            best = self.select(regime, rng)\n",
    "            counts[best] += 1\n",
    "\n",
    "        total = sum(counts.values())\n",
    "        return {name: count / total for name, count in counts.items()}\n",
    "\n",
    "    def update(self, strategy_name: str, reward: float, regime: int = 0):\n",
    "        \"\"\"Update posterior after observing reward.\n",
    "\n",
    "        Conjugate Normal-Normal update:\n",
    "            new_var = 1 / (1/prior_var + 1/obs_var)\n",
    "            new_mean = new_var * (prior_mean/prior_var + reward/obs_var)\n",
    "        \"\"\"\n",
    "        regime = int(np.clip(regime, 0, self.n_regimes - 1))\n",
    "        post = self.posteriors[regime][strategy_name]\n",
    "\n",
    "        post['n'] += 1\n",
    "        post['sum_rewards'] += reward\n",
    "\n",
    "        # Conjugate update\n",
    "        prior_prec = 1.0 / (post['var'] + 1e-10)\n",
    "        obs_prec = 1.0 / self.obs_var\n",
    "\n",
    "        new_prec = prior_prec + obs_prec\n",
    "        new_var = 1.0 / new_prec\n",
    "        new_mean = new_var * (post['mean'] * prior_prec + reward * obs_prec)\n",
    "\n",
    "        post['mean'] = new_mean\n",
    "        post['var'] = new_var\n",
    "\n",
    "    def summary(self) -> str:\n",
    "        \"\"\"Print posterior summary per regime.\"\"\"\n",
    "        lines = [\"Thompson Sampling Meta-Learner Summary:\"]\n",
    "        regime_labels = {0: 'Bear', 1: 'Sideways', 2: 'Bull'}\n",
    "        for r in range(self.n_regimes):\n",
    "            lines.append(f\"\\n  Regime {r} ({regime_labels.get(r, '?')}):\")\n",
    "            for name in self.names:\n",
    "                p = self.posteriors[r][name]\n",
    "                lines.append(\n",
    "                    f\"    {name:12s}: mean={p['mean']:+.4f}, \"\n",
    "                    f\"std={np.sqrt(p['var']):.4f}, n={p['n']}\"\n",
    "                )\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "def run_thompson_ensemble(oos_signals_dict: Dict[str, np.ndarray],\n",
    "                          oos_returns: np.ndarray,\n",
    "                          regime_labels: np.ndarray,\n",
    "                          strategy_names: List[str],\n",
    "                          bps_cost: float = 0.001,\n",
    "                          warmup: int = 21) -> Tuple[np.ndarray, ThompsonSamplingMetaLearner]:\n",
    "    \"\"\"Run Thompson Sampling ensemble over OOS period.\n",
    "\n",
    "    For each day:\n",
    "      1. Observe current regime\n",
    "      2. Thompson Sampling selects best strategy for this regime\n",
    "      3. Use selected strategy's signal\n",
    "      4. Observe reward, update posterior\n",
    "\n",
    "    Args:\n",
    "        oos_signals_dict: {strategy_name: signal_array}\n",
    "        oos_returns: actual forward returns\n",
    "        regime_labels: regime label for each day (0/1/2)\n",
    "        strategy_names: list of strategy names\n",
    "        bps_cost: transaction cost per position change\n",
    "        warmup: days of equal-weight warmup before Thompson kicks in\n",
    "\n",
    "    Returns: (ensemble_returns, meta_learner)\n",
    "    \"\"\"\n",
    "    n_days = len(oos_returns)\n",
    "    meta = ThompsonSamplingMetaLearner(strategy_names, n_regimes=3)\n",
    "    rng = np.random.default_rng(42)\n",
    "\n",
    "    ens_returns = np.zeros(n_days)\n",
    "    selected_strategies = []\n",
    "    prev_pos = 0.0\n",
    "\n",
    "    for t in range(n_days):\n",
    "        regime = int(regime_labels[t]) if not np.isnan(regime_labels[t]) else 1\n",
    "\n",
    "        if t < warmup:\n",
    "            # Equal weight during warmup\n",
    "            signal = np.mean([\n",
    "                np.sign(oos_signals_dict[s][t])\n",
    "                for s in strategy_names\n",
    "            ])\n",
    "            position = np.sign(signal)\n",
    "        else:\n",
    "            # Thompson Sampling selection\n",
    "            selected = meta.select(regime, rng)\n",
    "            position = np.sign(oos_signals_dict[selected][t])\n",
    "            selected_strategies.append(selected)\n",
    "\n",
    "        # Compute return with costs\n",
    "        strat_ret = position * oos_returns[t]\n",
    "        cost = abs(position - prev_pos) * bps_cost\n",
    "        ens_returns[t] = strat_ret - cost\n",
    "        prev_pos = position\n",
    "\n",
    "        # Update ALL strategies with their hypothetical reward\n",
    "        for s in strategy_names:\n",
    "            s_pos = np.sign(oos_signals_dict[s][t])\n",
    "            s_ret = s_pos * oos_returns[t]\n",
    "            meta.update(s, s_ret, regime)\n",
    "\n",
    "    return ens_returns, meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21bf14a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:36:46.880689Z",
     "iopub.status.busy": "2026-02-14T16:36:46.880531Z",
     "iopub.status.idle": "2026-02-14T16:36:46.908897Z",
     "shell.execute_reply": "2026-02-14T16:36:46.908201Z"
    },
    "papermill": {
     "duration": 0.036085,
     "end_time": "2026-02-14T16:36:46.909425",
     "exception": false,
     "start_time": "2026-02-14T16:36:46.873340",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 6. TRAINING & WALK-FORWARD VALIDATION ENGINE\n",
    "\n",
    "def expanding_normalize(train_df, test_df, feature_cols):\n",
    "    \"\"\"Normalize test data using ONLY training statistics (no look-ahead).\n",
    "\n",
    "    Args:\n",
    "        train_df: DataFrame with training data\n",
    "        test_df: DataFrame with test data\n",
    "        feature_cols: list of feature column names to normalize\n",
    "\n",
    "    Returns: (train_normalized, test_normalized) DataFrames\n",
    "    \"\"\"\n",
    "    means = train_df[feature_cols].mean()\n",
    "    stds = train_df[feature_cols].std()\n",
    "    stds = stds.replace(0, 1.0)  # prevent division by zero\n",
    "    train_norm = train_df.copy()\n",
    "    test_norm = test_df.copy()\n",
    "    train_norm[feature_cols] = (train_df[feature_cols] - means) / stds\n",
    "    test_norm[feature_cols] = (test_df[feature_cols] - means) / stds\n",
    "    return train_norm, test_norm\n",
    "\n",
    "\n",
    "def make_sequences(feature_array, target_array, window_size):\n",
    "    \"\"\"Create (window, n_features) sequences with proper target alignment.\n",
    "\n",
    "    Args:\n",
    "        feature_array: np.ndarray shape (n, n_feat)\n",
    "        target_array: np.ndarray shape (n,) -- target_ret (forward return)\n",
    "        window_size: int W\n",
    "\n",
    "    Returns:\n",
    "        X: (n_valid, W, n_feat) float32\n",
    "        y: (n_valid,) float32\n",
    "        indices: (n_valid,) int -- row indices in the original array for each\n",
    "                 sequence's \"current time\" (i.e., the last row of each window).\n",
    "                 Use these to recover dates: df.index[indices]\n",
    "\n",
    "    Alignment:\n",
    "        X[i] = features[i:i+W] -- \"current time\" is i+W-1\n",
    "        y[i] = target[i+W-1]   -- forward return at that time\n",
    "        indices[i] = i+W-1\n",
    "    \"\"\"\n",
    "    n = len(feature_array)\n",
    "    n_seq = n - window_size\n",
    "    if n_seq <= 0:\n",
    "        return (np.zeros((0, window_size, feature_array.shape[1]), dtype=np.float32),\n",
    "                np.zeros(0, dtype=np.float32),\n",
    "                np.array([], dtype=np.int64))\n",
    "\n",
    "    X = np.array([feature_array[i:i + window_size] for i in range(n_seq)])\n",
    "    y = target_array[window_size - 1:window_size - 1 + n_seq]\n",
    "    indices = np.arange(window_size - 1, window_size - 1 + n_seq, dtype=np.int64)\n",
    "\n",
    "    # Remove NaN targets\n",
    "    valid = ~np.isnan(y)\n",
    "    return X[valid].astype(np.float32), y[valid].astype(np.float32), indices[valid]\n",
    "\n",
    "\n",
    "def _predict_safe(model, X, desc=None):\n",
    "    \"\"\"Run model inference one sample at a time.\n",
    "\n",
    "    Keras 3 (TF 2.20) bakes the batch dimension from the first model() call\n",
    "    into the traced graph. Since MomentumTransformer is built with batch=1\n",
    "    (dummy forward pass), subsequent calls with batch>1 collapse the batch\n",
    "    dimension. Processing one-at-a-time matches the build shape and is\n",
    "    guaranteed correct. Cost: ~5ms per sample, negligible vs training time.\n",
    "\n",
    "    Args:\n",
    "        model: trained MomentumTransformer\n",
    "        X: np.ndarray shape (n_samples, window, n_features)\n",
    "        desc: optional tqdm description\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray shape (n_samples,) -- signal at last timestep for each sequence\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    preds = np.empty(n, dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        out = model(tf.constant(X[i:i+1]), training=False)\n",
    "        preds[i] = float(out[0, -1, 0])\n",
    "    return preds\n",
    "\n",
    "\n",
    "def walk_forward_train(data_dict, cfg):\n",
    "    \"\"\"Universe-mode walk-forward OOS validation with purge gaps.\n",
    "\n",
    "    Trains ONE model per fold on pooled sequences from ALL tickers,\n",
    "    then predicts OOS for each ticker separately. This lets the model\n",
    "    learn cross-asset patterns and benefit from a larger training set.\n",
    "\n",
    "    Fold schedule is date-based: uses the SHORTEST ticker to define\n",
    "    fold boundaries, ensuring all tickers have data for every fold.\n",
    "\n",
    "    Args:\n",
    "        data_dict: {ticker_name: pd.DataFrame with FEATURE_COLUMNS + 'target_ret'}\n",
    "        cfg: MonolithConfig\n",
    "\n",
    "    Returns: dict with:\n",
    "        'oos_returns': pd.DataFrame (index=dates, columns=tickers)\n",
    "        'oos_signals': pd.DataFrame (same shape)\n",
    "        'fold_metrics': list of dicts {ticker, fold, sharpe, n_days, train_days}\n",
    "        'models': {fold_number: last trained model}\n",
    "        'vsn_weights': {'universe': np.ndarray of feature importance}\n",
    "    \"\"\"\n",
    "    tickers = list(data_dict.keys())\n",
    "    n_feat = len(FEATURE_COLUMNS)\n",
    "\n",
    "    # Use the shortest ticker to define fold boundaries\n",
    "    min_len = min(len(df) for df in data_dict.values())\n",
    "\n",
    "    # Pre-compute number of folds for progress bar\n",
    "    n_folds_est = 0\n",
    "    _ts = cfg.min_train_days\n",
    "    while _ts + cfg.window_size < min_len:\n",
    "        _te = _ts - cfg.purge_gap\n",
    "        if _te >= cfg.window_size + 10:\n",
    "            n_folds_est += 1\n",
    "        _ts += cfg.test_days\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  Universe Walk-Forward: {len(tickers)} tickers, ~{n_folds_est} folds\")\n",
    "    print(f\"  Tickers: {tickers}\")\n",
    "    print(f\"  Shortest series: {min_len} days\")\n",
    "    print(f\"  Est. time: ~{n_folds_est * len(tickers) * 10 // 60} min \"\n",
    "          f\"({n_folds_est} folds x {len(tickers)} tickers)\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    all_oos_returns = {t: [] for t in tickers}\n",
    "    all_oos_signals = {t: [] for t in tickers}\n",
    "    all_fold_metrics = []\n",
    "    all_models = {}\n",
    "    latest_vsn_w = None\n",
    "\n",
    "    fold = 0\n",
    "    test_start = cfg.min_train_days\n",
    "    fold_pbar = tqdm(total=n_folds_est, desc=\"Walk-Forward\", unit=\"fold\",\n",
    "                     bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} '\n",
    "                                '[{elapsed}<{remaining}, {rate_fmt}]')\n",
    "\n",
    "    while test_start + cfg.window_size < min_len:\n",
    "        fold += 1\n",
    "        train_end = test_start - cfg.purge_gap\n",
    "        test_end = min(test_start + cfg.test_days, min_len)\n",
    "\n",
    "        if train_end < cfg.window_size + 10:\n",
    "            test_start += cfg.test_days\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n  --- Fold {fold} (train: 0-{train_end}, \"\n",
    "              f\"purge: {train_end}-{test_start}, \"\n",
    "              f\"test: {test_start}-{test_end}) ---\")\n",
    "\n",
    "        # ── Gather train/test data from ALL tickers ──\n",
    "        X_train_all = []\n",
    "        y_train_all = []\n",
    "        # {ticker: (X_test, y_test, valid_indices, test_df)}\n",
    "        ticker_test_data = {}\n",
    "\n",
    "        for ticker in tickers:\n",
    "            df = data_dict[ticker]\n",
    "\n",
    "            train_df = df.iloc[:train_end].copy()\n",
    "            test_df = df.iloc[test_start:test_end].copy()\n",
    "\n",
    "            # Purge: NaN-out last purge_gap+1 training targets (both raw and vol-scaled)\n",
    "            if cfg.purge_gap > 0 and len(train_df) > cfg.purge_gap + 1:\n",
    "                train_df.iloc[-(cfg.purge_gap + 1):,\n",
    "                              train_df.columns.get_loc('target_ret')] = np.nan\n",
    "                train_df.iloc[-(cfg.purge_gap + 1):,\n",
    "                              train_df.columns.get_loc('target_train')] = np.nan\n",
    "\n",
    "            # Normalize each ticker with its OWN training stats (no cross-ticker leakage)\n",
    "            train_norm, test_norm = expanding_normalize(train_df, test_df, FEATURE_COLUMNS)\n",
    "\n",
    "            # Training sequences — use vol-scaled target for better gradient signal\n",
    "            X_tr, y_tr, _ = make_sequences(\n",
    "                train_norm[FEATURE_COLUMNS].values,\n",
    "                train_norm['target_train'].values,\n",
    "                cfg.window_size\n",
    "            )\n",
    "            if len(X_tr) > 0:\n",
    "                X_train_all.append(X_tr)\n",
    "                y_train_all.append(y_tr)\n",
    "                print(f\"    {ticker}: train_rows={len(train_df)}, \"\n",
    "                      f\"train_seqs={len(X_tr)}, \"\n",
    "                      f\"target_NaN={int(train_df['target_train'].isna().sum())}\")\n",
    "\n",
    "            # Test sequences — use vol-scaled for alignment, raw returns for OOS eval\n",
    "            X_te, _, te_idx = make_sequences(\n",
    "                test_norm[FEATURE_COLUMNS].values,\n",
    "                test_norm['target_train'].values,\n",
    "                cfg.window_size\n",
    "            )\n",
    "            # Raw forward returns at the same valid indices (for OOS evaluation)\n",
    "            y_te_raw = test_df['target_ret'].values[te_idx] if len(te_idx) > 0 else np.array([])\n",
    "            print(f\"    {ticker}: test_rows={len(test_df)}, \"\n",
    "                  f\"test_seqs={len(X_te)}, \"\n",
    "                  f\"target_NaN={int(test_df['target_train'].isna().sum())}\")\n",
    "            if len(X_te) > 0:\n",
    "                ticker_test_data[ticker] = (X_te, y_te_raw, te_idx, test_df)\n",
    "\n",
    "        # Combine all tickers' training data into one pool\n",
    "        if not X_train_all:\n",
    "            print(f\"  Fold {fold}: no training data, skipping\")\n",
    "            test_start += cfg.test_days\n",
    "            continue\n",
    "\n",
    "        X_train = np.concatenate(X_train_all, axis=0)\n",
    "        y_train = np.concatenate(y_train_all, axis=0)\n",
    "\n",
    "        # Shuffle the pooled training data (sequences from different tickers\n",
    "        # are interleaved — this prevents the model from memorizing ticker order)\n",
    "        shuffle_idx = np.random.permutation(len(X_train))\n",
    "        X_train = X_train[shuffle_idx]\n",
    "        y_train = y_train[shuffle_idx]\n",
    "\n",
    "        total_test = sum(len(v[0]) for v in ticker_test_data.values())\n",
    "        if len(X_train) < 50 or total_test < 5:\n",
    "            print(f\"  Fold {fold}: insufficient data \"\n",
    "                  f\"(train={len(X_train)}, test={total_test}), skipping\")\n",
    "            test_start += cfg.test_days\n",
    "            continue\n",
    "\n",
    "        print(f\"    UNIVERSE POOL: {len(X_train)} train sequences \"\n",
    "              f\"from {len(tickers)} tickers\")\n",
    "\n",
    "        # ── Build fresh universe model ──\n",
    "        model = MomentumTransformer(\n",
    "            cfg.window_size, n_feat, 1,\n",
    "            cfg.hidden_size, cfg.num_heads, cfg.dropout_rate\n",
    "        )\n",
    "        _ = model(np.zeros((1, cfg.window_size, n_feat), dtype=np.float32))\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(cfg.learning_rate, clipnorm=cfg.clipnorm),\n",
    "            loss=SharpeLoss()\n",
    "        )\n",
    "\n",
    "        # ── Train on pooled universe data ──\n",
    "        t0 = time.time()\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=cfg.epochs,\n",
    "            batch_size=cfg.batch_size,\n",
    "            validation_split=0.15,\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss', patience=cfg.early_stop_patience,\n",
    "                    restore_best_weights=True, verbose=0\n",
    "                ),\n",
    "                keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss', factor=cfg.lr_reduce_factor,\n",
    "                    patience=cfg.lr_reduce_patience, min_lr=cfg.min_lr, verbose=0\n",
    "                )\n",
    "            ],\n",
    "            verbose=0\n",
    "        )\n",
    "        train_time = time.time() - t0\n",
    "\n",
    "        # ── Predict OOS per-ticker ──\n",
    "        for ticker, (X_test, y_test, valid_idx, test_df) in ticker_test_data.items():\n",
    "            n_test = len(X_test)\n",
    "\n",
    "            # Predict one-at-a-time (Keras 3 bakes batch dim from build step;\n",
    "            # model was built with batch=1, so batch>1 collapses output)\n",
    "            preds = _predict_safe(model, X_test)\n",
    "\n",
    "            assert len(preds) == n_test == len(y_test), (\n",
    "                f\"Shape mismatch: preds={len(preds)}, X_test={n_test}, \"\n",
    "                f\"y_test={len(y_test)}\")\n",
    "\n",
    "            # Build date index from make_sequences' valid_idx\n",
    "            test_pred_idx = test_df.index[valid_idx]\n",
    "\n",
    "            # Use y_test directly (already aligned by make_sequences)\n",
    "            all_oos_returns[ticker].append(\n",
    "                pd.Series(y_test, index=test_pred_idx, name=ticker))\n",
    "            all_oos_signals[ticker].append(\n",
    "                pd.Series(preds, index=test_pred_idx, name=ticker))\n",
    "\n",
    "            # Per-ticker fold metrics\n",
    "            fold_positions = np.sign(preds)\n",
    "            fold_strat_ret = fold_positions * y_test\n",
    "            fold_strat_ret = fold_strat_ret[~np.isnan(fold_strat_ret)]\n",
    "            if len(fold_strat_ret) > 1:\n",
    "                fold_sharpe = (np.mean(fold_strat_ret)\n",
    "                               / (np.std(fold_strat_ret, ddof=1) + 1e-9)\n",
    "                               * np.sqrt(252))\n",
    "            else:\n",
    "                fold_sharpe = 0.0\n",
    "\n",
    "            all_fold_metrics.append({\n",
    "                'ticker': ticker, 'fold': fold,\n",
    "                'sharpe': round(fold_sharpe, 2),\n",
    "                'n_days': len(preds),\n",
    "                'train_days': len(X_train),\n",
    "                'train_time': round(train_time / len(tickers), 1)\n",
    "            })\n",
    "\n",
    "            print(f\"    {ticker}: test={len(preds):3d} Sharpe={fold_sharpe:+.2f}\")\n",
    "\n",
    "        # Extract VSN weights from the universe model (one sample, matching batch=1 build)\n",
    "        first_ticker_X = list(ticker_test_data.values())[0][0]\n",
    "        _, weights_dict = model(tf.constant(first_ticker_X[:1]), return_weights=True)\n",
    "        latest_vsn_w = weights_dict['vsn_weights'].numpy().mean(axis=(0, 1)).flatten()\n",
    "\n",
    "        all_models[fold] = model\n",
    "\n",
    "        # Update progress bar with fold summary\n",
    "        fold_sharpes = [m['sharpe'] for m in all_fold_metrics\n",
    "                        if m['fold'] == fold]\n",
    "        avg_s = np.mean(fold_sharpes) if fold_sharpes else 0.0\n",
    "        fold_pbar.set_postfix_str(\n",
    "            f\"train={train_time:.0f}s, avg_sharpe={avg_s:+.2f}\")\n",
    "        fold_pbar.update(1)\n",
    "\n",
    "        test_start += cfg.test_days\n",
    "\n",
    "        # Memory cleanup\n",
    "        del X_train, y_train, X_train_all, y_train_all, ticker_test_data\n",
    "        gc.collect()\n",
    "\n",
    "    fold_pbar.close()\n",
    "\n",
    "    # ── Assemble per-ticker OOS results ──\n",
    "    oos_returns_dict = {}\n",
    "    oos_signals_dict = {}\n",
    "    for ticker in tickers:\n",
    "        if all_oos_returns[ticker]:\n",
    "            combined_ret = pd.concat(all_oos_returns[ticker])\n",
    "            combined_sig = pd.concat(all_oos_signals[ticker])\n",
    "            oos_returns_dict[ticker] = combined_ret[~combined_ret.index.duplicated(keep='first')]\n",
    "            oos_signals_dict[ticker] = combined_sig[~combined_sig.index.duplicated(keep='first')]\n",
    "\n",
    "    # VSN weights: store under 'universe' key and duplicate per-ticker for viz compatibility\n",
    "    vsn_weights = {}\n",
    "    if latest_vsn_w is not None:\n",
    "        for ticker in tickers:\n",
    "            vsn_weights[ticker] = latest_vsn_w\n",
    "\n",
    "    return {\n",
    "        'oos_returns': pd.DataFrame(oos_returns_dict),\n",
    "        'oos_signals': pd.DataFrame(oos_signals_dict),\n",
    "        'fold_metrics': all_fold_metrics,\n",
    "        'models': all_models,\n",
    "        'vsn_weights': vsn_weights,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5340dbbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T16:36:46.923415Z",
     "iopub.status.busy": "2026-02-14T16:36:46.923232Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2026-02-14T16:36:46.915843",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-14 16:36:47,483 [INFO] KiteAuth: using cached token\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "  PHASE 1: DATA ACQUISITION\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f83c11bd41946abaee3fc214d1a361e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching data:   0%|          | 0/14 [00:00<?, ?ticker/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NIFTY (NSE)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Resolved NIFTY -> NIFTY26FEBFUT (expiry: 2026-02-24 00:00:00)\n",
      "  Fetching NIFTY26FEBFUT: 2019-04-12 to 2026-02-14 (2500 calendar days)\n",
      "  NIFTY26FEBFUT: 1697 bars, 2019-04-12 to 2026-02-13\n",
      "    -> 1697 days (2019-04-12 to 2026-02-13)\n",
      "  BANKNIFTY (NSE)...\n",
      "  Resolved BANKNIFTY -> BANKNIFTY26FEBFUT (expiry: 2026-02-24 00:00:00)\n",
      "  Fetching BANKNIFTY26FEBFUT: 2019-04-12 to 2026-02-14 (2500 calendar days)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  BANKNIFTY26FEBFUT: 1697 bars, 2019-04-12 to 2026-02-13\n",
      "    -> 1697 days (2019-04-12 to 2026-02-13)\n",
      "  FINNIFTY (NSE)...\n",
      "  Resolved FINNIFTY -> FINNIFTY26FEBFUT (expiry: 2026-02-24 00:00:00)\n",
      "  Fetching FINNIFTY26FEBFUT: 2019-04-12 to 2026-02-14 (2500 calendar days)\n",
      "  FINNIFTY26FEBFUT: 1264 bars, 2021-01-11 to 2026-02-13\n",
      "    -> 1264 days (2021-01-11 to 2026-02-13)\n",
      "  GOLD (MCX)...\n",
      "  Resolved GOLD -> GOLDTEN26FEBFUT (expiry: 2026-02-27 00:00:00)\n",
      "  Fetching GOLDTEN26FEBFUT: 2019-04-12 to 2026-02-14 (2500 calendar days)\n",
      "  GOLDTEN26FEBFUT: 226 bars, 2025-03-31 to 2026-02-13\n",
      "    -> 226 days (2025-03-31 to 2026-02-13)\n",
      "  SILVER (MCX)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Resolved SILVER -> SILVERM26FEBFUT (expiry: 2026-02-27 00:00:00)\n",
      "  Fetching SILVERM26FEBFUT: 2019-04-12 to 2026-02-14 (2500 calendar days)\n",
      "  SILVERM26FEBFUT: 1748 bars, 2019-04-12 to 2026-02-13\n",
      "    -> 1748 days (2019-04-12 to 2026-02-13)\n",
      "  CRUDEOIL (MCX)...\n",
      "  Resolved CRUDEOIL -> CRUDEOIL26FEBFUT (expiry: 2026-02-19 00:00:00)\n",
      "  Fetching CRUDEOIL26FEBFUT: 2019-04-12 to 2026-02-14 (2500 calendar days)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  CRUDEOIL26FEBFUT: 1752 bars, 2019-04-12 to 2026-02-13\n",
      "    -> 1752 days (2019-04-12 to 2026-02-13)\n",
      "  NATURALGAS (MCX)...\n",
      "  Resolved NATURALGAS -> NATURALGAS26FEBFUT (expiry: 2026-02-24 00:00:00)\n",
      "  Fetching NATURALGAS26FEBFUT: 2019-04-12 to 2026-02-14 (2500 calendar days)\n",
      "  NATURALGAS26FEBFUT: 1752 bars, 2019-04-12 to 2026-02-13\n",
      "    -> 1752 days (2019-04-12 to 2026-02-13)\n",
      "  COPPER (MCX)...\n",
      "  Resolved COPPER -> COPPER26FEBFUT (expiry: 2026-02-27 00:00:00)\n",
      "  Fetching COPPER26FEBFUT: 2019-04-12 to 2026-02-14 (2500 calendar days)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  COPPER26FEBFUT: 1735 bars, 2019-04-12 to 2026-02-13\n",
      "    -> 1735 days (2019-04-12 to 2026-02-13)\n",
      "  RELIANCE (NFO)...\n",
      "  Resolved RELIANCE -> RELIANCE26FEBFUT (expiry: 2026-02-24 00:00:00)\n",
      "  Fetching RELIANCE26FEBFUT: 2019-04-12 to 2026-02-14 (2500 calendar days)\n",
      "  RELIANCE26FEBFUT: 1697 bars, 2019-04-12 to 2026-02-13\n",
      "    -> 1697 days (2019-04-12 to 2026-02-13)\n",
      "  HDFCBANK (NFO)...\n",
      "  Resolved HDFCBANK -> HDFCBANK26FEBFUT (expiry: 2026-02-24 00:00:00)\n",
      "  Fetching HDFCBANK26FEBFUT: 2019-04-12 to 2026-02-14 (2500 calendar days)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  HDFCBANK26FEBFUT: 1697 bars, 2019-04-12 to 2026-02-13\n",
      "    -> 1697 days (2019-04-12 to 2026-02-13)\n",
      "  ICICIBANK (NFO)...\n",
      "  Resolved ICICIBANK -> ICICIBANK26FEBFUT (expiry: 2026-02-24 00:00:00)\n",
      "  Fetching ICICIBANK26FEBFUT: 2019-04-12 to 2026-02-14 (2500 calendar days)\n",
      "  ICICIBANK26FEBFUT: 1697 bars, 2019-04-12 to 2026-02-13\n",
      "    -> 1697 days (2019-04-12 to 2026-02-13)\n",
      "  INFY (NFO)...\n",
      "  Resolved INFY -> INFY26FEBFUT (expiry: 2026-02-24 00:00:00)\n",
      "  Fetching INFY26FEBFUT: 2019-04-12 to 2026-02-14 (2500 calendar days)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  INFY26FEBFUT: 1697 bars, 2019-04-12 to 2026-02-13\n",
      "    -> 1697 days (2019-04-12 to 2026-02-13)\n",
      "  TCS (NFO)...\n",
      "  Resolved TCS -> TCS26FEBFUT (expiry: 2026-02-24 00:00:00)\n",
      "  Fetching TCS26FEBFUT: 2019-04-12 to 2026-02-14 (2500 calendar days)\n",
      "  TCS26FEBFUT: 1697 bars, 2019-04-12 to 2026-02-13\n",
      "    -> 1697 days (2019-04-12 to 2026-02-13)\n",
      "  SBIN (NFO)...\n",
      "  Resolved SBIN -> SBIN26FEBFUT (expiry: 2026-02-24 00:00:00)\n",
      "  Fetching SBIN26FEBFUT: 2019-04-12 to 2026-02-14 (2500 calendar days)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  SBIN26FEBFUT: 1697 bars, 2019-04-12 to 2026-02-13\n",
      "    -> 1697 days (2019-04-12 to 2026-02-13)\n",
      "\n",
      "======================================================================\n",
      "  PHASE 1.5: CROSS-ASSET DATA (News Sentiment + India VIX)\n",
      "======================================================================\n",
      "\n",
      "  [1/2] News Sentiment (GDELT + FinBERT)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb6569fc94334d4ab331feac27d21f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GDELT headlines:   0%|          | 0/895 [00:00<?, ?req/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7. ORCHESTRATION, METRICS & VISUALIZATION\n",
    "\n",
    "def calculate_metrics(returns, costs=0.0):\n",
    "    \"\"\"Full metrics suite for strategy returns.\n",
    "    \n",
    "    Args:\n",
    "        returns: np.ndarray or pd.Series of daily returns (simple, not log)\n",
    "        costs: total transaction costs already deducted (for reporting only)\n",
    "    \n",
    "    Returns: dict with:\n",
    "        total_return, annual_return, sharpe, sortino, calmar, max_dd,\n",
    "        win_rate, profit_factor, n_trades, avg_hold_days\n",
    "    \"\"\"\n",
    "    r = np.asarray(returns, dtype=np.float64)\n",
    "    r = r[~np.isnan(r)]\n",
    "    \n",
    "    n = len(r)\n",
    "    if n < 2:\n",
    "        return {\n",
    "            'total_return': 0.0, 'annual_return': 0.0, 'sharpe': 0.0,\n",
    "            'sortino': 0.0, 'calmar': 0.0, 'max_dd': 0.0,\n",
    "            'win_rate': 0.0, 'profit_factor': 0.0, 'n_days': 0,\n",
    "        }\n",
    "    \n",
    "    # Total and annualized return (simple compounding)\n",
    "    equity = np.cumprod(1.0 + r)\n",
    "    total_return = float(equity[-1] - 1.0)\n",
    "    n_years = n / 252.0\n",
    "    if n_years > 0 and equity[-1] > 0:\n",
    "        annual_return = float(equity[-1] ** (1.0 / n_years) - 1.0)\n",
    "    else:\n",
    "        annual_return = 0.0\n",
    "    \n",
    "    # Sharpe ratio: sqrt(252) * mean / std(ddof=1)\n",
    "    mean_r = np.mean(r)\n",
    "    std_r = np.std(r, ddof=1)\n",
    "    sharpe = float(np.sqrt(252) * mean_r / std_r) if std_r > 1e-9 else 0.0\n",
    "    \n",
    "    # Sortino ratio: sqrt(252) * mean / downside_std(ddof=1)\n",
    "    downside = r[r < 0]\n",
    "    if len(downside) > 1:\n",
    "        downside_std = np.std(downside, ddof=1)\n",
    "        sortino = float(np.sqrt(252) * mean_r / downside_std) if downside_std > 1e-9 else 0.0\n",
    "    else:\n",
    "        sortino = 0.0\n",
    "    \n",
    "    # Maximum drawdown from equity curve\n",
    "    peak = np.maximum.accumulate(equity)\n",
    "    dd_series = (equity - peak) / peak\n",
    "    max_dd = float(np.min(dd_series))\n",
    "    \n",
    "    # Calmar ratio: annual_return / |max_dd|\n",
    "    calmar = float(annual_return / abs(max_dd)) if abs(max_dd) > 1e-9 else 0.0\n",
    "    \n",
    "    # Win rate: fraction of positive return days\n",
    "    win_rate = float(np.mean(r > 0))\n",
    "    \n",
    "    # Profit factor: sum(gains) / |sum(losses)|\n",
    "    gains = r[r > 0]\n",
    "    losses = r[r < 0]\n",
    "    sum_gains = float(np.sum(gains)) if len(gains) > 0 else 0.0\n",
    "    sum_losses = float(np.abs(np.sum(losses))) if len(losses) > 0 else 0.0\n",
    "    profit_factor = float(sum_gains / sum_losses) if sum_losses > 1e-12 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'total_return': total_return,\n",
    "        'annual_return': annual_return,\n",
    "        'sharpe': sharpe,\n",
    "        'sortino': sortino,\n",
    "        'calmar': calmar,\n",
    "        'max_dd': max_dd,\n",
    "        'win_rate': win_rate,\n",
    "        'profit_factor': profit_factor,\n",
    "        'n_days': n,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_enhanced_lab(cfg=None):\n",
    "    \"\"\"Main orchestrator: Data -> Features -> Walk-Forward -> Backtest -> Visualize.\n",
    "    \n",
    "    This is the primary entry point for the notebook.\n",
    "    \"\"\"\n",
    "    if cfg is None:\n",
    "        cfg = CFG\n",
    "    \n",
    "    tickers_to_run = [cfg.tickers[0]] if cfg.quick_mode else cfg.tickers\n",
    "    \n",
    "    # ── PHASE 1: DATA ACQUISITION ──\n",
    "    print(\"=\" * 70)\n",
    "    print(\"  PHASE 1: DATA ACQUISITION\")\n",
    "    print(\"=\" * 70)\n",
    "    auth = KiteAuth()\n",
    "    kite = auth.get_session()\n",
    "    if not kite:\n",
    "        raise RuntimeError(\"Kite authentication failed. Check .env credentials.\")\n",
    "\n",
    "    fetcher = KiteFetcher(kite)\n",
    "    raw_data = {}\n",
    "    for ticker in tqdm(tickers_to_run, desc=\"Fetching data\", unit=\"ticker\"):\n",
    "        exchange = cfg.exchanges.get(ticker, 'NSE')\n",
    "        tqdm.write(f\"  {ticker} ({exchange})...\")\n",
    "        try:\n",
    "            raw_data[ticker] = fetcher.fetch_daily(ticker, exchange, cfg.lookback_days)\n",
    "            tqdm.write(f\"    -> {len(raw_data[ticker])} days \"\n",
    "                       f\"({raw_data[ticker].index[0].date()} to \"\n",
    "                       f\"{raw_data[ticker].index[-1].date()})\")\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"    SKIP: {ticker} failed ({e})\")\n",
    "\n",
    "    if not raw_data:\n",
    "        raise RuntimeError(\"No tickers fetched successfully.\")\n",
    "\n",
    "    # ── PHASE 1.5: CROSS-ASSET DATA (News + VIX + World Monitor) ──\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"  PHASE 1.5: CROSS-ASSET DATA (News Sentiment + India VIX)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Build union of all trading dates for alignment\n",
    "    all_dates = pd.DatetimeIndex([])\n",
    "    for df in raw_data.values():\n",
    "        all_dates = all_dates.union(df.index)\n",
    "    all_dates = all_dates.sort_values()\n",
    "\n",
    "    cross_asset_df = fetch_cross_asset_features(\n",
    "        start_date=all_dates[0].strftime('%Y-%m-%d'),\n",
    "        end_date=all_dates[-1].strftime('%Y-%m-%d'),\n",
    "        date_index=all_dates,\n",
    "        kite=kite,\n",
    "    )\n",
    "\n",
    "    # ── World Monitor Intelligence ──\n",
    "    print(\"\\n  [World Monitor] Macro Regime Signals...\")\n",
    "    try:\n",
    "        macro_df = fetch_yahoo_macro(\n",
    "            all_dates[0].strftime('%Y-%m-%d'),\n",
    "            all_dates[-1].strftime('%Y-%m-%d'),\n",
    "        )\n",
    "        if not macro_df.empty:\n",
    "            macro_df = macro_df.reindex(all_dates).ffill(limit=3)\n",
    "            macro_df['macro_composite'] = compute_macro_composite(macro_df)\n",
    "            for col in MACRO_COLUMNS:\n",
    "                if col in macro_df.columns:\n",
    "                    cross_asset_df[col] = macro_df[col]\n",
    "                    if col not in CROSS_ASSET_COLUMNS:\n",
    "                        CROSS_ASSET_COLUMNS.append(col)\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Macro signals failed: {e}\")\n",
    "        print(f\"    SKIP: Macro signals failed ({e})\")\n",
    "\n",
    "    print(\"  [World Monitor] Fear & Greed Index...\")\n",
    "    try:\n",
    "        fg_df = fetch_fear_greed()\n",
    "        if not fg_df.empty:\n",
    "            fg_df = fg_df.reindex(all_dates).ffill(limit=3)\n",
    "            cross_asset_df['macro_fear_greed_z'] = fg_df['macro_fear_greed_z']\n",
    "            if 'macro_fear_greed_z' not in CROSS_ASSET_COLUMNS:\n",
    "                CROSS_ASSET_COLUMNS.append('macro_fear_greed_z')\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Fear & Greed failed: {e}\")\n",
    "\n",
    "    # ── PHASE 2: FEATURE ENGINEERING ──\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"  PHASE 2: FEATURE ENGINEERING ({len(FEATURE_COLUMNS)} features)\")\n",
    "    print(\"=\" * 70)\n",
    "    featured_data = {}\n",
    "    min_required = cfg.min_train_days + 2 * cfg.test_days\n",
    "    for ticker, df in tqdm(raw_data.items(), desc=\"Feature engineering\",\n",
    "                           total=len(raw_data), unit=\"ticker\"):\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            feat_df = build_all_features(df, cfg)\n",
    "            elapsed = time.time() - t0\n",
    "            if len(feat_df) < min_required:\n",
    "                tqdm.write(f\"  SKIP: {ticker} has {len(feat_df)} featured days \"\n",
    "                           f\"(need {min_required})\")\n",
    "                continue\n",
    "            # Merge cross-asset features (same for all tickers)\n",
    "            if not cross_asset_df.empty:\n",
    "                feat_df = feat_df.join(cross_asset_df, how='left')\n",
    "                for col in CROSS_ASSET_COLUMNS:\n",
    "                    if col in feat_df.columns:\n",
    "                        feat_df[col] = feat_df[col].ffill(limit=3).fillna(0.0)\n",
    "            # Build advanced features (Groups 15-19)\n",
    "            adv_df = build_advanced_features(df, feat_df['close'] if 'close' in feat_df.columns else df['close'], cfg)\n",
    "            feat_df = feat_df.join(adv_df, how='left')\n",
    "            for col in ADVANCED_FEATURE_COLUMNS:\n",
    "                if col in feat_df.columns:\n",
    "                    feat_df[col] = feat_df[col].ffill(limit=3).fillna(0.0)\n",
    "\n",
    "            # Welford anomaly features (from raw OHLCV)\n",
    "            welf_df = compute_welford_anomalies(df)\n",
    "            welf_df = welf_df.reindex(feat_df.index)\n",
    "            feat_df = feat_df.join(welf_df, how='left')\n",
    "            for col in WELFORD_COLUMNS:\n",
    "                if col in feat_df.columns:\n",
    "                    feat_df[col] = feat_df[col].fillna(0.0)\n",
    "\n",
    "            featured_data[ticker] = feat_df\n",
    "            n_total = len(FEATURE_COLUMNS)\n",
    "            tqdm.write(f\"  {ticker}: {len(feat_df)} days, \"\n",
    "                       f\"{n_total} base + advanced features ({elapsed:.1f}s)\")\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"  SKIP: {ticker} feature build failed ({e})\")\n",
    "\n",
    "    if not featured_data:\n",
    "        raise RuntimeError(\"No tickers survived feature engineering.\")\n",
    "\n",
    "    # Extend FEATURE_COLUMNS with cross-asset + advanced + welford features\n",
    "    for col_list in [CROSS_ASSET_COLUMNS, ADVANCED_FEATURE_COLUMNS, WELFORD_COLUMNS]:\n",
    "        for col in col_list:\n",
    "            if col not in FEATURE_COLUMNS:\n",
    "                FEATURE_COLUMNS.append(col)\n",
    "\n",
    "    n_cross = len(CROSS_ASSET_COLUMNS)\n",
    "    n_adv = len(ADVANCED_FEATURE_COLUMNS)\n",
    "    n_welf = len(WELFORD_COLUMNS)\n",
    "    print(f\"\\n  Features extended: {len(FEATURE_COLUMNS)} total\")\n",
    "    print(f\"    Cross-asset: +{n_cross} ({CROSS_ASSET_COLUMNS})\")\n",
    "    print(f\"    Advanced:    +{n_adv} (HMM, Wavelet, InfoTheory, MF-DFA, TDA)\")\n",
    "    print(f\"    Welford:     +{n_welf} (Anomaly detection)\")\n",
    "    print(f\"    Macro:       +{len(MACRO_COLUMNS)} (World Monitor)\")\n",
    "\n",
    "    print(f\"\\n  Universe: {len(featured_data)} tickers survived \"\n",
    "          f\"(min {min_required} featured days required)\")\n",
    "    \n",
    "    # ── PHASE 3: WALK-FORWARD TRAINING ──\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"  PHASE 3: WALK-FORWARD OOS VALIDATION\")\n",
    "    print(f\"  Train: {cfg.min_train_days}d | Test: {cfg.test_days}d | \"\n",
    "          f\"Purge: {cfg.purge_gap}d | Window: {cfg.window_size}d\")\n",
    "    print(\"=\" * 70)\n",
    "    results = walk_forward_train(featured_data, cfg)\n",
    "    \n",
    "    # ── PHASE 4: STRATEGY CONSTRUCTION & METRICS ──\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"  PHASE 4: OOS STRATEGY METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    oos_returns = results['oos_returns']\n",
    "    oos_signals = results['oos_signals']\n",
    "    \n",
    "    if oos_returns.empty:\n",
    "        print(\"  WARNING: No OOS data produced. Check data length vs min_train_days.\")\n",
    "        return {'raw_data': raw_data, 'featured_data': featured_data,\n",
    "                'results': results, 'all_metrics': {}, 'cfg': cfg}\n",
    "    \n",
    "    all_metrics = {}\n",
    "    strat_net_returns = {}  # store for plotting\n",
    "    \n",
    "    for ticker in oos_returns.columns:\n",
    "        ret = oos_returns[ticker].dropna()\n",
    "        sig = oos_signals[ticker].dropna()\n",
    "        \n",
    "        # Align on common dates\n",
    "        common_idx = ret.index.intersection(sig.index)\n",
    "        ret = ret.loc[common_idx]\n",
    "        sig = sig.loc[common_idx]\n",
    "        \n",
    "        if len(ret) < 5:\n",
    "            print(f\"  {ticker}: insufficient OOS data ({len(ret)} days), skipping metrics\")\n",
    "            continue\n",
    "        \n",
    "        # Strategy returns: position = sign(signal), NOT raw magnitude\n",
    "        position = np.sign(sig.values)\n",
    "        strat_ret = position * ret.values\n",
    "        \n",
    "        # Transaction costs: deduct |delta_position| * bps_cost\n",
    "        pos_change = np.abs(np.diff(position, prepend=0))\n",
    "        costs_arr = pos_change * cfg.bps_cost\n",
    "        net_ret = strat_ret - costs_arr\n",
    "        \n",
    "        metrics = calculate_metrics(net_ret, costs=costs_arr.sum())\n",
    "        metrics['ticker'] = ticker\n",
    "        metrics['total_costs'] = float(costs_arr.sum())\n",
    "        all_metrics[ticker] = metrics\n",
    "        strat_net_returns[ticker] = pd.Series(net_ret, index=common_idx)\n",
    "        \n",
    "        print(f\"\\n  {ticker} (OOS: {ret.index[0].date()} to \"\n",
    "              f\"{ret.index[-1].date()}, {len(ret)} days):\")\n",
    "        print(f\"    Sharpe:        {metrics['sharpe']:+.2f}\")\n",
    "        print(f\"    Total Return:  {metrics['total_return']:+.2%}\")\n",
    "        print(f\"    Annual Return: {metrics['annual_return']:+.2%}\")\n",
    "        print(f\"    Max Drawdown:  {metrics['max_dd']:.2%}\")\n",
    "        print(f\"    Sortino:       {metrics['sortino']:+.2f}\")\n",
    "        print(f\"    Calmar:        {metrics['calmar']:+.2f}\")\n",
    "        print(f\"    Win Rate:      {metrics['win_rate']:.1%}\")\n",
    "        print(f\"    Profit Factor: {metrics['profit_factor']:.2f}\")\n",
    "        print(f\"    Total Costs:   {metrics['total_costs']:.4f}\")\n",
    "    \n",
    "    # ── PHASE 5: FOLD-BY-FOLD SUMMARY ──\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"  FOLD-BY-FOLD BREAKDOWN\")\n",
    "    print(\"=\" * 70)\n",
    "    fold_df = pd.DataFrame(results['fold_metrics'])\n",
    "    if not fold_df.empty:\n",
    "        print(fold_df.to_string(index=False))\n",
    "        \n",
    "        avg_sharpe = fold_df.groupby('ticker')['sharpe'].mean()\n",
    "        std_sharpe = fold_df.groupby('ticker')['sharpe'].std(ddof=1)\n",
    "        n_folds = fold_df.groupby('ticker')['sharpe'].count()\n",
    "        print(f\"\\n  Average OOS Sharpe per ticker:\")\n",
    "        for t in avg_sharpe.index:\n",
    "            se = std_sharpe[t] / np.sqrt(n_folds[t]) if n_folds[t] > 1 else 0.0\n",
    "            print(f\"    {t}: {avg_sharpe[t]:+.2f} +/- {std_sharpe[t]:.2f} \"\n",
    "                  f\"(SE={se:.2f}, {int(n_folds[t])} folds)\")\n",
    "    \n",
    "    # ── PHASE 5.5: ENSEMBLE (TFT + Momentum + Mean Reversion) ──\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"  PHASE 5.5: ENSEMBLE (RL Thompson Sampling Meta-Learner)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    ensemble_metrics = {}\n",
    "    ensemble_net_returns = {}\n",
    "    for ticker in oos_returns.columns:\n",
    "        ret = oos_returns[ticker].dropna()\n",
    "        sig = oos_signals[ticker].dropna()\n",
    "        common_idx = ret.index.intersection(sig.index)\n",
    "\n",
    "        if len(common_idx) < 10 or ticker not in featured_data:\n",
    "            continue\n",
    "\n",
    "        df_feat = featured_data[ticker]\n",
    "        feat_idx = common_idx.intersection(df_feat.index)\n",
    "        if len(feat_idx) < 10:\n",
    "            continue\n",
    "\n",
    "        ret_e = ret.loc[feat_idx].values\n",
    "        sig_e = sig.loc[feat_idx].values\n",
    "\n",
    "        # Three signals\n",
    "        tft_pos = np.sign(sig_e)\n",
    "        mom_pos = np.sign(df_feat.loc[feat_idx, 'norm_ret_21d'].values)\n",
    "        mr_pos = -np.sign(df_feat.loc[feat_idx, 'mr_zscore'].values)\n",
    "\n",
    "        # Majority vote: sign of sum (2-of-3 or 3-of-3 agree)\n",
    "        ens_signal = tft_pos + mom_pos + mr_pos\n",
    "        ens_pos = np.sign(ens_signal)\n",
    "\n",
    "        # Ensemble returns with costs\n",
    "        ens_strat_ret = ens_pos * ret_e\n",
    "        pos_change = np.abs(np.diff(ens_pos, prepend=0))\n",
    "        costs = pos_change * cfg.bps_cost\n",
    "        ens_net = ens_strat_ret - costs\n",
    "\n",
    "        ens_m = calculate_metrics(ens_net, costs=costs.sum())\n",
    "        ensemble_metrics[ticker] = ens_m\n",
    "        ensemble_net_returns[ticker] = pd.Series(ens_net, index=feat_idx)\n",
    "\n",
    "        # Individual signal Sharpes (gross) for comparison\n",
    "        tft_sr = (float(np.mean(tft_pos * ret_e))\n",
    "                  / (float(np.std(tft_pos * ret_e, ddof=1)) + 1e-9)\n",
    "                  * np.sqrt(252))\n",
    "        mom_sr = (float(np.mean(mom_pos * ret_e))\n",
    "                  / (float(np.std(mom_pos * ret_e, ddof=1)) + 1e-9)\n",
    "                  * np.sqrt(252))\n",
    "        mr_sr = (float(np.mean(mr_pos * ret_e))\n",
    "                 / (float(np.std(mr_pos * ret_e, ddof=1)) + 1e-9)\n",
    "                 * np.sqrt(252))\n",
    "\n",
    "        # Signal agreement rate\n",
    "        agree_all = float(np.mean((tft_pos == mom_pos) & (mom_pos == mr_pos)))\n",
    "        agree_2of3 = float(np.mean(np.abs(ens_signal) >= 2))\n",
    "\n",
    "        print(f\"\\n  {ticker} ({len(ret_e)} days):\")\n",
    "        print(f\"    TFT Sharpe:        {tft_sr:+.2f}\")\n",
    "        print(f\"    Momentum Sharpe:   {mom_sr:+.2f}\")\n",
    "        print(f\"    MR Sharpe:         {mr_sr:+.2f}\")\n",
    "        print(f\"    Ensemble Sharpe:   {ens_m['sharpe']:+.2f}  (net of costs)\")\n",
    "        print(f\"    Ensemble Return:   {ens_m['total_return']:+.2%}\")\n",
    "        print(f\"    Ensemble MaxDD:    {ens_m['max_dd']:.2%}\")\n",
    "        print(f\"    Agreement (3/3):   {agree_all:.1%}\")\n",
    "        print(f\"    Agreement (2+/3):  {agree_2of3:.1%}\")\n",
    "\n",
    "    # ── PHASE 6: EDGE AUDIT ──\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"  PHASE 6: EDGE AUDIT — Is the alpha real?\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\n  Target definition:\")\n",
    "    print(\"    target_ret[t] = (close[t+1] - close[t]) / close[t]  [1-day forward return]\")\n",
    "    print(\"    signal[t] from features[t-W+1 : t]  [causal window, W=21]\")\n",
    "    print(\"    strat_ret[t] = sign(signal[t]) * target_ret[t]  [trade at close of day t]\")\n",
    "\n",
    "    for ticker in oos_returns.columns:\n",
    "        ret = oos_returns[ticker].dropna()\n",
    "        sig = oos_signals[ticker].dropna()\n",
    "        common_idx = ret.index.intersection(sig.index)\n",
    "        ret_a = ret.loc[common_idx]\n",
    "        sig_a = sig.loc[common_idx]\n",
    "\n",
    "        if len(ret_a) < 10:\n",
    "            continue\n",
    "\n",
    "        r = ret_a.values\n",
    "        position = np.sign(sig_a.values)\n",
    "        strat_ret_gross = position * r\n",
    "\n",
    "        # Turnover stats\n",
    "        pos_changes = np.abs(np.diff(position, prepend=0))\n",
    "        n_trades = int(np.sum(pos_changes > 0))\n",
    "        daily_turnover = np.mean(pos_changes)\n",
    "        gross_sharpe = (float(np.mean(strat_ret_gross))\n",
    "                        / (float(np.std(strat_ret_gross, ddof=1)) + 1e-9)\n",
    "                        * np.sqrt(252))\n",
    "\n",
    "        print(f\"\\n  {ticker} ({len(r)} OOS days, {n_trades} position changes, \"\n",
    "              f\"daily turnover {daily_turnover:.3f})\")\n",
    "        print(f\"    Gross Sharpe (no costs): {gross_sharpe:+.2f}\")\n",
    "\n",
    "        # ── Test 1: Sign-flip ──\n",
    "        flip_ret = -position * r\n",
    "        flip_sharpe = (float(np.mean(flip_ret))\n",
    "                       / (float(np.std(flip_ret, ddof=1)) + 1e-9)\n",
    "                       * np.sqrt(252))\n",
    "        verdict_1 = \"PASS\" if flip_sharpe < 0 else \"FAIL (signal is noise or drift)\"\n",
    "        print(f\"\\n    Test 1 — Sign flip:\")\n",
    "        print(f\"      Flipped Sharpe:  {flip_sharpe:+.2f}  [{verdict_1}]\")\n",
    "\n",
    "        # ── Test 2: Delay execution by 1 day ──\n",
    "        if len(r) > 2:\n",
    "            # position[t] applied to target_ret[t+1] instead of target_ret[t]\n",
    "            delay_ret = position[:-1] * r[1:]\n",
    "            delay_sharpe = (float(np.mean(delay_ret))\n",
    "                            / (float(np.std(delay_ret, ddof=1)) + 1e-9)\n",
    "                            * np.sqrt(252))\n",
    "            decay = gross_sharpe - delay_sharpe\n",
    "            if delay_sharpe < gross_sharpe * 0.5:\n",
    "                verdict_2 = \"CLEAN (edge decays with delay)\"\n",
    "            elif delay_sharpe < 0:\n",
    "                verdict_2 = \"CLEAN (edge reverses with delay)\"\n",
    "            else:\n",
    "                verdict_2 = \"SUSPICIOUS (edge persists — possible drift capture)\"\n",
    "            print(f\"\\n    Test 2 — Delay 1 day:\")\n",
    "            print(f\"      Delayed Sharpe:  {delay_sharpe:+.2f}  \"\n",
    "                  f\"(decay: {decay:+.2f})  [{verdict_2}]\")\n",
    "\n",
    "        # ── Test 3: Dumb baselines ──\n",
    "        if ticker in featured_data:\n",
    "            df_feat = featured_data[ticker]\n",
    "            common_feat_idx = common_idx.intersection(df_feat.index)\n",
    "            if len(common_feat_idx) > 10:\n",
    "                ret_bl = ret_a.loc[common_feat_idx].values\n",
    "\n",
    "                # Momentum: sign(yesterday's normalized return)\n",
    "                mom_pos = np.sign(\n",
    "                    df_feat.loc[common_feat_idx, 'norm_ret_1d'].values)\n",
    "                mom_ret = mom_pos * ret_bl\n",
    "                mom_sharpe = (float(np.mean(mom_ret))\n",
    "                              / (float(np.std(mom_ret, ddof=1)) + 1e-9)\n",
    "                              * np.sqrt(252))\n",
    "\n",
    "                # Mean reversion: -sign(yesterday's normalized return)\n",
    "                mr_ret = -mom_pos * ret_bl\n",
    "                mr_sharpe = (float(np.mean(mr_ret))\n",
    "                             / (float(np.std(mr_ret, ddof=1)) + 1e-9)\n",
    "                             * np.sqrt(252))\n",
    "\n",
    "                best_baseline = max(mom_sharpe, mr_sharpe)\n",
    "                advantage = gross_sharpe - best_baseline\n",
    "                if advantage > 0.5:\n",
    "                    verdict_3 = \"PASS\"\n",
    "                elif advantage > 0:\n",
    "                    verdict_3 = \"MARGINAL\"\n",
    "                else:\n",
    "                    verdict_3 = \"FAIL (TFT doesn't beat simple baseline)\"\n",
    "                print(f\"\\n    Test 3 — Baselines:\")\n",
    "                print(f\"      Momentum baseline: {mom_sharpe:+.2f}\")\n",
    "                print(f\"      MR baseline:       {mr_sharpe:+.2f}\")\n",
    "                print(f\"      TFT advantage:     {advantage:+.2f}  [{verdict_3}]\")\n",
    "\n",
    "        # ── Test 4: Cost sensitivity sweep ──\n",
    "        print(f\"\\n    Test 4 — Cost sensitivity:\")\n",
    "        for bps in [0, 5, 10, 15, 20, 30]:\n",
    "            cost_per_change = bps / 10000.0\n",
    "            costs = pos_changes * cost_per_change\n",
    "            net = strat_ret_gross - costs\n",
    "            net_sharpe = (float(np.mean(net))\n",
    "                          / (float(np.std(net, ddof=1)) + 1e-9)\n",
    "                          * np.sqrt(252))\n",
    "            total_cost_pct = costs.sum() * 100\n",
    "            marker = \" <-- current\" if bps == int(cfg.bps_cost * 10000) else \"\"\n",
    "            print(f\"      {bps:2d} bps: Sharpe {net_sharpe:+.2f}  \"\n",
    "                  f\"(total cost: {total_cost_pct:.2f}%){marker}\")\n",
    "\n",
    "        # ── Target alignment: first 5 OOS dates ──\n",
    "        print(f\"\\n    Target alignment (first 5 OOS dates):\")\n",
    "        for i, dt in enumerate(common_idx[:5]):\n",
    "            print(f\"      {dt.date()}: signal={sig_a.loc[dt]:+.4f}  \"\n",
    "                  f\"target_ret={ret_a.loc[dt]:+.6f}  \"\n",
    "                  f\"pos={int(np.sign(sig_a.loc[dt]))}\")\n",
    "\n",
    "    # ── Fold stability ──\n",
    "    if not fold_df.empty and len(fold_df) > 2:\n",
    "        print(f\"\\n  Fold Stability:\")\n",
    "        for ticker in fold_df['ticker'].unique():\n",
    "            t_folds = fold_df[fold_df['ticker'] == ticker]\n",
    "            sharpes = t_folds['sharpe'].values\n",
    "            n_pos = int(np.sum(sharpes > 0))\n",
    "            n_neg = int(np.sum(sharpes <= 0))\n",
    "            print(f\"    {ticker}: {len(sharpes)} folds, \"\n",
    "                  f\"mean={np.mean(sharpes):+.2f}, \"\n",
    "                  f\"std={np.std(sharpes, ddof=1):.2f}, \"\n",
    "                  f\"min={np.min(sharpes):+.2f}, max={np.max(sharpes):+.2f}, \"\n",
    "                  f\"+ve/-ve={n_pos}/{n_neg}\")\n",
    "            if np.std(sharpes, ddof=1) > 3.0:\n",
    "                print(f\"      WARNING: High fold variance — edge is unstable\")\n",
    "\n",
    "    # ── PHASE 7: VISUALIZATION ──\n",
    "    if not all_metrics:\n",
    "        print(\"\\n  No metrics to visualize.\")\n",
    "        return {'raw_data': raw_data, 'featured_data': featured_data,\n",
    "                'results': results, 'all_metrics': all_metrics, 'cfg': cfg}\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('QuantKubera Monolith v2 -- Walk-Forward OOS Results',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 6a. Equity curves (net of costs)\n",
    "    ax = axes[0, 0]\n",
    "    for ticker, net_s in strat_net_returns.items():\n",
    "        equity = (1.0 + net_s).cumprod()\n",
    "        label = f\"{ticker} (Sharpe={all_metrics[ticker]['sharpe']:+.2f})\"\n",
    "        ax.plot(equity.index, equity.values, label=label, linewidth=1.5)\n",
    "    ax.set_title('OOS Equity Curves (net of costs)')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylabel('Growth of $1')\n",
    "    ax.axhline(1.0, color='gray', linestyle='--', alpha=0.4)\n",
    "    \n",
    "    # 6b. Drawdown\n",
    "    ax = axes[0, 1]\n",
    "    for ticker, net_s in strat_net_returns.items():\n",
    "        equity = (1.0 + net_s).cumprod()\n",
    "        peak = equity.cummax()\n",
    "        dd = (equity - peak) / peak\n",
    "        ax.fill_between(dd.index, dd.values, alpha=0.3, label=ticker)\n",
    "    ax.set_title('Drawdown')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylabel('Drawdown')\n",
    "    \n",
    "    # 6c. VSN Feature Importance (first ticker with weights)\n",
    "    ax = axes[1, 0]\n",
    "    if results['vsn_weights']:\n",
    "        first_ticker = list(results['vsn_weights'].keys())[0]\n",
    "        vsn_w = results['vsn_weights'][first_ticker]\n",
    "        n_features_to_show = min(15, len(FEATURE_COLUMNS), len(vsn_w))\n",
    "        sorted_idx = np.argsort(vsn_w)[::-1][:n_features_to_show]\n",
    "        ax.barh(range(n_features_to_show),\n",
    "                vsn_w[sorted_idx],\n",
    "                color='steelblue')\n",
    "        ax.set_yticks(range(n_features_to_show))\n",
    "        ax.set_yticklabels([FEATURE_COLUMNS[i] for i in sorted_idx], fontsize=8)\n",
    "        ax.set_title(f'VSN Feature Importance ({first_ticker})')\n",
    "        ax.invert_yaxis()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlabel('Weight')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No VSN weights available',\n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    # 6d. Monthly returns bar chart (first ticker)\n",
    "    ax = axes[1, 1]\n",
    "    first_ticker_key = list(strat_net_returns.keys())[0]\n",
    "    monthly = strat_net_returns[first_ticker_key].resample('ME').sum()\n",
    "    colors = ['#d32f2f' if r < 0 else '#388e3c' for r in monthly.values]\n",
    "    x_pos = range(len(monthly))\n",
    "    ax.bar(x_pos, monthly.values * 100, color=colors, alpha=0.7)\n",
    "    ax.set_title(f'Monthly Returns (%) -- {first_ticker_key}')\n",
    "    ax.set_ylabel('Return %')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    # Label x-axis with month abbreviations if not too many\n",
    "    if len(monthly) <= 36:\n",
    "        ax.set_xticks(list(x_pos))\n",
    "        ax.set_xticklabels([d.strftime('%b %y') for d in monthly.index],\n",
    "                           rotation=45, ha='right', fontsize=7)\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return results for downstream cells (VBT, etc.)\n",
    "    return {\n",
    "        'raw_data': raw_data,\n",
    "        'featured_data': featured_data,\n",
    "        'results': results,\n",
    "        'all_metrics': all_metrics,\n",
    "        'strat_net_returns': strat_net_returns,\n",
    "        'cfg': cfg,\n",
    "    }\n",
    "\n",
    "\n",
    "# ── RUN THE LAB ──\n",
    "lab_output = run_enhanced_lab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b12ad",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 8. VECTORBTPRO TEARSHEET & TRADE ANALYSIS\n",
    "import vectorbtpro as vbt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  VECTORBTPRO TEARSHEET & TRADE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use results from Cell 7 (lab_output variable)\n",
    "oos_returns = lab_output['results']['oos_returns']\n",
    "oos_signals = lab_output['results']['oos_signals']\n",
    "cfg = lab_output['cfg']\n",
    "\n",
    "for ticker in oos_returns.columns:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  {ticker} TEARSHEET\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    sig = oos_signals[ticker].dropna()\n",
    "    ret = oos_returns[ticker].dropna()\n",
    "    \n",
    "    if len(sig) < 10:\n",
    "        print(f\"  Insufficient OOS data for {ticker} ({len(sig)} days), skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Get close prices for the OOS period from featured_data\n",
    "    feat_df = lab_output['featured_data'][ticker]\n",
    "    close = feat_df['close'].reindex(sig.index).dropna()\n",
    "    \n",
    "    # Align all series on common dates\n",
    "    common_idx = sig.index.intersection(close.index)\n",
    "    if len(common_idx) < 10:\n",
    "        print(f\"  Insufficient aligned data for {ticker} ({len(common_idx)} days), skipping.\")\n",
    "        continue\n",
    "    \n",
    "    sig = sig.loc[common_idx]\n",
    "    close = close.loc[common_idx]\n",
    "    \n",
    "    # Build position series with T+1 lag (signal at t -> trade at t+1)\n",
    "    position = np.sign(sig).shift(1).fillna(0)\n",
    "    \n",
    "    # Derive entry/exit signals from position changes\n",
    "    prev_pos = position.shift(1).fillna(0)\n",
    "    long_entries  = (position > 0) & (prev_pos <= 0)\n",
    "    long_exits    = (position <= 0) & (prev_pos > 0)\n",
    "    short_entries = (position < 0) & (prev_pos >= 0)\n",
    "    short_exits   = (position >= 0) & (prev_pos < 0)\n",
    "    \n",
    "    # VBT Portfolio: long/short from signals\n",
    "    pf = vbt.Portfolio.from_signals(\n",
    "        close=close,\n",
    "        long_entries=long_entries,\n",
    "        long_exits=long_exits,\n",
    "        short_entries=short_entries,\n",
    "        short_exits=short_exits,\n",
    "        fees=cfg.bps_cost / 2,       # per-side fee\n",
    "        slippage=cfg.bps_cost / 2,    # per-side slippage\n",
    "        freq='1D',\n",
    "        init_cash=1_000_000,\n",
    "    )\n",
    "    \n",
    "    # Print full stats\n",
    "    print(pf.stats())\n",
    "    \n",
    "    # Trade-level analysis\n",
    "    if hasattr(pf, 'trades') and pf.trades.count > 0:\n",
    "        trades = pf.trades\n",
    "        print(f\"\\n  --- Trade Summary ---\")\n",
    "        print(f\"  Total Trades:   {trades.count}\")\n",
    "        print(f\"  Win Rate:       {trades.win_rate:.2%}\")\n",
    "        print(f\"  Profit Factor:  {trades.profit_factor:.2f}\")\n",
    "        print(f\"  Avg P&L:        {trades.pnl.mean():.2f}\")\n",
    "        print(f\"  Max Win:        {trades.pnl.max():.2f}\")\n",
    "        print(f\"  Max Loss:       {trades.pnl.min():.2f}\")\n",
    "        print(f\"  Expectancy:     {trades.expectancy:.2f}\")\n",
    "        print(f\"  Avg Duration:   {trades.duration.mean()}\")\n",
    "        \n",
    "        # Show recent trades\n",
    "        readable = trades.records_readable\n",
    "        if len(readable) > 0:\n",
    "            print(f\"\\n  --- Last 10 Trades ---\")\n",
    "            print(readable.tail(10).to_string())\n",
    "    else:\n",
    "        print(\"  No trades executed.\")\n",
    "    \n",
    "    # Plot equity curve\n",
    "    try:\n",
    "        fig = pf.plot()\n",
    "        fig.update_layout(\n",
    "            title=f'{ticker} -- VectorBTPro Equity Curve',\n",
    "            height=400,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  Plot failed: {e}\")\n",
    "        # Fallback: matplotlib equity curve\n",
    "        equity = pf.value()\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(equity.index, equity.values, linewidth=1.5)\n",
    "        plt.title(f'{ticker} -- VectorBTPro Equity Curve')\n",
    "        plt.ylabel('Portfolio Value')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"  VectorBTPro analysis complete\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qk_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "QuantKubera_Monolith_v3.ipynb",
   "output_path": "QuantKubera_Monolith_v3_output.ipynb",
   "parameters": {},
   "start_time": "2026-02-14T16:36:13.276881",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}