{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83477b61",
   "metadata": {},
   "source": [
    "# QuantKubera Monolith v2 — Enhanced\n",
    "\n",
    "## Institutional-Grade Momentum Transformer for Indian Derivatives\n",
    "\n",
    "**Self-contained** | **Zero Look-Ahead Bias** | **Walk-Forward OOS Validation**\n",
    "\n",
    "| Component | Reference | Features |\n",
    "|-----------|-----------|----------|\n",
    "| Temporal Fusion Transformer | Lim et al. (2021) | VSN, Interpretable MHA, GRN |\n",
    "| AFML Event Pipeline | Lopez de Prado (2018) | CUSUM, Triple Barrier, Meta-Labeling |\n",
    "| Fractional Differentiation | Hosking (1981) | Memory-preserving stationarity |\n",
    "| Ramanujan Sum Filter Bank | Planat (2002) | Integer-period cycle detection |\n",
    "| NIG Changepoint Detection | Adams & MacKay (2007) | Regime shift scoring |\n",
    "| Market Microstructure | Easley et al. (2012), Kyle (1985) | VPIN, Lambda, Amihud |\n",
    "| Information-Theoretic Entropy | Shannon, Masters | Predictability regime filter |\n",
    "| Garman-Klass / Parkinson Vol | GK (1980), Parkinson (1980) | Efficient OHLC volatility |\n",
    "\n",
    "### Pipeline\n",
    "```\n",
    "Zerodha Kite API → 31 Features (10 groups) → Variable Selection Network\n",
    "GDELT News → FinBERT → 9 Sentiment Features ↗\n",
    "India VIX → 3 VIX Features ↗\n",
    "→ LSTM Encoder → Interpretable Multi-Head Attention → Momentum Signal\n",
    "→ Walk-Forward OOS (purge gaps) → Meta-Labeling → Probit Bet Sizing\n",
    "→ VectorBTPro Tearsheet\n",
    "```\n",
    "\n",
    "### Feature Groups (31 per-ticker + 12 cross-asset)\n",
    "1. **Normalized Returns** (5): Vol-normalized multi-horizon returns\n",
    "2. **MACD** (3): Z-scored momentum oscillators\n",
    "3. **Volatility** (4): Realized, Garman-Klass, Parkinson estimators\n",
    "4. **Changepoint Detection** (2): NIG Bayesian regime shifts\n",
    "5. **Fractional Calculus** (3): Hosking differencing + Hurst exponent\n",
    "6. **Ramanujan Periodogram** (4): Weekly/biweekly/monthly/quarterly cycles\n",
    "7. **Microstructure** (4): VPIN, Kyle's λ, Amihud, HL spread\n",
    "8. **Information Theory** (1): Shannon entropy of price patterns\n",
    "9. **Momentum Quality** (3): Trend strength, consistency, mean-reversion\n",
    "10. **Volume** (2): Volume z-score and momentum\n",
    "11. **News Sentiment** (9): FinBERT scores from GDELT headlines (T+1 lag)\n",
    "12. **India VIX** (3): Fear gauge z-score, 5d change, mean-reversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc78d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Setup & Configuration\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.special import gammaln, gamma as gamma_fn\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from kiteconnect import KiteConnect\n",
    "from dotenv import load_dotenv\n",
    "import pyotp\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Suppress warnings\n",
    "# ---------------------------------------------------------------------------\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "# Suppress Keras masking warnings — we use fixed-length sequences (window_size=21),\n",
    "# no padding, no variable-length inputs. Masking is irrelevant.\n",
    "warnings.filterwarnings('ignore', message='.*does not support masking.*')\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s')\n",
    "logger = logging.getLogger('QuantKubera')\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# GPU detection & memory growth\n",
    "# ---------------------------------------------------------------------------\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        except RuntimeError as e:\n",
    "            logger.warning(f\"GPU memory growth setting failed: {e}\")\n",
    "    GPU_AVAILABLE = True\n",
    "    GPU_NAME = tf.test.gpu_device_name() or gpus[0].name\n",
    "else:\n",
    "    GPU_AVAILABLE = False\n",
    "    GPU_NAME = \"None\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Seed everything\n",
    "# ---------------------------------------------------------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Configuration\n",
    "# ---------------------------------------------------------------------------\n",
    "@dataclass\n",
    "class MonolithConfig:\n",
    "    # Data\n",
    "    tickers: list = field(default_factory=lambda: [\n",
    "        # NSE Index Futures\n",
    "        'NIFTY', 'BANKNIFTY', 'FINNIFTY',\n",
    "        # MCX Commodity Futures\n",
    "        'GOLD', 'SILVER', 'CRUDEOIL', 'NATURALGAS', 'COPPER',\n",
    "        # NSE Stock Futures (top FnO by volume)\n",
    "        'RELIANCE', 'HDFCBANK', 'ICICIBANK', 'INFY', 'TCS', 'SBIN',\n",
    "    ])\n",
    "    exchanges: dict = field(default_factory=lambda: {\n",
    "        'NIFTY': 'NSE', 'BANKNIFTY': 'NSE', 'FINNIFTY': 'NSE',\n",
    "        'GOLD': 'MCX', 'SILVER': 'MCX', 'CRUDEOIL': 'MCX',\n",
    "        'NATURALGAS': 'MCX', 'COPPER': 'MCX',\n",
    "        'RELIANCE': 'NFO', 'HDFCBANK': 'NFO', 'ICICIBANK': 'NFO',\n",
    "        'INFY': 'NFO', 'TCS': 'NFO', 'SBIN': 'NFO',\n",
    "    })\n",
    "    lookback_days: int = 2500\n",
    "    window_size: int = 21\n",
    "    # Model\n",
    "    hidden_size: int = 128\n",
    "    num_heads: int = 4\n",
    "    dropout_rate: float = 0.2\n",
    "    # Training\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 50\n",
    "    learning_rate: float = 1e-3\n",
    "    early_stop_patience: int = 10\n",
    "    lr_reduce_patience: int = 5\n",
    "    lr_reduce_factor: float = 0.5\n",
    "    min_lr: float = 1e-6\n",
    "    clipnorm: float = 1.0\n",
    "    # Walk-forward\n",
    "    min_train_days: int = 504   # 2 years minimum\n",
    "    test_days: int = 63         # 3 months\n",
    "    purge_gap: int = 5\n",
    "    # Costs\n",
    "    bps_cost: float = 0.0010   # 10 bps per side\n",
    "    # Quick mode\n",
    "    quick_mode: bool = False    # True: 1 ticker, fewer epochs; False: full universe\n",
    "\n",
    "CFG = MonolithConfig()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Version tag — verify after Kernel Restart that you see this version\n",
    "# ---------------------------------------------------------------------------\n",
    "NOTEBOOK_VERSION = \"v2.1-2026-02-14\"\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Summary\n",
    "# ---------------------------------------------------------------------------\n",
    "print(\"=\" * 70)\n",
    "print(f\"QuantKubera Monolith {NOTEBOOK_VERSION}\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  GPU Available : {GPU_AVAILABLE} ({GPU_NAME})\")\n",
    "print(f\"  TensorFlow    : {tf.__version__}\")\n",
    "print(f\"  NumPy         : {np.__version__}\")\n",
    "print(f\"  Pandas        : {pd.__version__}\")\n",
    "print(f\"  Seed          : {SEED}\")\n",
    "print(f\"  Quick Mode    : {CFG.quick_mode}\")\n",
    "print(f\"  Tickers       : {CFG.tickers}\")\n",
    "print(f\"  Hidden Size   : {CFG.hidden_size}\")\n",
    "print(f\"  Num Heads     : {CFG.num_heads}\")\n",
    "print(f\"  Epochs        : {CFG.epochs}\")\n",
    "print(f\"  Batch Size    : {CFG.batch_size}\")\n",
    "print(f\"  Walk-forward  : train>={CFG.min_train_days}d, test={CFG.test_days}d, purge={CFG.purge_gap}d\")\n",
    "print(f\"  Cost Model    : {CFG.bps_cost * 10000:.0f} bps per side\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c265875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Data Engine — KiteAuth + KiteFetcher\n",
    "# ============================================================================\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "class KiteAuth:\n",
    "    \"\"\"TOTP-based Zerodha authentication with token caching.\"\"\"\n",
    "\n",
    "    TOKEN_PATH = Path.home() / '.zerodha_access_token'\n",
    "    LOGIN_URL = 'https://kite.zerodha.com/api/login'\n",
    "    TWOFA_URL = 'https://kite.zerodha.com/api/twofa'\n",
    "    CONNECT_URL = 'https://kite.zerodha.com/connect/login'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.api_key = os.getenv('ZERODHA_API_KEY', '')\n",
    "        self.api_secret = os.getenv('ZERODHA_API_SECRET', '')\n",
    "        self.totp_secret = os.getenv('ZERODHA_TOTP_SECRET', '')\n",
    "        self.user_id = os.getenv('ZERODHA_USER_ID', '')\n",
    "        self.password = os.getenv('ZERODHA_PASSWORD', '')\n",
    "\n",
    "    def _load_cached_token(self) -> Optional[str]:\n",
    "        \"\"\"Load cached access token if it exists and was created today.\"\"\"\n",
    "        if not self.TOKEN_PATH.exists():\n",
    "            return None\n",
    "        stat = self.TOKEN_PATH.stat()\n",
    "        modified = datetime.fromtimestamp(stat.st_mtime).date()\n",
    "        if modified != datetime.now().date():\n",
    "            return None\n",
    "        try:\n",
    "            token_data = json.loads(self.TOKEN_PATH.read_text())\n",
    "            return token_data.get('access_token')\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            return None\n",
    "\n",
    "    def _save_token(self, access_token: str):\n",
    "        \"\"\"Cache the access token to disk.\"\"\"\n",
    "        self.TOKEN_PATH.write_text(json.dumps({\n",
    "            'access_token': access_token,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'user_id': self.user_id,\n",
    "        }))\n",
    "\n",
    "    def _auto_login(self) -> str:\n",
    "        \"\"\"Perform full auto-login flow and return access token.\"\"\"\n",
    "        session = requests.Session()\n",
    "\n",
    "        # Step 1: POST /api/login\n",
    "        logger.info(\"KiteAuth: Step 1 — login\")\n",
    "        resp = session.post(self.LOGIN_URL, data={\n",
    "            'user_id': self.user_id,\n",
    "            'password': self.password,\n",
    "        })\n",
    "        resp.raise_for_status()\n",
    "        login_data = resp.json()\n",
    "        if login_data.get('status') != 'success':\n",
    "            raise RuntimeError(f\"Login failed: {login_data}\")\n",
    "        request_id = login_data['data']['request_id']\n",
    "\n",
    "        # Step 2: POST /api/twofa with TOTP\n",
    "        logger.info(\"KiteAuth: Step 2 — TOTP 2FA\")\n",
    "        totp = pyotp.TOTP(self.totp_secret)\n",
    "        twofa_value = totp.now()\n",
    "        resp = session.post(self.TWOFA_URL, data={\n",
    "            'user_id': self.user_id,\n",
    "            'request_id': request_id,\n",
    "            'twofa_value': twofa_value,\n",
    "            'twofa_type': 'totp',\n",
    "        })\n",
    "        resp.raise_for_status()\n",
    "        twofa_data = resp.json()\n",
    "        if twofa_data.get('status') != 'success':\n",
    "            raise RuntimeError(f\"2FA failed: {twofa_data}\")\n",
    "\n",
    "        # Step 3: GET /connect/login to extract request_token\n",
    "        # Follow the full redirect chain — request_token appears in the\n",
    "        # final redirect to the registered callback URL.\n",
    "        logger.info(\"KiteAuth: Step 3 — extract request_token\")\n",
    "        from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "        resp = session.get(self.CONNECT_URL, params={\n",
    "            'api_key': self.api_key,\n",
    "            'v': '3',\n",
    "        }, allow_redirects=True)\n",
    "\n",
    "        # Search for request_token in the final URL and all redirect history\n",
    "        candidate_urls = [resp.url] + [r.headers.get('Location', '') for r in resp.history]\n",
    "        request_token = None\n",
    "        for url in candidate_urls:\n",
    "            parsed = urlparse(url)\n",
    "            params = parse_qs(parsed.query)\n",
    "            if 'request_token' in params:\n",
    "                request_token = params['request_token'][0]\n",
    "                break\n",
    "\n",
    "        if request_token is None:\n",
    "            raise RuntimeError(\n",
    "                f\"Could not extract request_token. \"\n",
    "                f\"Final URL: {resp.url}, \"\n",
    "                f\"History: {[r.url for r in resp.history]}\"\n",
    "            )\n",
    "\n",
    "        # Step 4: Generate session\n",
    "        logger.info(\"KiteAuth: Step 4 — generate session\")\n",
    "        kite = KiteConnect(api_key=self.api_key)\n",
    "        data = kite.generate_session(request_token, api_secret=self.api_secret)\n",
    "        access_token = data['access_token']\n",
    "\n",
    "        self._save_token(access_token)\n",
    "        logger.info(\"KiteAuth: login complete, token cached\")\n",
    "        return access_token\n",
    "\n",
    "    def get_session(self) -> KiteConnect:\n",
    "        \"\"\"Return an authenticated KiteConnect instance.\"\"\"\n",
    "        # Try cached token first\n",
    "        cached = self._load_cached_token()\n",
    "        if cached:\n",
    "            kite = KiteConnect(api_key=self.api_key)\n",
    "            kite.set_access_token(cached)\n",
    "            try:\n",
    "                kite.profile()\n",
    "                logger.info(\"KiteAuth: using cached token\")\n",
    "                return kite\n",
    "            except Exception:\n",
    "                logger.info(\"KiteAuth: cached token expired, re-logging in\")\n",
    "\n",
    "        # Full login\n",
    "        access_token = self._auto_login()\n",
    "        kite = KiteConnect(api_key=self.api_key)\n",
    "        kite.set_access_token(access_token)\n",
    "        return kite\n",
    "\n",
    "\n",
    "class KiteFetcher:\n",
    "    \"\"\"Fetches daily OHLCV data from Zerodha Kite.\"\"\"\n",
    "\n",
    "    MAX_CHUNK_DAYS = 1900  # API limit is 2000, use 1900 for safety\n",
    "\n",
    "    def __init__(self, kite: KiteConnect):\n",
    "        self.kite = kite\n",
    "        self._instruments_cache: Optional[pd.DataFrame] = None\n",
    "\n",
    "    def _get_instruments(self, exchange: str) -> pd.DataFrame:\n",
    "        \"\"\"Fetch and cache instruments list for the given exchange.\"\"\"\n",
    "        if self._instruments_cache is None:\n",
    "            all_instruments = self.kite.instruments()\n",
    "            self._instruments_cache = pd.DataFrame(all_instruments)\n",
    "        return self._instruments_cache[\n",
    "            self._instruments_cache['exchange'] == exchange\n",
    "        ].copy()\n",
    "\n",
    "    def _resolve_instrument(self, symbol: str, exchange: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Resolve symbol to instrument_token.\n",
    "        Try exact match first, then fuzzy match for derivatives (FUT, nearest expiry).\n",
    "        Returns (instrument_token, resolved_tradingsymbol).\n",
    "        \"\"\"\n",
    "        instruments = self._get_instruments(exchange)\n",
    "\n",
    "        # Exact match on tradingsymbol\n",
    "        exact = instruments[instruments['tradingsymbol'] == symbol]\n",
    "        if len(exact) > 0:\n",
    "            row = exact.iloc[0]\n",
    "            return str(row['instrument_token']), row['tradingsymbol']\n",
    "\n",
    "        # Try NFO/MCX-FUT: look for symbol + FUT with nearest expiry\n",
    "        # For index derivatives, check NFO exchange\n",
    "        fut_exchange = exchange\n",
    "        if exchange == 'NSE':\n",
    "            fut_exchange = 'NFO'\n",
    "            instruments = self._get_instruments(fut_exchange)\n",
    "        elif exchange == 'MCX':\n",
    "            # MCX futures are on MCX itself\n",
    "            pass\n",
    "\n",
    "        # Fuzzy match: tradingsymbol starts with symbol, segment contains FUT\n",
    "        fuzzy = instruments[\n",
    "            instruments['tradingsymbol'].str.startswith(symbol) &\n",
    "            (instruments['instrument_type'] == 'FUT')\n",
    "        ].copy()\n",
    "\n",
    "        if len(fuzzy) == 0:\n",
    "            # Try with exchange-specific naming\n",
    "            fuzzy = instruments[\n",
    "                instruments['tradingsymbol'].str.contains(symbol, case=False, na=False) &\n",
    "                (instruments['instrument_type'] == 'FUT')\n",
    "            ].copy()\n",
    "\n",
    "        if len(fuzzy) == 0:\n",
    "            raise ValueError(\n",
    "                f\"Could not resolve instrument: {symbol} on {exchange}/{fut_exchange}. \"\n",
    "                f\"Check symbol name and exchange.\"\n",
    "            )\n",
    "\n",
    "        # Pick the nearest expiry\n",
    "        if 'expiry' in fuzzy.columns:\n",
    "            fuzzy['expiry'] = pd.to_datetime(fuzzy['expiry'], errors='coerce')\n",
    "            fuzzy = fuzzy.dropna(subset=['expiry'])\n",
    "            fuzzy = fuzzy.sort_values('expiry')\n",
    "            # Pick the nearest future expiry (>= today)\n",
    "            today = pd.Timestamp.now().normalize()\n",
    "            future_expiries = fuzzy[fuzzy['expiry'] >= today]\n",
    "            if len(future_expiries) > 0:\n",
    "                row = future_expiries.iloc[0]\n",
    "            else:\n",
    "                row = fuzzy.iloc[-1]  # Fallback to latest\n",
    "        else:\n",
    "            row = fuzzy.iloc[0]\n",
    "\n",
    "        resolved = row['tradingsymbol']\n",
    "        token = str(row['instrument_token'])\n",
    "        print(f\"  Resolved {symbol} -> {resolved} (expiry: {row.get('expiry', 'N/A')})\")\n",
    "        return token, resolved\n",
    "\n",
    "    def fetch_daily(self, symbol: str, exchange: str, days: int = 2500) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Fetch daily OHLCV data for the given symbol.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        symbol : str\n",
    "            Trading symbol (e.g., 'NIFTY', 'BANKNIFTY', 'SILVER')\n",
    "        exchange : str\n",
    "            Exchange (e.g., 'NSE', 'MCX')\n",
    "        days : int\n",
    "            Number of calendar days to look back\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with index=date (tz-naive), columns: open, high, low, close, volume\n",
    "        \"\"\"\n",
    "        instrument_token, resolved_name = self._resolve_instrument(symbol, exchange)\n",
    "\n",
    "        end_date = datetime.now().date()\n",
    "        start_date = end_date - timedelta(days=days)\n",
    "\n",
    "        print(f\"  Fetching {resolved_name}: {start_date} to {end_date} ({days} calendar days)\")\n",
    "\n",
    "        all_records = []\n",
    "        chunk_start = start_date\n",
    "\n",
    "        while chunk_start < end_date:\n",
    "            chunk_end = min(chunk_start + timedelta(days=self.MAX_CHUNK_DAYS), end_date)\n",
    "\n",
    "            try:\n",
    "                records = self.kite.historical_data(\n",
    "                    instrument_token=int(instrument_token),\n",
    "                    from_date=chunk_start,\n",
    "                    to_date=chunk_end,\n",
    "                    interval='day',\n",
    "                    continuous=True,\n",
    "                )\n",
    "                all_records.extend(records)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"  Chunk {chunk_start}-{chunk_end} failed: {e}\")\n",
    "\n",
    "            chunk_start = chunk_end + timedelta(days=1)\n",
    "\n",
    "        if not all_records:\n",
    "            raise ValueError(f\"No data returned for {symbol} ({resolved_name})\")\n",
    "\n",
    "        df = pd.DataFrame(all_records)\n",
    "        df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "        df = df.set_index('date').sort_index()\n",
    "\n",
    "        # Standardize column names to lowercase\n",
    "        df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "        # Keep only OHLCV\n",
    "        keep_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "        for col in keep_cols:\n",
    "            if col not in df.columns:\n",
    "                df[col] = np.nan\n",
    "\n",
    "        df = df[keep_cols]\n",
    "        df = df[~df.index.duplicated(keep='last')]\n",
    "\n",
    "        print(f\"  {resolved_name}: {len(df)} bars, {df.index[0].date()} to {df.index[-1].date()}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 2b: Cross-Asset Data — News Sentiment + India VIX\n",
    "# ============================================================================\n",
    "#\n",
    "# Cross-asset features: COMMON across all tickers (one value per day).\n",
    "# Captures market-wide regime through:\n",
    "#   1. FinBERT sentiment of GDELT financial news headlines (9 features)\n",
    "#   2. India VIX fear gauge (3 features)\n",
    "#\n",
    "# Architecture:\n",
    "#   - Headlines fetched from GDELT DOC 2.0 API (free, no API key)\n",
    "#   - Scored with ProsusAI/finbert on GPU (~5ms/headline)\n",
    "#   - All data cached to disk for instant re-runs\n",
    "#   - T+1 causality: headlines from day D -> features for day D+1\n",
    "#   - India VIX fetched via Kite API (already authenticated)\n",
    "# ============================================================================\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# FinBERT availability check\n",
    "# ---------------------------------------------------------------------------\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "    HAS_FINBERT = True\n",
    "except ImportError:\n",
    "    HAS_FINBERT = False\n",
    "    logger.warning(\"torch/transformers not installed — sentiment features disabled\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Cache directories\n",
    "# ---------------------------------------------------------------------------\n",
    "_QK_CACHE_DIR = Path.home() / '.quantkubera'\n",
    "_HEADLINE_CACHE_DIR = _QK_CACHE_DIR / 'headlines'\n",
    "_SCORE_CACHE_DIR = _QK_CACHE_DIR / 'sentiment_scores'\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# GDELT DOC 2.0 API\n",
    "# ---------------------------------------------------------------------------\n",
    "_GDELT_ENDPOINT = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "_GDELT_DELAY = 2.0  # seconds between requests\n",
    "\n",
    "# Focused queries: India markets + commodities + global macro + crypto\n",
    "_GDELT_QUERIES = [\n",
    "    # India markets — covers NIFTY, BANKNIFTY, Sensex, broad market\n",
    "    '\"NIFTY\" OR \"Sensex\" OR \"BSE\" OR \"NSE\" OR \"Indian stock market\" '\n",
    "    'sourceCountry:IN sourcelang:eng',\n",
    "    # India FnO stocks\n",
    "    '\"Reliance\" OR \"TCS\" OR \"HDFC\" OR \"Infosys\" OR \"ICICI\" OR \"SBI\" '\n",
    "    'sourceCountry:IN sourcelang:eng',\n",
    "    # Commodities — gold, oil, metals (relevant for MCX tickers)\n",
    "    '\"gold price\" OR \"crude oil\" OR \"silver price\" OR \"copper\" OR \"natural gas\" '\n",
    "    'sourcelang:eng',\n",
    "    # Global macro — Fed, rates, trade (affects all markets)\n",
    "    '\"Federal Reserve\" OR \"interest rate\" OR \"global markets\" OR \"trade war\" '\n",
    "    'OR \"tariffs\" sourcelang:eng',\n",
    "    # Crypto\n",
    "    '\"bitcoin\" OR \"ethereum\" OR \"cryptocurrency\" sourcelang:eng',\n",
    "]\n",
    "\n",
    "\n",
    "def _gdelt_fetch_articles(query: str, start_dt: datetime,\n",
    "                           end_dt: datetime) -> list:\n",
    "    \"\"\"Fetch articles from GDELT DOC 2.0 API (single request, max 250).\"\"\"\n",
    "    params = {\n",
    "        'query': query,\n",
    "        'mode': 'ArtList',\n",
    "        'format': 'json',\n",
    "        'maxrecords': 250,\n",
    "        'startdatetime': start_dt.strftime('%Y%m%d%H%M%S'),\n",
    "        'enddatetime': end_dt.strftime('%Y%m%d%H%M%S'),\n",
    "        'sort': 'DateDesc',\n",
    "    }\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            resp = requests.get(_GDELT_ENDPOINT, params=params, timeout=30,\n",
    "                                headers={'User-Agent': 'QuantKubera/2.1'})\n",
    "            if resp.status_code == 429:\n",
    "                wait = 5 * (attempt + 1)\n",
    "                time.sleep(wait)\n",
    "                continue\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            return data.get('articles', [])\n",
    "        except requests.exceptions.JSONDecodeError:\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"GDELT fetch failed (attempt {attempt+1}/3): {e}\")\n",
    "            if attempt < 2:\n",
    "                time.sleep(3)\n",
    "    return []\n",
    "\n",
    "\n",
    "def _parse_gdelt_date(seendate: str) -> Optional[datetime]:\n",
    "    \"\"\"Parse GDELT seendate like '20260210T083000Z'.\"\"\"\n",
    "    try:\n",
    "        clean = seendate.replace('T', '').replace('Z', '')\n",
    "        return datetime.strptime(clean, '%Y%m%d%H%M%S')\n",
    "    except (ValueError, AttributeError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def fetch_gdelt_headlines(start_date: str, end_date: str,\n",
    "                          chunk_days: int = 14) -> Dict[str, list]:\n",
    "    \"\"\"Fetch financial news headlines from GDELT with disk caching.\n",
    "\n",
    "    Args:\n",
    "        start_date, end_date: 'YYYY-MM-DD'\n",
    "        chunk_days: calendar days per API request\n",
    "\n",
    "    Returns: {date_str: [{'title': str, 'source': str}]}\n",
    "    \"\"\"\n",
    "    _HEADLINE_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Check disk cache\n",
    "    cache_file = _HEADLINE_CACHE_DIR / f\"gdelt_{start_date}_{end_date}.json\"\n",
    "    if cache_file.exists():\n",
    "        try:\n",
    "            with open(cache_file) as f:\n",
    "                cached = json.load(f)\n",
    "            n_total = sum(len(v) for v in cached.values())\n",
    "            print(f\"  GDELT cache hit: {n_total} headlines across \"\n",
    "                  f\"{len(cached)} days\")\n",
    "            return cached\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            pass\n",
    "\n",
    "    start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_dt = datetime.strptime(end_date, '%Y-%m-%d').replace(\n",
    "        hour=23, minute=59, second=59)\n",
    "\n",
    "    seen_titles = set()\n",
    "    daily_headlines: Dict[str, list] = {}\n",
    "    total = 0\n",
    "\n",
    "    n_queries = len(_GDELT_QUERIES)\n",
    "    n_chunks_per_query = max(1, (end_dt - start_dt).days // chunk_days + 1)\n",
    "    total_requests = n_queries * n_chunks_per_query\n",
    "\n",
    "    pbar = tqdm(total=total_requests, desc=\"GDELT headlines\",\n",
    "                unit=\"req\", leave=False)\n",
    "\n",
    "    for q_idx, query in enumerate(_GDELT_QUERIES):\n",
    "        chunk_start = start_dt\n",
    "        while chunk_start < end_dt:\n",
    "            chunk_end = min(\n",
    "                chunk_start + timedelta(days=chunk_days), end_dt)\n",
    "\n",
    "            articles = _gdelt_fetch_articles(query, chunk_start, chunk_end)\n",
    "\n",
    "            for art in articles:\n",
    "                title = art.get('title', '').strip()\n",
    "                if not title or len(title) < 15:\n",
    "                    continue\n",
    "\n",
    "                norm = title.lower().strip()\n",
    "                if norm in seen_titles:\n",
    "                    continue\n",
    "                seen_titles.add(norm)\n",
    "\n",
    "                dt = _parse_gdelt_date(art.get('seendate', ''))\n",
    "                if dt is None:\n",
    "                    continue\n",
    "\n",
    "                date_str = dt.strftime('%Y-%m-%d')\n",
    "                if date_str not in daily_headlines:\n",
    "                    daily_headlines[date_str] = []\n",
    "\n",
    "                daily_headlines[date_str].append({\n",
    "                    'title': title,\n",
    "                    'source': art.get('domain', 'unknown'),\n",
    "                })\n",
    "                total += 1\n",
    "\n",
    "            chunk_start = chunk_end + timedelta(days=1)\n",
    "            time.sleep(_GDELT_DELAY)\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix_str(f\"{total} headlines\")\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "    # Save cache\n",
    "    with open(cache_file, 'w') as f:\n",
    "        json.dump(daily_headlines, f)\n",
    "\n",
    "    print(f\"  GDELT: {total} unique headlines across {len(daily_headlines)} days\")\n",
    "    return daily_headlines\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# FinBERT Sentiment Scorer with disk caching\n",
    "# ---------------------------------------------------------------------------\n",
    "_finbert_model = None\n",
    "_finbert_tokenizer = None\n",
    "_finbert_device = None\n",
    "\n",
    "\n",
    "def _load_finbert():\n",
    "    \"\"\"Load ProsusAI/finbert model (singleton, GPU if available).\"\"\"\n",
    "    global _finbert_model, _finbert_tokenizer, _finbert_device\n",
    "    if _finbert_model is not None:\n",
    "        return\n",
    "\n",
    "    if not HAS_FINBERT:\n",
    "        raise RuntimeError(\"torch/transformers not installed\")\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"  Loading ProsusAI/finbert on {device}...\")\n",
    "\n",
    "    _finbert_tokenizer = AutoTokenizer.from_pretrained('ProsusAI/finbert')\n",
    "    _finbert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        'ProsusAI/finbert')\n",
    "    _finbert_model.to(device)\n",
    "    _finbert_model.eval()\n",
    "    _finbert_device = device\n",
    "    print(f\"  FinBERT loaded on {device}\")\n",
    "\n",
    "\n",
    "def _finbert_score_batch(texts: list, batch_size: int = 64) -> list:\n",
    "    \"\"\"Score texts with FinBERT. Returns [(score, confidence, label), ...]\n",
    "\n",
    "    score: float in [-1, +1] (positive - negative probability)\n",
    "    confidence: float in [0, 1] (max class probability)\n",
    "    label: str \"positive\" | \"negative\" | \"neutral\"\n",
    "    \"\"\"\n",
    "    _load_finbert()\n",
    "    label_map = {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
    "    results = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        inputs = _finbert_tokenizer(\n",
    "            batch, padding=True, truncation=True,\n",
    "            max_length=128, return_tensors='pt'\n",
    "        ).to(_finbert_device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = _finbert_model(**inputs)\n",
    "            probs = torch.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "        for j in range(len(batch)):\n",
    "            p = probs[j]\n",
    "            pos, neg = p[0].item(), p[1].item()\n",
    "            score = pos - neg\n",
    "            confidence = p.max().item()\n",
    "            label = label_map[p.argmax().item()]\n",
    "            results.append((score, confidence, label))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _title_hash(title: str) -> str:\n",
    "    \"\"\"Deterministic hash for cache key.\"\"\"\n",
    "    return hashlib.sha256(title.strip().lower().encode()).hexdigest()[:16]\n",
    "\n",
    "\n",
    "def score_headlines_with_cache(\n",
    "    daily_headlines: Dict[str, list],\n",
    ") -> Dict[str, list]:\n",
    "    \"\"\"Score all headlines with FinBERT, using disk cache.\n",
    "\n",
    "    Args:\n",
    "        daily_headlines: {date_str: [{'title': str, 'source': str}]}\n",
    "\n",
    "    Returns: {date_str: [(score, confidence, label), ...]}\n",
    "    \"\"\"\n",
    "    if not HAS_FINBERT:\n",
    "        return {}\n",
    "\n",
    "    _SCORE_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    cache_file = _SCORE_CACHE_DIR / 'finbert_scores.json'\n",
    "\n",
    "    # Load existing cache {title_hash: [score, confidence, label]}\n",
    "    score_cache: Dict[str, list] = {}\n",
    "    if cache_file.exists():\n",
    "        try:\n",
    "            with open(cache_file) as f:\n",
    "                score_cache = json.load(f)\n",
    "        except (json.JSONDecodeError, KeyError):\n",
    "            pass\n",
    "\n",
    "    # Collect uncached titles\n",
    "    uncached = []  # (date_str, idx, title, hash)\n",
    "    for date_str, headlines in daily_headlines.items():\n",
    "        for idx, h in enumerate(headlines):\n",
    "            th = _title_hash(h['title'])\n",
    "            if th not in score_cache:\n",
    "                uncached.append((date_str, idx, h['title'], th))\n",
    "\n",
    "    if uncached:\n",
    "        print(f\"  Scoring {len(uncached)} uncached headlines with FinBERT...\")\n",
    "        titles = [u[2] for u in uncached]\n",
    "        scores = _finbert_score_batch(titles)\n",
    "\n",
    "        for (date_str, idx, title, th), (score, conf, label) in zip(\n",
    "                uncached, scores):\n",
    "            score_cache[th] = [score, conf, label]\n",
    "\n",
    "        # Persist cache\n",
    "        with open(cache_file, 'w') as f:\n",
    "            json.dump(score_cache, f)\n",
    "        print(f\"  Scored {len(uncached)} headlines, cache now \"\n",
    "              f\"{len(score_cache)} entries\")\n",
    "    else:\n",
    "        print(f\"  All {sum(len(v) for v in daily_headlines.values())} \"\n",
    "              f\"headlines already cached\")\n",
    "\n",
    "    # Build result\n",
    "    result: Dict[str, list] = {}\n",
    "    for date_str, headlines in daily_headlines.items():\n",
    "        day_scores = []\n",
    "        for h in headlines:\n",
    "            th = _title_hash(h['title'])\n",
    "            if th in score_cache:\n",
    "                day_scores.append(tuple(score_cache[th]))\n",
    "            else:\n",
    "                day_scores.append((0.0, 0.5, 'neutral'))\n",
    "        result[date_str] = day_scores\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Sentiment Feature Builder (9 features, T+1 lag)\n",
    "# ---------------------------------------------------------------------------\n",
    "SENTIMENT_COLUMNS = [\n",
    "    'ns_sent_mean', 'ns_sent_std', 'ns_pos_ratio', 'ns_neg_ratio',\n",
    "    'ns_confidence_mean', 'ns_news_count',\n",
    "    'ns_sent_5d_ma', 'ns_news_count_5d', 'ns_sent_momentum',\n",
    "]\n",
    "\n",
    "\n",
    "def build_sentiment_features(\n",
    "    daily_scores: Dict[str, list],\n",
    "    date_index: pd.DatetimeIndex,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Build 9 daily sentiment features with T+1 causal lag.\n",
    "\n",
    "    Headlines from day D -> features for day D+1 (zero look-ahead).\n",
    "\n",
    "    Args:\n",
    "        daily_scores: {date_str: [(score, confidence, label), ...]}\n",
    "        date_index: DatetimeIndex to align features to\n",
    "\n",
    "    Returns: DataFrame with SENTIMENT_COLUMNS, indexed by date\n",
    "    \"\"\"\n",
    "    if not daily_scores:\n",
    "        return pd.DataFrame(columns=SENTIMENT_COLUMNS,\n",
    "                            index=date_index, dtype=float)\n",
    "\n",
    "    rows = []\n",
    "    for date_str, scores in sorted(daily_scores.items()):\n",
    "        if not scores:\n",
    "            continue\n",
    "        scores_arr = np.array([s[0] for s in scores])\n",
    "        confs_arr = np.array([s[1] for s in scores])\n",
    "        labels = [s[2] for s in scores]\n",
    "        n = len(scores_arr)\n",
    "        n_pos = sum(1 for l in labels if l == 'positive')\n",
    "        n_neg = sum(1 for l in labels if l == 'negative')\n",
    "\n",
    "        rows.append({\n",
    "            'date': pd.Timestamp(date_str),\n",
    "            'ns_sent_mean': float(np.mean(scores_arr)),\n",
    "            'ns_sent_std': float(np.std(scores_arr, ddof=1)) if n > 1 else 0.0,\n",
    "            'ns_pos_ratio': n_pos / n if n > 0 else 0.0,\n",
    "            'ns_neg_ratio': n_neg / n if n > 0 else 0.0,\n",
    "            'ns_confidence_mean': float(np.mean(confs_arr)),\n",
    "            'ns_news_count': float(n),\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=SENTIMENT_COLUMNS,\n",
    "                            index=date_index, dtype=float)\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index('date').sort_index()\n",
    "\n",
    "    # Rolling features (causal — use only past data)\n",
    "    df['ns_sent_5d_ma'] = df['ns_sent_mean'].rolling(5, min_periods=1).mean()\n",
    "    df['ns_news_count_5d'] = df['ns_news_count'].rolling(5, min_periods=1).sum()\n",
    "    df['ns_sent_momentum'] = df['ns_sent_mean'] - df['ns_sent_5d_ma']\n",
    "\n",
    "    # T+1 lag: headlines from day D become features for day D+1\n",
    "    df = df.shift(1)\n",
    "\n",
    "    # Align to target date index, forward-fill short gaps (weekends)\n",
    "    df = df.reindex(date_index)\n",
    "    df = df.ffill(limit=3)\n",
    "\n",
    "    return df[SENTIMENT_COLUMNS]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# India VIX Fetcher (via Kite API)\n",
    "# ---------------------------------------------------------------------------\n",
    "def fetch_india_vix_kite(kite: KiteConnect, lookback_days: int = 2500\n",
    "                         ) -> pd.DataFrame:\n",
    "    \"\"\"Fetch India VIX daily OHLCV via Kite API.\n",
    "\n",
    "    Uses the authenticated KiteConnect instance to fetch INDIA VIX from NSE.\n",
    "\n",
    "    Args:\n",
    "        kite: authenticated KiteConnect instance\n",
    "        lookback_days: calendar days of history\n",
    "\n",
    "    Returns: DataFrame with [vix_close, vix_open, vix_high, vix_low],\n",
    "             index=date (tz-naive DatetimeIndex)\n",
    "    \"\"\"\n",
    "    empty = pd.DataFrame(\n",
    "        columns=['vix_open', 'vix_high', 'vix_low', 'vix_close'])\n",
    "\n",
    "    # Resolve INDIA VIX instrument token\n",
    "    try:\n",
    "        instruments = pd.DataFrame(kite.instruments('NSE'))\n",
    "        vix_rows = instruments[\n",
    "            instruments['tradingsymbol'] == 'INDIA VIX']\n",
    "        if vix_rows.empty:\n",
    "            # Try partial match\n",
    "            vix_rows = instruments[\n",
    "                instruments['tradingsymbol'].str.contains(\n",
    "                    'VIX', case=False, na=False)]\n",
    "        if vix_rows.empty:\n",
    "            logger.warning(\"INDIA VIX instrument not found on NSE\")\n",
    "            return empty\n",
    "        token = int(vix_rows.iloc[0]['instrument_token'])\n",
    "        symbol = vix_rows.iloc[0]['tradingsymbol']\n",
    "        print(f\"  Resolved VIX: {symbol} (token={token})\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"VIX instrument resolution failed: {e}\")\n",
    "        return empty\n",
    "\n",
    "    end_date = datetime.now().date()\n",
    "    start_date = end_date - timedelta(days=lookback_days)\n",
    "\n",
    "    all_records = []\n",
    "    chunk_start = start_date\n",
    "    max_chunk = 1900\n",
    "\n",
    "    while chunk_start < end_date:\n",
    "        chunk_end = min(chunk_start + timedelta(days=max_chunk), end_date)\n",
    "        try:\n",
    "            records = kite.historical_data(\n",
    "                instrument_token=token,\n",
    "                from_date=chunk_start,\n",
    "                to_date=chunk_end,\n",
    "                interval='day',\n",
    "                continuous=False,  # VIX is an index, not futures\n",
    "            )\n",
    "            all_records.extend(records)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"VIX chunk {chunk_start}-{chunk_end} failed: {e}\")\n",
    "        chunk_start = chunk_end + timedelta(days=1)\n",
    "\n",
    "    if not all_records:\n",
    "        return empty\n",
    "\n",
    "    df = pd.DataFrame(all_records)\n",
    "    df['date'] = pd.to_datetime(df['date']).dt.tz_localize(None)\n",
    "    df = df.set_index('date').sort_index()\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "    # Rename to vix_* prefix\n",
    "    rename_map = {'close': 'vix_close', 'open': 'vix_open',\n",
    "                  'high': 'vix_high', 'low': 'vix_low'}\n",
    "    df = df.rename(columns=rename_map)\n",
    "    keep = [c for c in ['vix_open', 'vix_high', 'vix_low', 'vix_close']\n",
    "            if c in df.columns]\n",
    "    df = df[keep]\n",
    "    df = df[~df.index.duplicated(keep='last')]\n",
    "\n",
    "    print(f\"  India VIX: {len(df)} days \"\n",
    "          f\"({df.index[0].date()} to {df.index[-1].date()})\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# VIX Feature Builder (3 features)\n",
    "# ---------------------------------------------------------------------------\n",
    "VIX_COLUMNS = ['vix_level', 'vix_change_5d', 'vix_mean_reversion']\n",
    "\n",
    "\n",
    "def build_vix_features(vix_df: pd.DataFrame,\n",
    "                       date_index: pd.DatetimeIndex) -> pd.DataFrame:\n",
    "    \"\"\"Build 3 VIX features.\n",
    "\n",
    "    Args:\n",
    "        vix_df: DataFrame with vix_close column\n",
    "        date_index: DatetimeIndex to align to\n",
    "\n",
    "    Returns: DataFrame with VIX_COLUMNS\n",
    "    \"\"\"\n",
    "    if vix_df.empty or 'vix_close' not in vix_df.columns:\n",
    "        return pd.DataFrame(columns=VIX_COLUMNS,\n",
    "                            index=date_index, dtype=float)\n",
    "\n",
    "    vix = vix_df['vix_close'].copy()\n",
    "    result = pd.DataFrame(index=vix.index)\n",
    "\n",
    "    # VIX z-score: (VIX - 60d mean) / 60d std\n",
    "    vix_mean = vix.rolling(60, min_periods=20).mean()\n",
    "    vix_std = vix.rolling(60, min_periods=20).std(ddof=1)\n",
    "    vix_std = vix_std.replace(0, np.nan)\n",
    "    result['vix_level'] = (vix - vix_mean) / vix_std\n",
    "\n",
    "    # 5-day VIX change (percent)\n",
    "    result['vix_change_5d'] = vix.pct_change(5)\n",
    "\n",
    "    # VIX mean reversion: (VIX - 20d MA) / 20d std\n",
    "    vix_ma20 = vix.rolling(20, min_periods=10).mean()\n",
    "    vix_std20 = vix.rolling(20, min_periods=10).std(ddof=1)\n",
    "    vix_std20 = vix_std20.replace(0, np.nan)\n",
    "    result['vix_mean_reversion'] = (vix - vix_ma20) / vix_std20\n",
    "\n",
    "    # Align to target date index, forward-fill short gaps\n",
    "    result = result.reindex(date_index)\n",
    "    result = result.ffill(limit=3)\n",
    "\n",
    "    return result[VIX_COLUMNS]\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Master Cross-Asset Function\n",
    "# ---------------------------------------------------------------------------\n",
    "CROSS_ASSET_COLUMNS: List[str] = []  # populated at runtime\n",
    "\n",
    "\n",
    "def fetch_cross_asset_features(\n",
    "    start_date: str,\n",
    "    end_date: str,\n",
    "    date_index: pd.DatetimeIndex,\n",
    "    kite: Optional[KiteConnect] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Fetch all cross-asset data and build features.\n",
    "\n",
    "    Populates the global CROSS_ASSET_COLUMNS with available feature names.\n",
    "\n",
    "    Args:\n",
    "        start_date, end_date: 'YYYY-MM-DD'\n",
    "        date_index: DatetimeIndex to align all features to\n",
    "        kite: authenticated KiteConnect (for VIX)\n",
    "\n",
    "    Returns: DataFrame with available cross-asset features\n",
    "    \"\"\"\n",
    "    global CROSS_ASSET_COLUMNS\n",
    "\n",
    "    all_features = pd.DataFrame(index=date_index)\n",
    "    available_cols: List[str] = []\n",
    "\n",
    "    # --- News Sentiment (GDELT + FinBERT) ---\n",
    "    print(\"\\n  [1/2] News Sentiment (GDELT + FinBERT)\")\n",
    "    try:\n",
    "        daily_headlines = fetch_gdelt_headlines(start_date, end_date)\n",
    "\n",
    "        if daily_headlines and HAS_FINBERT:\n",
    "            daily_scores = score_headlines_with_cache(daily_headlines)\n",
    "            sent_df = build_sentiment_features(daily_scores, date_index)\n",
    "\n",
    "            for col in SENTIMENT_COLUMNS:\n",
    "                if col in sent_df.columns:\n",
    "                    all_features[col] = sent_df[col]\n",
    "                    available_cols.append(col)\n",
    "\n",
    "            n_days = sent_df.notna().any(axis=1).sum()\n",
    "            print(f\"    -> {len(available_cols)} features, \"\n",
    "                  f\"{n_days} days with data\")\n",
    "        elif not HAS_FINBERT:\n",
    "            print(\"    SKIP: FinBERT not available \"\n",
    "                  \"(pip install torch transformers)\")\n",
    "        else:\n",
    "            print(\"    SKIP: No headlines fetched from GDELT\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Sentiment features failed: {e}\")\n",
    "        print(f\"    SKIP: Failed ({e})\")\n",
    "\n",
    "    # --- India VIX ---\n",
    "    print(\"  [2/2] India VIX\")\n",
    "    try:\n",
    "        if kite is not None:\n",
    "            vix_df = fetch_india_vix_kite(kite)\n",
    "            if not vix_df.empty:\n",
    "                vix_feats = build_vix_features(vix_df, date_index)\n",
    "                for col in VIX_COLUMNS:\n",
    "                    if col in vix_feats.columns:\n",
    "                        all_features[col] = vix_feats[col]\n",
    "                        available_cols.append(col)\n",
    "                n_days = vix_feats.notna().any(axis=1).sum()\n",
    "                print(f\"    -> {len(VIX_COLUMNS)} features, \"\n",
    "                      f\"{n_days} days with data\")\n",
    "            else:\n",
    "                print(\"    SKIP: No VIX data from Kite\")\n",
    "        else:\n",
    "            print(\"    SKIP: No Kite instance provided\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"VIX features failed: {e}\")\n",
    "        print(f\"    SKIP: Failed ({e})\")\n",
    "\n",
    "    CROSS_ASSET_COLUMNS = available_cols\n",
    "\n",
    "    print(f\"\\n  Cross-Asset Summary: {len(available_cols)} features available\")\n",
    "    if available_cols:\n",
    "        print(f\"    Columns: {available_cols}\")\n",
    "\n",
    "    if available_cols:\n",
    "        return all_features[available_cols]\n",
    "    return pd.DataFrame(index=date_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1ffd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Advanced Feature Engineering — 31 Features in 10 Groups\n",
    "# ============================================================================\n",
    "\n",
    "FEATURE_COLUMNS = [\n",
    "    'norm_ret_1d', 'norm_ret_21d', 'norm_ret_63d', 'norm_ret_126d', 'norm_ret_252d',\n",
    "    'macd_8_24', 'macd_16_48', 'macd_32_96',\n",
    "    'rvol_20d', 'rvol_60d', 'gk_vol_20d', 'parkinson_vol_20d',\n",
    "    'cp_rl_21', 'cp_score_21',\n",
    "    'frac_diff_03', 'frac_diff_05', 'hurst_exp',\n",
    "    'ram_5', 'ram_10', 'ram_21', 'ram_63',\n",
    "    'vpin', 'kyles_lambda', 'amihud_illiq', 'hl_spread',\n",
    "    'entropy',\n",
    "    'trend_strength', 'momentum_consistency', 'mr_zscore',\n",
    "    'vol_zscore', 'vol_momentum',\n",
    "]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 1: Normalized Returns (5 features)\n",
    "# ============================================================================\n",
    "def compute_returns(close: pd.Series, horizons: List[int] = [1, 21, 63, 126, 252]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute horizon returns normalized by EWM volatility.\n",
    "    norm_ret_h = log(close / close.shift(h)) / (vol * sqrt(h))\n",
    "    where vol = ewm(span=60).std() of daily log returns.\n",
    "    \"\"\"\n",
    "    log_ret = np.log(close / close.shift(1))\n",
    "    vol = log_ret.ewm(span=60, min_periods=20).std()\n",
    "\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "    for h in horizons:\n",
    "        raw_ret = np.log(close / close.shift(h))\n",
    "        denom = vol * np.sqrt(h)\n",
    "        # Avoid division by zero\n",
    "        denom = denom.replace(0, np.nan)\n",
    "        result[f'norm_ret_{h}d'] = raw_ret / denom\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 2: MACD (3 features)\n",
    "# ============================================================================\n",
    "def compute_macd(close: pd.Series, pairs: List[Tuple[int, int]] = [(8, 24), (16, 48), (32, 96)]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute MACD z-scores for stationarity.\n",
    "    Raw MACD = EWM(fast) - EWM(slow), then z-scored over 126-day rolling window.\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "    for fast, slow in pairs:\n",
    "        ema_fast = close.ewm(span=fast, min_periods=fast).mean()\n",
    "        ema_slow = close.ewm(span=slow, min_periods=slow).mean()\n",
    "        raw_macd = ema_fast - ema_slow\n",
    "\n",
    "        roll_mean = raw_macd.rolling(window=126, min_periods=63).mean()\n",
    "        roll_std = raw_macd.rolling(window=126, min_periods=63).std(ddof=1)\n",
    "        roll_std = roll_std.replace(0, np.nan)\n",
    "\n",
    "        result[f'macd_{fast}_{slow}'] = (raw_macd - roll_mean) / roll_std\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 3: Volatility (4 features)\n",
    "# ============================================================================\n",
    "def compute_volatility(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute 4 volatility estimators:\n",
    "      - rvol_20d, rvol_60d: realized vol (rolling std of log returns, annualized)\n",
    "      - gk_vol_20d: Garman-Klass volatility\n",
    "      - parkinson_vol_20d: Parkinson high-low volatility\n",
    "    \"\"\"\n",
    "    close = df['close']\n",
    "    high = df['high']\n",
    "    low = df['low']\n",
    "    opn = df['open']\n",
    "\n",
    "    log_ret = np.log(close / close.shift(1))\n",
    "\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # Realized vol\n",
    "    result['rvol_20d'] = log_ret.rolling(window=20, min_periods=15).std(ddof=1) * np.sqrt(252)\n",
    "    result['rvol_60d'] = log_ret.rolling(window=60, min_periods=40).std(ddof=1) * np.sqrt(252)\n",
    "\n",
    "    # Garman-Klass: sqrt(mean(0.5*ln(H/L)^2 - (2*ln2 - 1)*ln(C/O)^2) * 252)\n",
    "    log_hl = np.log(high / low)\n",
    "    log_co = np.log(close / opn)\n",
    "    gk_term = 0.5 * log_hl ** 2 - (2.0 * np.log(2.0) - 1.0) * log_co ** 2\n",
    "    gk_mean = gk_term.rolling(window=20, min_periods=15).mean()\n",
    "    # Clamp to non-negative before sqrt\n",
    "    gk_mean = gk_mean.clip(lower=0.0)\n",
    "    result['gk_vol_20d'] = np.sqrt(gk_mean * 252)\n",
    "\n",
    "    # Parkinson: sqrt(mean(ln(H/L)^2 / (4*ln2)) * 252)\n",
    "    park_term = log_hl ** 2 / (4.0 * np.log(2.0))\n",
    "    park_mean = park_term.rolling(window=20, min_periods=15).mean()\n",
    "    park_mean = park_mean.clip(lower=0.0)\n",
    "    result['parkinson_vol_20d'] = np.sqrt(park_mean * 252)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 4: Changepoint Detection (2 features)\n",
    "# ============================================================================\n",
    "def nig_log_marginal(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Normal-Inverse-Gamma log marginal likelihood P(x | NIG prior).\n",
    "    Prior: mu0=0, kappa0=1, alpha0=1, beta0=1\n",
    "    Posterior update:\n",
    "      n = len(x), x_bar = mean(x), s2 = var(x)\n",
    "      kappa_n = kappa0 + n\n",
    "      alpha_n = alpha0 + n/2\n",
    "      beta_n = beta0 + 0.5*n*s2 + 0.5*kappa0*n*x_bar^2 / kappa_n\n",
    "    Log marginal = gammaln(alpha_n) - gammaln(alpha0) + alpha0*log(beta0) - alpha_n*log(beta_n)\n",
    "                   + 0.5*log(kappa0/kappa_n) - (n/2)*log(2*pi)\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    if n < 2:\n",
    "        return -np.inf\n",
    "\n",
    "    mu0, kappa0, alpha0, beta0 = 0.0, 1.0, 1.0, 1.0\n",
    "\n",
    "    x_bar = np.mean(x)\n",
    "    s2 = np.var(x, ddof=1) if n > 1 else 0.0\n",
    "\n",
    "    kappa_n = kappa0 + n\n",
    "    alpha_n = alpha0 + n / 2.0\n",
    "    beta_n = beta0 + 0.5 * (n - 1) * s2 + 0.5 * kappa0 * n * x_bar ** 2 / kappa_n\n",
    "\n",
    "    # Protect against non-positive beta_n\n",
    "    if beta_n <= 0:\n",
    "        beta_n = 1e-300\n",
    "\n",
    "    log_ml = (\n",
    "        gammaln(alpha_n) - gammaln(alpha0)\n",
    "        + alpha0 * np.log(beta0) - alpha_n * np.log(beta_n)\n",
    "        + 0.5 * np.log(kappa0 / kappa_n)\n",
    "        - (n / 2.0) * np.log(2.0 * np.pi)\n",
    "    )\n",
    "    return log_ml\n",
    "\n",
    "\n",
    "def compute_cpd(close: pd.Series, lookback: int = 21, min_seg: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Changepoint detection via NIG Bayesian model comparison.\n",
    "    For each position, take lookback-window of log returns.\n",
    "    Try all split points; best split maximizes sum of two-segment likelihoods.\n",
    "    cp_rl = best_split_position / lookback (relative location in [0, 1])\n",
    "    cp_score = sigmoid(best_split_ll - full_ll) (severity in [0, 1])\n",
    "    \"\"\"\n",
    "    log_ret = np.log(close / close.shift(1)).values\n",
    "    n = len(log_ret)\n",
    "\n",
    "    cp_rl = np.full(n, np.nan)\n",
    "    cp_score = np.full(n, np.nan)\n",
    "\n",
    "    for i in range(lookback, n):\n",
    "        window = log_ret[i - lookback + 1: i + 1]  # lookback values ending at i\n",
    "\n",
    "        # Skip if any NaN\n",
    "        if np.any(np.isnan(window)):\n",
    "            continue\n",
    "\n",
    "        full_ll = nig_log_marginal(window)\n",
    "\n",
    "        best_split_ll = -np.inf\n",
    "        best_split_pos = lookback // 2  # default to middle\n",
    "\n",
    "        for s in range(min_seg, lookback - min_seg + 1):\n",
    "            left = window[:s]\n",
    "            right = window[s:]\n",
    "            split_ll = nig_log_marginal(left) + nig_log_marginal(right)\n",
    "\n",
    "            if split_ll > best_split_ll:\n",
    "                best_split_ll = split_ll\n",
    "                best_split_pos = s\n",
    "\n",
    "        cp_rl[i] = best_split_pos / lookback\n",
    "\n",
    "        # Severity: sigmoid of log-likelihood ratio\n",
    "        delta = best_split_ll - full_ll\n",
    "        # Clamp to avoid overflow in exp\n",
    "        delta_clamped = np.clip(delta, -500, 500)\n",
    "        cp_score[i] = 1.0 / (1.0 + np.exp(-delta_clamped))\n",
    "\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "    result['cp_rl_21'] = cp_rl\n",
    "    result['cp_score_21'] = cp_score\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 5: Fractional Calculus (3 features)\n",
    "# ============================================================================\n",
    "def frac_diff_weights(d: float, thresh: float = 1e-5, max_width: int = 500) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Hosking (1981) fractional differencing weights.\n",
    "    w[0] = 1, w[k] = -w[k-1] * (d - k + 1) / k\n",
    "    Iterate until |w[k]| < thresh or max_width reached.\n",
    "\n",
    "    Note: For d=0.3 with thresh=1e-5, weights decay as k^{-1.3} requiring\n",
    "    ~7000 terms. max_width caps this to keep the warmup period practical\n",
    "    while preserving >99.9% of the filter energy.\n",
    "\n",
    "    Returns weights array from oldest (index 0) to newest (index -1).\n",
    "    \"\"\"\n",
    "    weights = [1.0]\n",
    "    k = 1\n",
    "    while True:\n",
    "        w_k = -weights[-1] * (d - k + 1) / k\n",
    "        if abs(w_k) < thresh:\n",
    "            break\n",
    "        weights.append(w_k)\n",
    "        k += 1\n",
    "        if k >= max_width:\n",
    "            break\n",
    "    # Reverse so index 0 = oldest weight\n",
    "    return np.array(weights[::-1])\n",
    "\n",
    "\n",
    "def compute_frac_diff(close: pd.Series, d: float) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply fractional differencing of order d to log(close).\n",
    "    Convolve log prices with frac_diff weights.\n",
    "    \"\"\"\n",
    "    log_price = np.log(close.values)\n",
    "    w = frac_diff_weights(d)\n",
    "    width = len(w)\n",
    "\n",
    "    n = len(log_price)\n",
    "    result = np.full(n, np.nan)\n",
    "\n",
    "    for i in range(width - 1, n):\n",
    "        segment = log_price[i - width + 1: i + 1]\n",
    "        result[i] = np.dot(w, segment)\n",
    "\n",
    "    return pd.Series(result, index=close.index)\n",
    "\n",
    "\n",
    "def _compute_hurst_single(returns: np.ndarray, max_lag: int = 50) -> float:\n",
    "    \"\"\"\n",
    "    Compute Hurst exponent from returns using MSD (Mean Squared Displacement).\n",
    "    MSD(tau) = E[(X(t+tau) - X(t))^2] where X = cumsum(returns)\n",
    "    Regress log(MSD) on log(tau) -> slope / 2 = H\n",
    "    \"\"\"\n",
    "    if len(returns) < max_lag + 10:\n",
    "        return np.nan\n",
    "\n",
    "    X = np.cumsum(returns)\n",
    "    taus = np.arange(1, max_lag + 1)\n",
    "    msd = np.full(max_lag, np.nan)\n",
    "\n",
    "    for idx, tau in enumerate(taus):\n",
    "        diffs = X[tau:] - X[:-tau]\n",
    "        if len(diffs) < 5:\n",
    "            continue\n",
    "        msd[idx] = np.mean(diffs ** 2)\n",
    "\n",
    "    # Filter valid MSD values (positive and finite)\n",
    "    valid = np.isfinite(msd) & (msd > 0)\n",
    "    if valid.sum() < 5:\n",
    "        return np.nan\n",
    "\n",
    "    log_tau = np.log(taus[valid])\n",
    "    log_msd = np.log(msd[valid])\n",
    "\n",
    "    # Linear regression: log_msd = slope * log_tau + intercept\n",
    "    n_valid = len(log_tau)\n",
    "    sum_x = np.sum(log_tau)\n",
    "    sum_y = np.sum(log_msd)\n",
    "    sum_xy = np.sum(log_tau * log_msd)\n",
    "    sum_xx = np.sum(log_tau ** 2)\n",
    "\n",
    "    denom = n_valid * sum_xx - sum_x ** 2\n",
    "    if abs(denom) < 1e-15:\n",
    "        return np.nan\n",
    "\n",
    "    slope = (n_valid * sum_xy - sum_x * sum_y) / denom\n",
    "    hurst = slope / 2.0\n",
    "\n",
    "    # Clamp to reasonable range\n",
    "    return np.clip(hurst, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def compute_hurst(close: pd.Series, window: int = 252, max_lag: int = 50) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Rolling Hurst exponent computed over a window of returns.\n",
    "    \"\"\"\n",
    "    log_ret = np.log(close / close.shift(1)).values\n",
    "    n = len(log_ret)\n",
    "    hurst_vals = np.full(n, np.nan)\n",
    "\n",
    "    for i in range(window, n):\n",
    "        segment = log_ret[i - window + 1: i + 1]\n",
    "        if np.any(np.isnan(segment)):\n",
    "            continue\n",
    "        hurst_vals[i] = _compute_hurst_single(segment, max_lag=max_lag)\n",
    "\n",
    "    return pd.Series(hurst_vals, index=close.index, name='hurst_exp')\n",
    "\n",
    "\n",
    "def compute_fractional_features(close: pd.Series) -> pd.DataFrame:\n",
    "    \"\"\"Compute all fractional calculus features.\"\"\"\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "    result['frac_diff_03'] = compute_frac_diff(close, d=0.3)\n",
    "    result['frac_diff_05'] = compute_frac_diff(close, d=0.5)\n",
    "    result['hurst_exp'] = compute_hurst(close)\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 6: Ramanujan Sum Filter Bank (4 features)\n",
    "# ============================================================================\n",
    "def euler_phi(n: int) -> int:\n",
    "    \"\"\"Euler's totient function: count of integers in [1, n] coprime to n.\"\"\"\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    result = n\n",
    "    p = 2\n",
    "    temp = n\n",
    "    while p * p <= temp:\n",
    "        if temp % p == 0:\n",
    "            while temp % p == 0:\n",
    "                temp //= p\n",
    "            result -= result // p\n",
    "        p += 1\n",
    "    if temp > 1:\n",
    "        result -= result // temp\n",
    "    return result\n",
    "\n",
    "\n",
    "def mobius(n: int) -> int:\n",
    "    \"\"\"\n",
    "    Mobius function:\n",
    "      mu(1) = 1\n",
    "      mu(n) = 0 if n has a squared prime factor\n",
    "      mu(n) = (-1)^k if n is a product of k distinct primes\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        return 0\n",
    "    if n == 1:\n",
    "        return 1\n",
    "\n",
    "    num_factors = 0\n",
    "    temp = n\n",
    "    p = 2\n",
    "\n",
    "    while p * p <= temp:\n",
    "        if temp % p == 0:\n",
    "            temp //= p\n",
    "            num_factors += 1\n",
    "            if temp % p == 0:\n",
    "                return 0  # Squared factor\n",
    "        p += 1\n",
    "\n",
    "    if temp > 1:\n",
    "        num_factors += 1\n",
    "\n",
    "    return 1 if num_factors % 2 == 0 else -1\n",
    "\n",
    "\n",
    "def ramanujan_sum(q: int, n: int) -> float:\n",
    "    \"\"\"\n",
    "    Ramanujan sum c_q(n) = sum over d|gcd(n,q) of mu(q/d) * phi(q) / phi(q/d)\n",
    "    Simplified: c_q(n) = mu(q/g) * phi(q) / phi(q/g) where g = gcd(n, q)\n",
    "    Actually the full definition sums over all d dividing gcd(n,q).\n",
    "    \"\"\"\n",
    "    g = math.gcd(n, q)\n",
    "    # Sum over divisors d of g: mu(q/d) * d * (phi(q/d) != 0 check)\n",
    "    # More standard: c_q(n) = sum_{d | gcd(n,q)} d * mu(q/d)\n",
    "    total = 0.0\n",
    "    for d in range(1, g + 1):\n",
    "        if g % d == 0:\n",
    "            qd = q // d\n",
    "            total += d * mobius(qd)\n",
    "    return total\n",
    "\n",
    "\n",
    "def compute_ramanujan(close: pd.Series, periods: List[int] = [5, 10, 21, 63],\n",
    "                      window: int = 252) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ramanujan Sum Filter Bank: convolve log-returns with Ramanujan sum kernels\n",
    "    to extract energy at specific trading cycle periods.\n",
    "    \"\"\"\n",
    "    log_ret = np.log(close / close.shift(1)).values\n",
    "    n = len(log_ret)\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "\n",
    "    for q in periods:\n",
    "        # Pre-compute kernel: kernel[j] = c_q(j+1) for j in [0, window)\n",
    "        kernel = np.array([ramanujan_sum(q, j + 1) for j in range(window)])\n",
    "        kernel = kernel / window  # Normalize\n",
    "\n",
    "        # Convolve (causal: only use past data)\n",
    "        filtered = np.full(n, np.nan)\n",
    "        for i in range(window, n):\n",
    "            segment = log_ret[i - window + 1: i + 1]\n",
    "            if np.any(np.isnan(segment)):\n",
    "                continue\n",
    "            # Kernel is applied: newest data * kernel[0], oldest * kernel[-1]\n",
    "            # Reverse kernel for convolution alignment (kernel[0] applies to most recent)\n",
    "            filtered[i] = np.dot(segment, kernel[::-1][:len(segment)])\n",
    "\n",
    "        result[f'ram_{q}'] = filtered\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 7: Microstructure (4 features)\n",
    "# ============================================================================\n",
    "def compute_microstructure(df: pd.DataFrame, window: int = 50) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute 4 microstructure features:\n",
    "      - VPIN: Volume-Synchronized Probability of Informed Trading\n",
    "      - Kyle's Lambda: price impact coefficient\n",
    "      - Amihud Illiquidity: |return| / dollar volume\n",
    "      - HL Spread: high-low spread proxy (Corwin-Schultz simplified)\n",
    "    \"\"\"\n",
    "    close = df['close']\n",
    "    high = df['high']\n",
    "    low = df['low']\n",
    "    volume = df['volume'].astype(float)\n",
    "\n",
    "    log_ret = np.log(close / close.shift(1))\n",
    "\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # --- VPIN ---\n",
    "    sigma = log_ret.rolling(window=20, min_periods=10).std()\n",
    "    sigma = sigma.replace(0, np.nan)\n",
    "    # Bulk volume classification: buy probability = Phi(ret / sigma)\n",
    "    z = log_ret / sigma\n",
    "    buy_prob = pd.Series(norm.cdf(z.values), index=close.index)\n",
    "    buy_vol = volume * buy_prob\n",
    "    sell_vol = volume * (1.0 - buy_prob)\n",
    "    abs_imbalance = (buy_vol - sell_vol).abs()\n",
    "    total_vol = volume.rolling(window=window, min_periods=window // 2).sum()\n",
    "    total_vol = total_vol.replace(0, np.nan)\n",
    "    vpin = abs_imbalance.rolling(window=window, min_periods=window // 2).sum() / total_vol\n",
    "    result['vpin'] = vpin\n",
    "\n",
    "    # --- Kyle's Lambda ---\n",
    "    abs_ret = log_ret.abs()\n",
    "    signed_vol = np.sign(log_ret) * volume\n",
    "    abs_signed_vol = signed_vol.abs()\n",
    "\n",
    "    # Rolling covariance / variance\n",
    "    cov_rv = abs_ret.rolling(window=window, min_periods=window // 2).cov(abs_signed_vol)\n",
    "    var_sv = abs_signed_vol.rolling(window=window, min_periods=window // 2).var(ddof=1)\n",
    "    var_sv = var_sv.replace(0, np.nan)\n",
    "    result['kyles_lambda'] = cov_rv / var_sv\n",
    "\n",
    "    # --- Amihud Illiquidity ---\n",
    "    dollar_vol = close * volume\n",
    "    dollar_vol = dollar_vol.replace(0, np.nan)\n",
    "    daily_illiq = abs_ret / dollar_vol\n",
    "    result['amihud_illiq'] = daily_illiq.rolling(window=window, min_periods=window // 2).mean()\n",
    "\n",
    "    # --- HL Spread (Corwin-Schultz simplified) ---\n",
    "    # Use rolling average of log(H/L) as spread proxy\n",
    "    log_hl = np.log(high / low)\n",
    "    # Corwin-Schultz: alpha derived from 2-day high-low ratio\n",
    "    # Simplified version: spread = 2*(exp(alpha) - 1) / (1 + exp(alpha))\n",
    "    # where alpha = (sqrt(2*beta) - sqrt(beta)) / (3 - 2*sqrt(2))\n",
    "    # beta = E[ln(H_t/L_t)^2]\n",
    "    beta = (log_hl ** 2).rolling(window=window, min_periods=window // 2).mean()\n",
    "    # Also compute gamma from 2-day range\n",
    "    high_2d = high.rolling(window=2).max()\n",
    "    low_2d = low.rolling(window=2).min()\n",
    "    gamma = np.log(high_2d / low_2d) ** 2\n",
    "\n",
    "    # alpha = (sqrt(2) - 1) * sqrt(beta) / (3 - 2*sqrt(2)) when gamma term is small\n",
    "    # Full: alpha = (sqrt(2*beta) - sqrt(beta)) / (3 - 2*sqrt(2)) - sqrt(gamma / (3 - 2*sqrt(2)))\n",
    "    sqrt2 = np.sqrt(2.0)\n",
    "    denom_cs = 3.0 - 2.0 * sqrt2\n",
    "\n",
    "    # Ensure beta is non-negative\n",
    "    beta_safe = beta.clip(lower=0.0)\n",
    "    gamma_safe = gamma.clip(lower=0.0)\n",
    "\n",
    "    alpha = (np.sqrt(2.0 * beta_safe) - np.sqrt(beta_safe)) / denom_cs\n",
    "    # Adjust with gamma correction\n",
    "    gamma_correction = np.sqrt(gamma_safe / denom_cs)\n",
    "    alpha = alpha - gamma_correction\n",
    "    # Clamp alpha to reasonable range to avoid extreme spread values\n",
    "    alpha = alpha.clip(lower=-1.0, upper=2.0)\n",
    "\n",
    "    spread = 2.0 * (np.exp(alpha) - 1.0) / (1.0 + np.exp(alpha))\n",
    "    # Clamp negative spreads to 0\n",
    "    spread = spread.clip(lower=0.0)\n",
    "    result['hl_spread'] = spread\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 8: Information Theory (1 feature)\n",
    "# ============================================================================\n",
    "def compute_entropy(close: pd.Series, word_len: int = 3, window: int = 252) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Shannon entropy of binary price movement patterns.\n",
    "    Encode: 1 if price up, 0 if down.\n",
    "    Form words of word_len consecutive bits.\n",
    "    Compute normalized Shannon entropy over rolling window.\n",
    "    \"\"\"\n",
    "    # Binary encoding: 1 if close > prev_close, 0 otherwise\n",
    "    direction = (close.diff() > 0).astype(int).values\n",
    "    n = len(direction)\n",
    "    n_words = 2 ** word_len\n",
    "    max_entropy = np.log2(n_words) if n_words > 1 else 1.0\n",
    "\n",
    "    entropy_vals = np.full(n, np.nan)\n",
    "\n",
    "    for i in range(window + word_len - 1, n):\n",
    "        # Extract the window of directions\n",
    "        seg = direction[i - window + 1: i + 1]\n",
    "\n",
    "        # Build words\n",
    "        words = []\n",
    "        for j in range(word_len - 1, len(seg)):\n",
    "            word = 0\n",
    "            for k in range(word_len):\n",
    "                word = (word << 1) | seg[j - word_len + 1 + k]\n",
    "            words.append(word)\n",
    "\n",
    "        if len(words) == 0:\n",
    "            continue\n",
    "\n",
    "        # Histogram\n",
    "        counts = np.bincount(words, minlength=n_words).astype(float)\n",
    "        probs = counts / counts.sum()\n",
    "\n",
    "        # Shannon entropy (base 2)\n",
    "        probs_pos = probs[probs > 0]\n",
    "        H = -np.sum(probs_pos * np.log2(probs_pos))\n",
    "\n",
    "        # Normalize to [0, 1]\n",
    "        entropy_vals[i] = H / max_entropy if max_entropy > 0 else 0.0\n",
    "\n",
    "    return pd.Series(entropy_vals, index=close.index, name='entropy')\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 9: Momentum Quality (3 features)\n",
    "# ============================================================================\n",
    "def compute_momentum_quality(close: pd.Series, window: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute momentum quality metrics:\n",
    "      - trend_strength: |avg_up - avg_down| / (avg_up + avg_down)\n",
    "      - momentum_consistency: fraction of positive returns in rolling window\n",
    "      - mr_zscore: (close - EMA) / rolling_std  (mean-reversion z-score)\n",
    "    \"\"\"\n",
    "    ret = close.pct_change()\n",
    "    result = pd.DataFrame(index=close.index)\n",
    "\n",
    "    # Trend strength\n",
    "    up_ret = ret.clip(lower=0)\n",
    "    down_ret = (-ret).clip(lower=0)  # magnitude of down moves\n",
    "\n",
    "    avg_up = up_ret.rolling(window=window, min_periods=window // 2).mean()\n",
    "    avg_down = down_ret.rolling(window=window, min_periods=window // 2).mean()\n",
    "\n",
    "    denom_ts = avg_up + avg_down\n",
    "    denom_ts = denom_ts.replace(0, np.nan)\n",
    "    result['trend_strength'] = (avg_up - avg_down).abs() / denom_ts\n",
    "\n",
    "    # Momentum consistency: fraction of positive returns\n",
    "    pos_indicator = (ret > 0).astype(float)\n",
    "    result['momentum_consistency'] = pos_indicator.rolling(\n",
    "        window=window, min_periods=window // 2\n",
    "    ).mean()\n",
    "\n",
    "    # Mean reversion z-score: (close - EMA) / rolling_std\n",
    "    ema = close.ewm(span=window, min_periods=window // 2).mean()\n",
    "    rolling_std = close.rolling(window=window, min_periods=window // 2).std(ddof=1)\n",
    "    rolling_std = rolling_std.replace(0, np.nan)\n",
    "    result['mr_zscore'] = (close - ema) / rolling_std\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Group 10: Volume Features (2 features)\n",
    "# ============================================================================\n",
    "def compute_volume_features(df: pd.DataFrame, window: int = 20) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute volume-based features:\n",
    "      - vol_zscore: (volume - rolling_mean) / rolling_std\n",
    "      - vol_momentum: volume.pct_change(5)  (5-day volume momentum)\n",
    "    \"\"\"\n",
    "    volume = df['volume'].astype(float)\n",
    "    result = pd.DataFrame(index=df.index)\n",
    "\n",
    "    roll_mean = volume.rolling(window=window, min_periods=window // 2).mean()\n",
    "    roll_std = volume.rolling(window=window, min_periods=window // 2).std(ddof=1)\n",
    "    roll_std = roll_std.replace(0, np.nan)\n",
    "\n",
    "    result['vol_zscore'] = (volume - roll_mean) / roll_std\n",
    "    result['vol_momentum'] = volume.pct_change(periods=5)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Master Function: Build All Features\n",
    "# ============================================================================\n",
    "def build_all_features(df: pd.DataFrame, cfg: Optional[MonolithConfig] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build all 31 engineered features from 10 research groups.\n",
    "    Adds forward target: target_ret = close.pct_change(1).shift(-1)\n",
    "    Drops NaN warmup rows.\n",
    "    Returns df with FEATURE_COLUMNS + 'target_ret' + original OHLCV.\n",
    "    \"\"\"\n",
    "    if cfg is None:\n",
    "        cfg = MonolithConfig()\n",
    "\n",
    "    close = df['close']\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(\"Building features...\")\n",
    "\n",
    "    # Group 1: Normalized Returns\n",
    "    print(\"  [1/10] Normalized Returns (5 features)\")\n",
    "    feat_ret = compute_returns(close)\n",
    "\n",
    "    # Group 2: MACD\n",
    "    print(\"  [2/10] MACD Z-scores (3 features)\")\n",
    "    feat_macd = compute_macd(close)\n",
    "\n",
    "    # Group 3: Volatility\n",
    "    print(\"  [3/10] Volatility Estimators (4 features)\")\n",
    "    feat_vol = compute_volatility(df)\n",
    "\n",
    "    # Group 4: Changepoint Detection\n",
    "    print(\"  [4/10] NIG Changepoint Detection (2 features)\")\n",
    "    feat_cpd = compute_cpd(close)\n",
    "\n",
    "    # Group 5: Fractional Calculus\n",
    "    print(\"  [5/10] Fractional Differentiation + Hurst (3 features)\")\n",
    "    feat_frac = compute_fractional_features(close)\n",
    "\n",
    "    # Group 6: Ramanujan Filter Bank\n",
    "    print(\"  [6/10] Ramanujan Sum Filter Bank (4 features)\")\n",
    "    feat_ram = compute_ramanujan(close)\n",
    "\n",
    "    # Group 7: Microstructure\n",
    "    print(\"  [7/10] Market Microstructure (4 features)\")\n",
    "    feat_micro = compute_microstructure(df)\n",
    "\n",
    "    # Group 8: Entropy\n",
    "    print(\"  [8/10] Information-Theoretic Entropy (1 feature)\")\n",
    "    feat_entropy = compute_entropy(close)\n",
    "\n",
    "    # Group 9: Momentum Quality\n",
    "    print(\"  [9/10] Momentum Quality (3 features)\")\n",
    "    feat_mq = compute_momentum_quality(close)\n",
    "\n",
    "    # Group 10: Volume Features\n",
    "    print(\"  [10/10] Volume Features (2 features)\")\n",
    "    feat_vf = compute_volume_features(df)\n",
    "\n",
    "    # Assemble all features into the dataframe\n",
    "    out = df.copy()\n",
    "\n",
    "    for col in feat_ret.columns:\n",
    "        out[col] = feat_ret[col]\n",
    "    for col in feat_macd.columns:\n",
    "        out[col] = feat_macd[col]\n",
    "    for col in feat_vol.columns:\n",
    "        out[col] = feat_vol[col]\n",
    "    for col in feat_cpd.columns:\n",
    "        out[col] = feat_cpd[col]\n",
    "    for col in feat_frac.columns:\n",
    "        out[col] = feat_frac[col]\n",
    "    for col in feat_ram.columns:\n",
    "        out[col] = feat_ram[col]\n",
    "    for col in feat_micro.columns:\n",
    "        out[col] = feat_micro[col]\n",
    "    out['entropy'] = feat_entropy\n",
    "    for col in feat_mq.columns:\n",
    "        out[col] = feat_mq[col]\n",
    "    for col in feat_vf.columns:\n",
    "        out[col] = feat_vf[col]\n",
    "\n",
    "    # Target: next-day return (shift -1 is the ONLY forward-looking value, used as label)\n",
    "    out['target_ret'] = close.pct_change(1).shift(-1)\n",
    "\n",
    "    # Vol-scaled training target: raw_fwd_return / realized_vol\n",
    "    # Vol computed on CURRENT (unshifted) returns — no leakage\n",
    "    log_ret = np.log(close / close.shift(1))\n",
    "    vol_20 = log_ret.rolling(20, min_periods=10).std()\n",
    "    vol_20 = vol_20.replace(0, np.nan)\n",
    "    out['target_train'] = out['target_ret'] / vol_20\n",
    "\n",
    "    # Verify all expected feature columns exist\n",
    "    missing = [c for c in FEATURE_COLUMNS if c not in out.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing feature columns: {missing}\")\n",
    "\n",
    "    # Drop rows where ANY feature or target is NaN (warmup period)\n",
    "    n_before = len(out)\n",
    "    out_pre_drop = out  # keep reference for diagnostics\n",
    "    out = out.dropna(subset=FEATURE_COLUMNS + ['target_ret', 'target_train'])\n",
    "    n_after = len(out)\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"\\nFeature engineering complete in {elapsed:.1f}s\")\n",
    "    print(f\"  Rows: {n_before} -> {n_after} (dropped {n_before - n_after} warmup rows)\")\n",
    "    print(f\"  Features: {len(FEATURE_COLUMNS)}\")\n",
    "\n",
    "    if n_after == 0:\n",
    "        # Diagnose which features are all-NaN\n",
    "        nan_cols = [c for c in FEATURE_COLUMNS if out_pre_drop[c].isna().all()]\n",
    "        raise ValueError(\n",
    "            f\"All rows dropped after NaN removal. \"\n",
    "            f\"Features that are entirely NaN: {nan_cols}. \"\n",
    "            f\"Data has {n_before} rows but longest warmup exceeds this.\"\n",
    "        )\n",
    "\n",
    "    print(f\"  Date range: {out.index[0].date()} to {out.index[-1].date()}\")\n",
    "    print(f\"  Columns: {list(out.columns)}\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f1121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: AFML EVENT-DRIVEN PIPELINE (Lopez de Prado, 2018)\n",
    "# ============================================================================\n",
    "# Implements: CUSUM filter, Triple Barrier Labels, Meta-Labeling, Bet Sizing\n",
    "# Reference: Advances in Financial Machine Learning, Chapters 2-3, 5\n",
    "# ============================================================================\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "def get_daily_vol(close, span=50):\n",
    "    \"\"\"EWM standard deviation of log returns.\n",
    "    \n",
    "    Args:\n",
    "        close: pd.Series of prices indexed by datetime.\n",
    "        span: int, span for exponential weighted moving average.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series of daily volatility estimates.\n",
    "    \"\"\"\n",
    "    log_ret = np.log(close).diff().dropna()\n",
    "    return log_ret.ewm(span=span, min_periods=max(1, span // 2)).std()\n",
    "\n",
    "\n",
    "def cusum_filter(close, threshold):\n",
    "    \"\"\"Symmetric CUSUM filter for event detection.\n",
    "    \n",
    "    Detects structural breaks by tracking positive and negative cumulative\n",
    "    sums of log returns. When either sum exceeds the threshold, an event\n",
    "    is recorded and the cumulative sum resets to zero.\n",
    "    \n",
    "    Args:\n",
    "        close: pd.Series of prices indexed by datetime.\n",
    "        threshold: float or pd.Series. If Series, must share close's index.\n",
    "                   Typical usage: daily_vol * multiplier (e.g., 2.0).\n",
    "    \n",
    "    Returns:\n",
    "        pd.DatetimeIndex of event timestamps where structural breaks detected.\n",
    "    \"\"\"\n",
    "    log_ret = np.log(close).diff().dropna()\n",
    "    events = []\n",
    "    s_pos = 0.0\n",
    "    s_neg = 0.0\n",
    "    \n",
    "    # Convert scalar threshold to Series for uniform handling\n",
    "    if isinstance(threshold, (int, float)):\n",
    "        thresh_series = pd.Series(threshold, index=log_ret.index)\n",
    "    else:\n",
    "        thresh_series = threshold.reindex(log_ret.index, method='ffill')\n",
    "    \n",
    "    for t, r in log_ret.items():\n",
    "        h = thresh_series.loc[t]\n",
    "        if np.isnan(h) or np.isnan(r):\n",
    "            continue\n",
    "        \n",
    "        # Update cumulative sums (reset floor at zero)\n",
    "        s_pos = max(0.0, s_pos + r)\n",
    "        s_neg = min(0.0, s_neg + r)\n",
    "        \n",
    "        # Check if either sum breaches threshold\n",
    "        if s_pos > h:\n",
    "            events.append(t)\n",
    "            s_pos = 0.0\n",
    "            s_neg = 0.0\n",
    "        elif s_neg < -h:\n",
    "            events.append(t)\n",
    "            s_pos = 0.0\n",
    "            s_neg = 0.0\n",
    "    \n",
    "    return pd.DatetimeIndex(events)\n",
    "\n",
    "\n",
    "def triple_barrier_labels(close, events, pt_sl=(2.0, 1.0), num_days=10, min_ret=0.0):\n",
    "    \"\"\"Triple barrier labeling with volatility-scaled barriers.\n",
    "    \n",
    "    For each event, sets three barriers:\n",
    "      - Upper (profit-take): pt_sl[0] * daily_vol above entry\n",
    "      - Lower (stop-loss):  -pt_sl[1] * daily_vol below entry\n",
    "      - Vertical:            num_days forward (max holding period)\n",
    "    \n",
    "    The label is determined by which barrier is touched first.\n",
    "    \n",
    "    Args:\n",
    "        close: pd.Series of prices indexed by datetime.\n",
    "        events: pd.DatetimeIndex of event timestamps (from cusum_filter).\n",
    "        pt_sl: tuple (profit_take_mult, stop_loss_mult) of daily vol.\n",
    "               Set either to 0 to disable that barrier.\n",
    "        num_days: int, maximum holding period in trading days.\n",
    "        min_ret: float, minimum absolute log return for a non-zero label\n",
    "                 when the vertical barrier is hit first.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame indexed by event timestamps with columns:\n",
    "            'ret':   realized log return at exit\n",
    "            'bin':   label (+1 profit-take, -1 stop-loss, 0 vertical/below min_ret)\n",
    "            't_end': timestamp when position was closed\n",
    "    \"\"\"\n",
    "    daily_vol = get_daily_vol(close, span=50)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for t0 in events:\n",
    "        if t0 not in close.index or t0 not in daily_vol.index:\n",
    "            continue\n",
    "        \n",
    "        vol = daily_vol.loc[t0]\n",
    "        if np.isnan(vol) or vol <= 0:\n",
    "            continue\n",
    "        \n",
    "        p0 = close.loc[t0]\n",
    "        \n",
    "        # Define barrier levels\n",
    "        upper_barrier = pt_sl[0] * vol if pt_sl[0] > 0 else np.inf\n",
    "        lower_barrier = -pt_sl[1] * vol if pt_sl[1] > 0 else -np.inf\n",
    "        \n",
    "        # Get the forward price path from t0 up to t0 + num_days bars\n",
    "        t0_idx = close.index.get_loc(t0)\n",
    "        t_end_idx = min(t0_idx + num_days, len(close) - 1)\n",
    "        path = close.iloc[t0_idx: t_end_idx + 1]\n",
    "        \n",
    "        if len(path) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Log returns relative to entry\n",
    "        log_returns = np.log(path / p0)\n",
    "        \n",
    "        # Find first crossing of each barrier\n",
    "        hit_upper = log_returns[log_returns >= upper_barrier].index\n",
    "        hit_lower = log_returns[log_returns <= lower_barrier].index\n",
    "        \n",
    "        # Determine first touch times (use NaT for unhit barriers)\n",
    "        t_upper = hit_upper[0] if len(hit_upper) > 0 else pd.NaT\n",
    "        t_lower = hit_lower[0] if len(hit_lower) > 0 else pd.NaT\n",
    "        t_vert = path.index[-1]  # vertical barrier always exists\n",
    "        \n",
    "        # Find earliest barrier touch\n",
    "        candidates = {}\n",
    "        if not pd.isna(t_upper):\n",
    "            candidates[t_upper] = 'upper'\n",
    "        if not pd.isna(t_lower):\n",
    "            candidates[t_lower] = 'lower'\n",
    "        candidates[t_vert] = 'vertical'\n",
    "        \n",
    "        t_end = min(candidates.keys())\n",
    "        barrier_type = candidates[t_end]\n",
    "        \n",
    "        # Realized log return at exit\n",
    "        ret = np.log(close.loc[t_end] / p0)\n",
    "        \n",
    "        # Assign label\n",
    "        if barrier_type == 'upper':\n",
    "            label = 1\n",
    "        elif barrier_type == 'lower':\n",
    "            label = -1\n",
    "        else:\n",
    "            # Vertical barrier: label based on return direction if above min_ret\n",
    "            if abs(ret) > min_ret:\n",
    "                label = int(np.sign(ret))\n",
    "            else:\n",
    "                label = 0\n",
    "        \n",
    "        results.append({'t0': t0, 'ret': ret, 'bin': label, 't_end': t_end})\n",
    "    \n",
    "    if len(results) == 0:\n",
    "        return pd.DataFrame(columns=['ret', 'bin', 't_end'])\n",
    "    \n",
    "    df = pd.DataFrame(results).set_index('t0')\n",
    "    df.index.name = None\n",
    "    return df\n",
    "\n",
    "\n",
    "def meta_labeling(primary_preds, true_labels):\n",
    "    \"\"\"Generate meta-labels: 1 if primary model correctly predicted direction.\n",
    "    \n",
    "    Meta-labeling separates side (direction) from size (confidence).\n",
    "    The primary model decides direction; the meta-model decides whether\n",
    "    to take the trade and how large to size it.\n",
    "    \n",
    "    Args:\n",
    "        primary_preds: pd.Series of predicted directions (+1/-1).\n",
    "        true_labels: pd.Series of actual directions (+1/-1/0).\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series of 0/1 meta-labels. 1 = primary model was correct.\n",
    "    \"\"\"\n",
    "    # Align indices\n",
    "    common_idx = primary_preds.index.intersection(true_labels.index)\n",
    "    preds = primary_preds.loc[common_idx]\n",
    "    actuals = true_labels.loc[common_idx]\n",
    "    \n",
    "    # Meta-label: 1 if signs agree (correct prediction), 0 otherwise\n",
    "    # A zero true label means the trade was unprofitable, so meta-label = 0\n",
    "    meta = (np.sign(preds) == np.sign(actuals)).astype(int)\n",
    "    \n",
    "    # If true label is 0, the trade had no edge -> meta = 0\n",
    "    meta[actuals == 0] = 0\n",
    "    \n",
    "    return meta\n",
    "\n",
    "\n",
    "def bet_size(meta_probs, max_leverage=1.0):\n",
    "    \"\"\"Probit-based position sizing from meta-model probabilities.\n",
    "    \n",
    "    Converts P(correct) from the meta-model into a continuous position\n",
    "    size using the inverse normal CDF (probit function). This maps\n",
    "    probabilities smoothly to position sizes with natural concavity\n",
    "    near 0 and 1.\n",
    "    \n",
    "    Args:\n",
    "        meta_probs: pd.Series of P(correct) from meta-model, in [0, 1].\n",
    "        max_leverage: float, maximum absolute position size.\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series of position sizes in [-max_leverage, max_leverage].\n",
    "        Values near 0.5 produce small sizes; values near 0 or 1 produce\n",
    "        sizes approaching max_leverage.\n",
    "    \"\"\"\n",
    "    # Clip to avoid infinities from norm.ppf at 0 and 1\n",
    "    clipped = meta_probs.clip(1e-5, 1.0 - 1e-5)\n",
    "    \n",
    "    # Probit transform: map probability to z-score\n",
    "    z = pd.Series(norm.ppf(clipped.values), index=clipped.index)\n",
    "    \n",
    "    # Convert z-score to position size via CDF\n",
    "    # size = (2 * Phi(z) - 1) maps z to [-1, 1]\n",
    "    # z > 0 (prob > 0.5) -> positive size, z < 0 (prob < 0.5) -> negative size\n",
    "    size = (2.0 * norm.cdf(z) - 1.0) * max_leverage\n",
    "    \n",
    "    return size\n",
    "\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"AFML EVENT-DRIVEN PIPELINE\")\n",
    "print(\"  Functions defined: get_daily_vol, cusum_filter, triple_barrier_labels,\")\n",
    "print(\"                     meta_labeling, bet_size\")\n",
    "print(\"  These will be applied during walk-forward training in Cell 6.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beec9b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: MOMENTUM TRANSFORMER (Temporal Fusion Transformer Architecture)\n",
    "# ============================================================================\n",
    "# Implements the full TFT architecture from Lim et al. (2021) adapted for\n",
    "# momentum signal generation with Sharpe ratio loss.\n",
    "# All layers are serializable with proper get_config() methods.\n",
    "# ============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class GluLayer(layers.Layer):\n",
    "    \"\"\"Gated Linear Unit: splits transformation into value and gate streams.\n",
    "    \n",
    "    GLU(x) = Dense_value(x) * sigmoid(Dense_gate(x))\n",
    "    \n",
    "    The gate learns which components of the transformation to pass through,\n",
    "    providing a learnable skip-like mechanism at the feature level.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, dropout_rate=None, activation=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.activation_name = activation\n",
    "        \n",
    "        # Value stream: optional activation (e.g., ELU for GRN intermediate)\n",
    "        self.dense_value = layers.Dense(hidden_size, activation=activation)\n",
    "        # Gate stream: always sigmoid for gating\n",
    "        self.dense_gate = layers.Dense(hidden_size, activation='sigmoid')\n",
    "        \n",
    "        self.dropout_layer = None\n",
    "        if dropout_rate is not None and dropout_rate > 0:\n",
    "            self.dropout_layer = layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        value = self.dense_value(inputs)\n",
    "        gate = self.dense_gate(inputs)\n",
    "        \n",
    "        if self.dropout_layer is not None:\n",
    "            value = self.dropout_layer(value)\n",
    "        \n",
    "        glu_output = value * gate\n",
    "        return glu_output, gate\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'activation': self.activation_name,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class GatedResidualNetwork(layers.Layer):\n",
    "    \"\"\"Gated Residual Network: the core building block of TFT.\n",
    "    \n",
    "    Architecture:\n",
    "        eta_1 = Dense(hidden_size)(input)             [+ Dense(hidden_size)(context) if context]\n",
    "        eta_2 = ELU(eta_1)\n",
    "        eta_1_prime = Dense(hidden_size)(eta_2)\n",
    "        glu_output = GLU(eta_1_prime)\n",
    "        output = LayerNorm(input_skip + glu_output)\n",
    "    \n",
    "    When output_size != input_size, a skip projection is applied to the input.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size=None, dropout_rate=None,\n",
    "                 context_size=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size or hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        # Primary pathway\n",
    "        self.dense1 = layers.Dense(hidden_size)\n",
    "        self.dense2 = layers.Dense(self.output_size)\n",
    "        self.glu = GluLayer(self.output_size, dropout_rate=dropout_rate)\n",
    "        self.layer_norm = layers.LayerNormalization()\n",
    "        \n",
    "        # Optional context injection\n",
    "        self.context_dense = None\n",
    "        if context_size is not None:\n",
    "            self.context_dense = layers.Dense(hidden_size, use_bias=False)\n",
    "        \n",
    "        # Skip projection (created dynamically if needed)\n",
    "        self._skip_layer = None\n",
    "        self._skip_built = False\n",
    "    \n",
    "    def call(self, inputs, context=None, return_gate=False):\n",
    "        # Build skip projection on first call if input dim != output_size\n",
    "        if not self._skip_built:\n",
    "            input_dim = inputs.shape[-1]\n",
    "            if input_dim is not None and input_dim != self.output_size:\n",
    "                self._skip_layer = layers.Dense(self.output_size)\n",
    "            self._skip_built = True\n",
    "        \n",
    "        # Skip connection\n",
    "        if self._skip_layer is not None:\n",
    "            skip = self._skip_layer(inputs)\n",
    "        else:\n",
    "            skip = inputs\n",
    "        \n",
    "        # Primary pathway\n",
    "        eta_1 = self.dense1(inputs)\n",
    "        \n",
    "        # Context injection (additive)\n",
    "        if context is not None and self.context_dense is not None:\n",
    "            eta_1 = eta_1 + self.context_dense(context)\n",
    "        \n",
    "        # ELU activation (using tf.nn.elu as specified)\n",
    "        eta_2 = tf.nn.elu(eta_1)\n",
    "        \n",
    "        # Second dense\n",
    "        eta_1_prime = self.dense2(eta_2)\n",
    "        \n",
    "        # GLU gating\n",
    "        glu_output, gate = self.glu(eta_1_prime)\n",
    "        \n",
    "        # Residual connection + layer norm\n",
    "        output = self.layer_norm(skip + glu_output)\n",
    "        \n",
    "        if return_gate:\n",
    "            return output, gate\n",
    "        return output\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'output_size': self.output_size,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'context_size': self.context_size,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class VariableSelectionNetwork(layers.Layer):\n",
    "    \"\"\"Variable Selection Network: soft feature importance via learned weights.\n",
    "    \n",
    "    Each input feature is processed by its own GRN, then a selection GRN\n",
    "    produces softmax weights over features. The output is the weighted\n",
    "    combination of per-feature GRN outputs.\n",
    "    \n",
    "    This is the TFT's primary interpretability mechanism -- the softmax\n",
    "    weights directly indicate feature importance at each timestep.\n",
    "    \n",
    "    Input shape:  (batch, time, num_inputs, hidden_size) -- after embedding\n",
    "    Output shape: (batch, time, hidden_size)\n",
    "    Weights shape: (batch, time, num_inputs, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs, hidden_size, dropout_rate=None,\n",
    "                 context_size=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.context_size = context_size\n",
    "        \n",
    "        # Per-feature GRNs: each processes one feature independently\n",
    "        self.feature_grns = [\n",
    "            GatedResidualNetwork(\n",
    "                hidden_size=hidden_size,\n",
    "                output_size=hidden_size,\n",
    "                dropout_rate=dropout_rate,\n",
    "                name=f\"feature_grn_{i}\"\n",
    "            )\n",
    "            for i in range(num_inputs)\n",
    "        ]\n",
    "        \n",
    "        # Selection GRN: produces weights over features\n",
    "        # Input is flattened features: (batch, time, num_inputs * hidden_size)\n",
    "        # Output: (batch, time, num_inputs) -> softmax -> weights\n",
    "        self.selection_grn = GatedResidualNetwork(\n",
    "            hidden_size=hidden_size,\n",
    "            output_size=num_inputs,\n",
    "            dropout_rate=dropout_rate,\n",
    "            context_size=context_size,\n",
    "            name=\"selection_grn\"\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs, context=None):\n",
    "        # inputs: (batch, time, num_inputs, hidden_size)\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        time_steps = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Flatten for selection GRN: (batch, time, num_inputs * hidden_size)\n",
    "        flattened = tf.reshape(inputs, [batch_size, time_steps,\n",
    "                                        self.num_inputs * self.hidden_size])\n",
    "        \n",
    "        # Selection weights via GRN + softmax\n",
    "        selection_output = self.selection_grn(flattened, context=context)\n",
    "        # selection_output: (batch, time, num_inputs)\n",
    "        weights = tf.nn.softmax(selection_output, axis=-1)\n",
    "        # weights: (batch, time, num_inputs)\n",
    "        weights_expanded = tf.expand_dims(weights, axis=-1)\n",
    "        # weights_expanded: (batch, time, num_inputs, 1)\n",
    "        \n",
    "        # Process each feature through its own GRN\n",
    "        processed_features = []\n",
    "        for i in range(self.num_inputs):\n",
    "            # Extract feature i: (batch, time, hidden_size)\n",
    "            feat_i = inputs[:, :, i, :]\n",
    "            # Apply per-feature GRN\n",
    "            grn_out = self.feature_grns[i](feat_i)\n",
    "            processed_features.append(grn_out)\n",
    "        \n",
    "        # Stack: (batch, time, num_inputs, hidden_size)\n",
    "        stacked = tf.stack(processed_features, axis=2)\n",
    "        \n",
    "        # Weighted combination: (batch, time, hidden_size)\n",
    "        selected = tf.reduce_sum(stacked * weights_expanded, axis=2)\n",
    "        \n",
    "        return selected, weights_expanded\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_inputs': self.num_inputs,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "            'context_size': self.context_size,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class ScaledDotProductAttention(layers.Layer):\n",
    "    \"\"\"Standard scaled dot-product attention.\n",
    "    \n",
    "    Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_k)) @ V\n",
    "    \n",
    "    Supports optional causal masking (lower-triangular) to prevent\n",
    "    attending to future positions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dropout_rate=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout_layer = layers.Dropout(dropout_rate) if dropout_rate > 0 else None\n",
    "    \n",
    "    def call(self, q, k, v, mask=None):\n",
    "        # q, k, v: (batch, ..., seq_len, d_k)\n",
    "        d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "        \n",
    "        # Scaled dot product: (batch, ..., seq_q, seq_k)\n",
    "        scores = tf.matmul(q, k, transpose_b=True) / tf.sqrt(d_k)\n",
    "        \n",
    "        # Apply mask (e.g., causal mask): masked positions get -1e9\n",
    "        if mask is not None:\n",
    "            scores = scores + (1.0 - mask) * (-1e9)\n",
    "        \n",
    "        # Softmax over keys\n",
    "        attention_weights = tf.nn.softmax(scores, axis=-1)\n",
    "        \n",
    "        if self.dropout_layer is not None:\n",
    "            attention_weights = self.dropout_layer(attention_weights)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        output = tf.matmul(attention_weights, v)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class InterpretableMultiHeadAttention(layers.Layer):\n",
    "    \"\"\"Interpretable Multi-Head Attention from TFT paper.\n",
    "    \n",
    "    Key differences from standard Transformer MHA:\n",
    "    1. All heads SHARE the same value projection (W_v)\n",
    "    2. Head outputs are AVERAGED, not concatenated\n",
    "    \n",
    "    This design enables direct interpretation of attention patterns\n",
    "    because each head attends to the same value representation.\n",
    "    The averaged attention weights can be examined per-head to understand\n",
    "    what temporal patterns the model has learned.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_heads, d_model, dropout_rate=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.d_head = d_model // num_heads\n",
    "        \n",
    "        # Per-head Q and K projections\n",
    "        self.W_q = [layers.Dense(self.d_head, use_bias=False, name=f\"W_q_{i}\")\n",
    "                     for i in range(num_heads)]\n",
    "        self.W_k = [layers.Dense(self.d_head, use_bias=False, name=f\"W_k_{i}\")\n",
    "                     for i in range(num_heads)]\n",
    "        \n",
    "        # SHARED value projection across all heads\n",
    "        self.W_v = layers.Dense(self.d_head, use_bias=False, name=\"W_v_shared\")\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = layers.Dense(d_model, name=\"W_o\")\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        self.attention = ScaledDotProductAttention(dropout_rate=dropout_rate)\n",
    "    \n",
    "    def call(self, q, k, v, mask=None):\n",
    "        # Shared value projection: (batch, seq, d_head)\n",
    "        v_shared = self.W_v(v)\n",
    "        \n",
    "        head_outputs = []\n",
    "        head_attentions = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            # Per-head query and key projections\n",
    "            q_i = self.W_q[i](q)  # (batch, seq_q, d_head)\n",
    "            k_i = self.W_k[i](k)  # (batch, seq_k, d_head)\n",
    "            \n",
    "            # Attention with shared values\n",
    "            attn_output, attn_weights = self.attention(q_i, k_i, v_shared, mask=mask)\n",
    "            head_outputs.append(attn_output)\n",
    "            head_attentions.append(attn_weights)\n",
    "        \n",
    "        # AVERAGE head outputs (not concatenate) -- key TFT design choice\n",
    "        # Stack: (num_heads, batch, seq, d_head)\n",
    "        stacked_outputs = tf.stack(head_outputs, axis=0)\n",
    "        averaged = tf.reduce_mean(stacked_outputs, axis=0)  # (batch, seq, d_head)\n",
    "        \n",
    "        # Output projection back to d_model\n",
    "        output = self.W_o(averaged)  # (batch, seq, d_model)\n",
    "        \n",
    "        # Stack attention weights for interpretability: (batch, num_heads, seq_q, seq_k)\n",
    "        stacked_attentions = tf.stack(head_attentions, axis=1)\n",
    "        \n",
    "        return output, stacked_attentions\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'num_heads': self.num_heads,\n",
    "            'd_model': self.d_model,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class MomentumTransformer(Model):\n",
    "    \"\"\"Complete Temporal Fusion Transformer for momentum signal generation.\n",
    "    \n",
    "    Architecture flow:\n",
    "    1. Feature Embedding: per-feature Dense projections to hidden_size\n",
    "    2. Variable Selection: learned soft feature importance via VSN\n",
    "    3. LSTM Encoder: capture temporal dependencies\n",
    "    4. Post-LSTM: GRN + GLU gate + skip + LayerNorm\n",
    "    5. Interpretable Multi-Head Self-Attention with causal mask\n",
    "    6. Post-Attention: GRN + GLU gate + skip + LayerNorm\n",
    "    7. Output: Dense(tanh) -> signal in [-1, 1]\n",
    "    \n",
    "    The model is fully interpretable: VSN weights show feature importance,\n",
    "    attention weights show temporal dependencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, time_steps, input_size, output_size, hidden_size,\n",
    "                 num_heads, dropout_rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.time_steps = time_steps\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # 1. Per-feature embedding layers\n",
    "        self.feature_embeddings = [\n",
    "            layers.Dense(hidden_size, name=f\"feat_embed_{i}\")\n",
    "            for i in range(input_size)\n",
    "        ]\n",
    "        \n",
    "        # 2. Variable Selection Network\n",
    "        self.vsn = VariableSelectionNetwork(\n",
    "            num_inputs=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=\"vsn\"\n",
    "        )\n",
    "        \n",
    "        # 3. LSTM Encoder\n",
    "        self.lstm = layers.LSTM(\n",
    "            hidden_size,\n",
    "            return_sequences=True,\n",
    "            dropout=dropout_rate if dropout_rate else 0.0,\n",
    "            name=\"lstm_encoder\"\n",
    "        )\n",
    "        \n",
    "        # 4. Post-LSTM processing\n",
    "        self.post_lstm_grn = GatedResidualNetwork(\n",
    "            hidden_size=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=\"post_lstm_grn\"\n",
    "        )\n",
    "        self.post_lstm_glu = GluLayer(\n",
    "            hidden_size=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=\"post_lstm_glu\"\n",
    "        )\n",
    "        self.post_lstm_norm = layers.LayerNormalization(name=\"post_lstm_norm\")\n",
    "        \n",
    "        # 5. Interpretable Multi-Head Self-Attention\n",
    "        self.mha = InterpretableMultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            d_model=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=\"interpretable_mha\"\n",
    "        )\n",
    "        \n",
    "        # 6. Post-attention processing\n",
    "        self.post_attn_grn = GatedResidualNetwork(\n",
    "            hidden_size=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=\"post_attn_grn\"\n",
    "        )\n",
    "        self.post_attn_glu = GluLayer(\n",
    "            hidden_size=hidden_size,\n",
    "            dropout_rate=dropout_rate,\n",
    "            name=\"post_attn_glu\"\n",
    "        )\n",
    "        self.post_attn_norm = layers.LayerNormalization(name=\"post_attn_norm\")\n",
    "        \n",
    "        # 7. Output layer\n",
    "        self.output_dense = layers.Dense(\n",
    "            output_size,\n",
    "            activation='tanh',\n",
    "            name=\"output_signal\"\n",
    "        )\n",
    "    \n",
    "    def call(self, x, return_weights=False):\n",
    "        # x shape: (batch, time_steps, input_size)\n",
    "        \n",
    "        # --- 1. Feature Embedding ---\n",
    "        # Project each feature to hidden_size independently\n",
    "        embedded = [self.feature_embeddings[i](x[:, :, i:i+1])\n",
    "                     for i in range(self.input_size)]\n",
    "        # Stack: (batch, time, num_features, hidden_size)\n",
    "        embedded = tf.stack(embedded, axis=2)\n",
    "        \n",
    "        # --- 2. Variable Selection ---\n",
    "        vsn_output, vsn_weights = self.vsn(embedded)\n",
    "        # vsn_output: (batch, time, hidden_size)\n",
    "        \n",
    "        # --- 3. LSTM Encoder ---\n",
    "        lstm_output = self.lstm(vsn_output)\n",
    "        # lstm_output: (batch, time, hidden_size)\n",
    "        \n",
    "        # --- 4. Post-LSTM: GRN + GLU gate + residual + LayerNorm ---\n",
    "        post_lstm = self.post_lstm_grn(lstm_output)\n",
    "        post_lstm_gated, _ = self.post_lstm_glu(post_lstm)\n",
    "        post_lstm_out = self.post_lstm_norm(vsn_output + post_lstm_gated)\n",
    "        \n",
    "        # --- 5. Causal Self-Attention ---\n",
    "        seq_len = tf.shape(post_lstm_out)[1]\n",
    "        causal_mask = tf.linalg.band_part(\n",
    "            tf.ones([seq_len, seq_len]), -1, 0\n",
    "        )  # Lower triangular: position i can attend to positions <= i\n",
    "        causal_mask = tf.expand_dims(tf.expand_dims(causal_mask, 0), 0)\n",
    "        # (1, 1, seq, seq) -- broadcasts over batch and heads\n",
    "        \n",
    "        attn_output, attn_weights = self.mha(\n",
    "            post_lstm_out, post_lstm_out, post_lstm_out, mask=causal_mask\n",
    "        )\n",
    "        \n",
    "        # --- 6. Post-Attention: GRN + GLU gate + residual + LayerNorm ---\n",
    "        post_attn = self.post_attn_grn(attn_output)\n",
    "        post_attn_gated, _ = self.post_attn_glu(post_attn)\n",
    "        post_attn_out = self.post_attn_norm(post_lstm_out + post_attn_gated)\n",
    "        \n",
    "        # --- 7. Output signal ---\n",
    "        signal = self.output_dense(post_attn_out)\n",
    "        # signal: (batch, time, output_size) with values in [-1, 1]\n",
    "        \n",
    "        if return_weights:\n",
    "            return signal, {\n",
    "                'vsn_weights': vsn_weights,   # (batch, time, num_inputs, 1)\n",
    "                'attn_weights': attn_weights,  # (batch, num_heads, seq, seq)\n",
    "            }\n",
    "        return signal\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'time_steps': self.time_steps,\n",
    "            'input_size': self.input_size,\n",
    "            'output_size': self.output_size,\n",
    "            'hidden_size': self.hidden_size,\n",
    "            'num_heads': self.num_heads,\n",
    "            'dropout_rate': self.dropout_rate,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"MomentumTFT\")\n",
    "class SharpeLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Negative annualized Sharpe ratio as differentiable loss function.\n",
    "    \n",
    "    loss = -(mean(signal * return) / std(signal * return)) * sqrt(252)\n",
    "    \n",
    "    Uses ddof=1 (Bessel's correction) for unbiased sample standard deviation.\n",
    "    The model learns to produce signals that maximize risk-adjusted returns\n",
    "    directly, rather than optimizing a proxy like MSE.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_size=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        # y_true: actual returns, y_pred: predicted signals\n",
    "        # Both: (batch, time, 1) or (batch, time)\n",
    "        strategy_returns = y_pred * y_true\n",
    "        \n",
    "        # Flatten to compute portfolio-level Sharpe\n",
    "        strategy_returns = tf.reshape(strategy_returns, [-1])\n",
    "        \n",
    "        mean_ret = tf.reduce_mean(strategy_returns)\n",
    "        n = tf.cast(tf.size(strategy_returns), tf.float32)\n",
    "        \n",
    "        # Unbiased variance with ddof=1: sum((x - mean)^2) / (n - 1)\n",
    "        var = tf.reduce_sum(tf.square(strategy_returns - mean_ret)) / (n - 1.0)\n",
    "        std = tf.sqrt(var + 1e-9)  # Small epsilon for numerical stability\n",
    "        \n",
    "        # Negative annualized Sharpe (negative because we minimize loss)\n",
    "        sharpe = (mean_ret / std) * tf.sqrt(252.0)\n",
    "        return -sharpe\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'output_size': self.output_size,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "\n",
    "# --- Build and verify the model ---\n",
    "print(\"=\" * 70)\n",
    "print(\"MOMENTUM TRANSFORMER (TFT Architecture)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Configuration\n",
    "TIME_STEPS = 20\n",
    "INPUT_SIZE = 8    # Number of features\n",
    "OUTPUT_SIZE = 1   # Single signal output\n",
    "HIDDEN_SIZE = 32  # Hidden dimension\n",
    "NUM_HEADS = 4     # Attention heads\n",
    "DROPOUT_RATE = 0.1\n",
    "\n",
    "# Build model\n",
    "model = MomentumTransformer(\n",
    "    time_steps=TIME_STEPS,\n",
    "    input_size=INPUT_SIZE,\n",
    "    output_size=OUTPUT_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "# Test forward pass with dummy data\n",
    "dummy_x = tf.random.normal([16, TIME_STEPS, INPUT_SIZE])\n",
    "dummy_y = tf.random.normal([16, TIME_STEPS, OUTPUT_SIZE])\n",
    "\n",
    "# Forward pass with weights\n",
    "signal, weights = model(dummy_x, return_weights=True)\n",
    "print(f\"\\nModel architecture verified:\")\n",
    "print(f\"  Input shape:       {dummy_x.shape}\")\n",
    "print(f\"  Signal shape:      {signal.shape}\")\n",
    "print(f\"  VSN weights shape: {weights['vsn_weights'].shape}\")\n",
    "print(f\"  Attn weights shape:{weights['attn_weights'].shape}\")\n",
    "print(f\"  Signal range:      [{tf.reduce_min(signal):.4f}, {tf.reduce_max(signal):.4f}]\")\n",
    "\n",
    "# Verify causal masking: attention at position i should have zero weight for j > i\n",
    "attn = weights['attn_weights'][0, 0].numpy()  # First sample, first head\n",
    "upper_triangle_sum = np.triu(attn, k=1).sum()\n",
    "print(f\"\\n  Causal mask check (upper triangle sum): {upper_triangle_sum:.10f}\")\n",
    "assert upper_triangle_sum < 1e-6, \"Causal masking is broken!\"\n",
    "print(f\"  Causal masking: VERIFIED\")\n",
    "\n",
    "# Test Sharpe loss\n",
    "loss_fn = SharpeLoss(output_size=OUTPUT_SIZE)\n",
    "loss_val = loss_fn(dummy_y, signal)\n",
    "print(f\"\\n  Sharpe loss value: {loss_val:.4f}\")\n",
    "print(f\"  (Negative = model has positive Sharpe)\")\n",
    "\n",
    "# Compile and do one training step to verify gradients flow\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=SharpeLoss(output_size=OUTPUT_SIZE)\n",
    ")\n",
    "\n",
    "# Single training step\n",
    "history = model.fit(dummy_x, dummy_y, epochs=1, batch_size=16, verbose=0)\n",
    "print(f\"  Training step loss: {history.history['loss'][0]:.4f}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = model.count_params()\n",
    "print(f\"\\n  Total parameters: {total_params:,}\")\n",
    "\n",
    "# VSN feature importance (interpretability demo)\n",
    "vsn_w = weights['vsn_weights'].numpy()\n",
    "mean_importance = vsn_w.mean(axis=(0, 1)).flatten()\n",
    "feature_names = [f\"feat_{i}\" for i in range(INPUT_SIZE)]\n",
    "importance_order = np.argsort(-mean_importance)\n",
    "print(f\"\\n  Feature importance (VSN weights, descending):\")\n",
    "for rank, idx in enumerate(importance_order[:5]):\n",
    "    print(f\"    {rank+1}. {feature_names[idx]}: {mean_importance[idx]:.4f}\")\n",
    "\n",
    "# Serialization verification\n",
    "config = model.get_config()\n",
    "print(f\"\\n  Serialization config keys: {sorted(config.keys())}\")\n",
    "print(f\"  Config: time_steps={config.get('time_steps')}, \"\n",
    "      f\"input_size={config.get('input_size')}, \"\n",
    "      f\"hidden_size={config.get('hidden_size')}, \"\n",
    "      f\"num_heads={config.get('num_heads')}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Momentum Transformer built, tested, and verified.\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. TRAINING & WALK-FORWARD VALIDATION ENGINE\n",
    "\n",
    "def expanding_normalize(train_df, test_df, feature_cols):\n",
    "    \"\"\"Normalize test data using ONLY training statistics (no look-ahead).\n",
    "\n",
    "    Args:\n",
    "        train_df: DataFrame with training data\n",
    "        test_df: DataFrame with test data\n",
    "        feature_cols: list of feature column names to normalize\n",
    "\n",
    "    Returns: (train_normalized, test_normalized) DataFrames\n",
    "    \"\"\"\n",
    "    means = train_df[feature_cols].mean()\n",
    "    stds = train_df[feature_cols].std()\n",
    "    stds = stds.replace(0, 1.0)  # prevent division by zero\n",
    "    train_norm = train_df.copy()\n",
    "    test_norm = test_df.copy()\n",
    "    train_norm[feature_cols] = (train_df[feature_cols] - means) / stds\n",
    "    test_norm[feature_cols] = (test_df[feature_cols] - means) / stds\n",
    "    return train_norm, test_norm\n",
    "\n",
    "\n",
    "def make_sequences(feature_array, target_array, window_size):\n",
    "    \"\"\"Create (window, n_features) sequences with proper target alignment.\n",
    "\n",
    "    Args:\n",
    "        feature_array: np.ndarray shape (n, n_feat)\n",
    "        target_array: np.ndarray shape (n,) -- target_ret (forward return)\n",
    "        window_size: int W\n",
    "\n",
    "    Returns:\n",
    "        X: (n_valid, W, n_feat) float32\n",
    "        y: (n_valid,) float32\n",
    "        indices: (n_valid,) int -- row indices in the original array for each\n",
    "                 sequence's \"current time\" (i.e., the last row of each window).\n",
    "                 Use these to recover dates: df.index[indices]\n",
    "\n",
    "    Alignment:\n",
    "        X[i] = features[i:i+W] -- \"current time\" is i+W-1\n",
    "        y[i] = target[i+W-1]   -- forward return at that time\n",
    "        indices[i] = i+W-1\n",
    "    \"\"\"\n",
    "    n = len(feature_array)\n",
    "    n_seq = n - window_size\n",
    "    if n_seq <= 0:\n",
    "        return (np.zeros((0, window_size, feature_array.shape[1]), dtype=np.float32),\n",
    "                np.zeros(0, dtype=np.float32),\n",
    "                np.array([], dtype=np.int64))\n",
    "\n",
    "    X = np.array([feature_array[i:i + window_size] for i in range(n_seq)])\n",
    "    y = target_array[window_size - 1:window_size - 1 + n_seq]\n",
    "    indices = np.arange(window_size - 1, window_size - 1 + n_seq, dtype=np.int64)\n",
    "\n",
    "    # Remove NaN targets\n",
    "    valid = ~np.isnan(y)\n",
    "    return X[valid].astype(np.float32), y[valid].astype(np.float32), indices[valid]\n",
    "\n",
    "\n",
    "def _predict_safe(model, X, desc=None):\n",
    "    \"\"\"Run model inference one sample at a time.\n",
    "\n",
    "    Keras 3 (TF 2.20) bakes the batch dimension from the first model() call\n",
    "    into the traced graph. Since MomentumTransformer is built with batch=1\n",
    "    (dummy forward pass), subsequent calls with batch>1 collapse the batch\n",
    "    dimension. Processing one-at-a-time matches the build shape and is\n",
    "    guaranteed correct. Cost: ~5ms per sample, negligible vs training time.\n",
    "\n",
    "    Args:\n",
    "        model: trained MomentumTransformer\n",
    "        X: np.ndarray shape (n_samples, window, n_features)\n",
    "        desc: optional tqdm description\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray shape (n_samples,) -- signal at last timestep for each sequence\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    preds = np.empty(n, dtype=np.float32)\n",
    "    for i in range(n):\n",
    "        out = model(tf.constant(X[i:i+1]), training=False)\n",
    "        preds[i] = float(out[0, -1, 0])\n",
    "    return preds\n",
    "\n",
    "\n",
    "def walk_forward_train(data_dict, cfg):\n",
    "    \"\"\"Universe-mode walk-forward OOS validation with purge gaps.\n",
    "\n",
    "    Trains ONE model per fold on pooled sequences from ALL tickers,\n",
    "    then predicts OOS for each ticker separately. This lets the model\n",
    "    learn cross-asset patterns and benefit from a larger training set.\n",
    "\n",
    "    Fold schedule is date-based: uses the SHORTEST ticker to define\n",
    "    fold boundaries, ensuring all tickers have data for every fold.\n",
    "\n",
    "    Args:\n",
    "        data_dict: {ticker_name: pd.DataFrame with FEATURE_COLUMNS + 'target_ret'}\n",
    "        cfg: MonolithConfig\n",
    "\n",
    "    Returns: dict with:\n",
    "        'oos_returns': pd.DataFrame (index=dates, columns=tickers)\n",
    "        'oos_signals': pd.DataFrame (same shape)\n",
    "        'fold_metrics': list of dicts {ticker, fold, sharpe, n_days, train_days}\n",
    "        'models': {fold_number: last trained model}\n",
    "        'vsn_weights': {'universe': np.ndarray of feature importance}\n",
    "    \"\"\"\n",
    "    tickers = list(data_dict.keys())\n",
    "    n_feat = len(FEATURE_COLUMNS)\n",
    "\n",
    "    # Use the shortest ticker to define fold boundaries\n",
    "    min_len = min(len(df) for df in data_dict.values())\n",
    "\n",
    "    # Pre-compute number of folds for progress bar\n",
    "    n_folds_est = 0\n",
    "    _ts = cfg.min_train_days\n",
    "    while _ts + cfg.window_size < min_len:\n",
    "        _te = _ts - cfg.purge_gap\n",
    "        if _te >= cfg.window_size + 10:\n",
    "            n_folds_est += 1\n",
    "        _ts += cfg.test_days\n",
    "\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  Universe Walk-Forward: {len(tickers)} tickers, ~{n_folds_est} folds\")\n",
    "    print(f\"  Tickers: {tickers}\")\n",
    "    print(f\"  Shortest series: {min_len} days\")\n",
    "    print(f\"  Est. time: ~{n_folds_est * len(tickers) * 10 // 60} min \"\n",
    "          f\"({n_folds_est} folds x {len(tickers)} tickers)\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    all_oos_returns = {t: [] for t in tickers}\n",
    "    all_oos_signals = {t: [] for t in tickers}\n",
    "    all_fold_metrics = []\n",
    "    all_models = {}\n",
    "    latest_vsn_w = None\n",
    "\n",
    "    fold = 0\n",
    "    test_start = cfg.min_train_days\n",
    "    fold_pbar = tqdm(total=n_folds_est, desc=\"Walk-Forward\", unit=\"fold\",\n",
    "                     bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} '\n",
    "                                '[{elapsed}<{remaining}, {rate_fmt}]')\n",
    "\n",
    "    while test_start + cfg.window_size < min_len:\n",
    "        fold += 1\n",
    "        train_end = test_start - cfg.purge_gap\n",
    "        test_end = min(test_start + cfg.test_days, min_len)\n",
    "\n",
    "        if train_end < cfg.window_size + 10:\n",
    "            test_start += cfg.test_days\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n  --- Fold {fold} (train: 0-{train_end}, \"\n",
    "              f\"purge: {train_end}-{test_start}, \"\n",
    "              f\"test: {test_start}-{test_end}) ---\")\n",
    "\n",
    "        # ── Gather train/test data from ALL tickers ──\n",
    "        X_train_all = []\n",
    "        y_train_all = []\n",
    "        # {ticker: (X_test, y_test, valid_indices, test_df)}\n",
    "        ticker_test_data = {}\n",
    "\n",
    "        for ticker in tickers:\n",
    "            df = data_dict[ticker]\n",
    "\n",
    "            train_df = df.iloc[:train_end].copy()\n",
    "            test_df = df.iloc[test_start:test_end].copy()\n",
    "\n",
    "            # Purge: NaN-out last purge_gap+1 training targets (both raw and vol-scaled)\n",
    "            if cfg.purge_gap > 0 and len(train_df) > cfg.purge_gap + 1:\n",
    "                train_df.iloc[-(cfg.purge_gap + 1):,\n",
    "                              train_df.columns.get_loc('target_ret')] = np.nan\n",
    "                train_df.iloc[-(cfg.purge_gap + 1):,\n",
    "                              train_df.columns.get_loc('target_train')] = np.nan\n",
    "\n",
    "            # Normalize each ticker with its OWN training stats (no cross-ticker leakage)\n",
    "            train_norm, test_norm = expanding_normalize(train_df, test_df, FEATURE_COLUMNS)\n",
    "\n",
    "            # Training sequences — use vol-scaled target for better gradient signal\n",
    "            X_tr, y_tr, _ = make_sequences(\n",
    "                train_norm[FEATURE_COLUMNS].values,\n",
    "                train_norm['target_train'].values,\n",
    "                cfg.window_size\n",
    "            )\n",
    "            if len(X_tr) > 0:\n",
    "                X_train_all.append(X_tr)\n",
    "                y_train_all.append(y_tr)\n",
    "                print(f\"    {ticker}: train_rows={len(train_df)}, \"\n",
    "                      f\"train_seqs={len(X_tr)}, \"\n",
    "                      f\"target_NaN={int(train_df['target_train'].isna().sum())}\")\n",
    "\n",
    "            # Test sequences — use vol-scaled for alignment, raw returns for OOS eval\n",
    "            X_te, _, te_idx = make_sequences(\n",
    "                test_norm[FEATURE_COLUMNS].values,\n",
    "                test_norm['target_train'].values,\n",
    "                cfg.window_size\n",
    "            )\n",
    "            # Raw forward returns at the same valid indices (for OOS evaluation)\n",
    "            y_te_raw = test_df['target_ret'].values[te_idx] if len(te_idx) > 0 else np.array([])\n",
    "            print(f\"    {ticker}: test_rows={len(test_df)}, \"\n",
    "                  f\"test_seqs={len(X_te)}, \"\n",
    "                  f\"target_NaN={int(test_df['target_train'].isna().sum())}\")\n",
    "            if len(X_te) > 0:\n",
    "                ticker_test_data[ticker] = (X_te, y_te_raw, te_idx, test_df)\n",
    "\n",
    "        # Combine all tickers' training data into one pool\n",
    "        if not X_train_all:\n",
    "            print(f\"  Fold {fold}: no training data, skipping\")\n",
    "            test_start += cfg.test_days\n",
    "            continue\n",
    "\n",
    "        X_train = np.concatenate(X_train_all, axis=0)\n",
    "        y_train = np.concatenate(y_train_all, axis=0)\n",
    "\n",
    "        # Shuffle the pooled training data (sequences from different tickers\n",
    "        # are interleaved — this prevents the model from memorizing ticker order)\n",
    "        shuffle_idx = np.random.permutation(len(X_train))\n",
    "        X_train = X_train[shuffle_idx]\n",
    "        y_train = y_train[shuffle_idx]\n",
    "\n",
    "        total_test = sum(len(v[0]) for v in ticker_test_data.values())\n",
    "        if len(X_train) < 50 or total_test < 5:\n",
    "            print(f\"  Fold {fold}: insufficient data \"\n",
    "                  f\"(train={len(X_train)}, test={total_test}), skipping\")\n",
    "            test_start += cfg.test_days\n",
    "            continue\n",
    "\n",
    "        print(f\"    UNIVERSE POOL: {len(X_train)} train sequences \"\n",
    "              f\"from {len(tickers)} tickers\")\n",
    "\n",
    "        # ── Build fresh universe model ──\n",
    "        model = MomentumTransformer(\n",
    "            cfg.window_size, n_feat, 1,\n",
    "            cfg.hidden_size, cfg.num_heads, cfg.dropout_rate\n",
    "        )\n",
    "        _ = model(np.zeros((1, cfg.window_size, n_feat), dtype=np.float32))\n",
    "        model.compile(\n",
    "            optimizer=keras.optimizers.Adam(cfg.learning_rate, clipnorm=cfg.clipnorm),\n",
    "            loss=SharpeLoss()\n",
    "        )\n",
    "\n",
    "        # ── Train on pooled universe data ──\n",
    "        t0 = time.time()\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=cfg.epochs,\n",
    "            batch_size=cfg.batch_size,\n",
    "            validation_split=0.15,\n",
    "            callbacks=[\n",
    "                keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss', patience=cfg.early_stop_patience,\n",
    "                    restore_best_weights=True, verbose=0\n",
    "                ),\n",
    "                keras.callbacks.ReduceLROnPlateau(\n",
    "                    monitor='val_loss', factor=cfg.lr_reduce_factor,\n",
    "                    patience=cfg.lr_reduce_patience, min_lr=cfg.min_lr, verbose=0\n",
    "                )\n",
    "            ],\n",
    "            verbose=0\n",
    "        )\n",
    "        train_time = time.time() - t0\n",
    "\n",
    "        # ── Predict OOS per-ticker ──\n",
    "        for ticker, (X_test, y_test, valid_idx, test_df) in ticker_test_data.items():\n",
    "            n_test = len(X_test)\n",
    "\n",
    "            # Predict one-at-a-time (Keras 3 bakes batch dim from build step;\n",
    "            # model was built with batch=1, so batch>1 collapses output)\n",
    "            preds = _predict_safe(model, X_test)\n",
    "\n",
    "            assert len(preds) == n_test == len(y_test), (\n",
    "                f\"Shape mismatch: preds={len(preds)}, X_test={n_test}, \"\n",
    "                f\"y_test={len(y_test)}\")\n",
    "\n",
    "            # Build date index from make_sequences' valid_idx\n",
    "            test_pred_idx = test_df.index[valid_idx]\n",
    "\n",
    "            # Use y_test directly (already aligned by make_sequences)\n",
    "            all_oos_returns[ticker].append(\n",
    "                pd.Series(y_test, index=test_pred_idx, name=ticker))\n",
    "            all_oos_signals[ticker].append(\n",
    "                pd.Series(preds, index=test_pred_idx, name=ticker))\n",
    "\n",
    "            # Per-ticker fold metrics\n",
    "            fold_positions = np.sign(preds)\n",
    "            fold_strat_ret = fold_positions * y_test\n",
    "            fold_strat_ret = fold_strat_ret[~np.isnan(fold_strat_ret)]\n",
    "            if len(fold_strat_ret) > 1:\n",
    "                fold_sharpe = (np.mean(fold_strat_ret)\n",
    "                               / (np.std(fold_strat_ret, ddof=1) + 1e-9)\n",
    "                               * np.sqrt(252))\n",
    "            else:\n",
    "                fold_sharpe = 0.0\n",
    "\n",
    "            all_fold_metrics.append({\n",
    "                'ticker': ticker, 'fold': fold,\n",
    "                'sharpe': round(fold_sharpe, 2),\n",
    "                'n_days': len(preds),\n",
    "                'train_days': len(X_train),\n",
    "                'train_time': round(train_time / len(tickers), 1)\n",
    "            })\n",
    "\n",
    "            print(f\"    {ticker}: test={len(preds):3d} Sharpe={fold_sharpe:+.2f}\")\n",
    "\n",
    "        # Extract VSN weights from the universe model (one sample, matching batch=1 build)\n",
    "        first_ticker_X = list(ticker_test_data.values())[0][0]\n",
    "        _, weights_dict = model(tf.constant(first_ticker_X[:1]), return_weights=True)\n",
    "        latest_vsn_w = weights_dict['vsn_weights'].numpy().mean(axis=(0, 1)).flatten()\n",
    "\n",
    "        all_models[fold] = model\n",
    "\n",
    "        # Update progress bar with fold summary\n",
    "        fold_sharpes = [m['sharpe'] for m in all_fold_metrics\n",
    "                        if m['fold'] == fold]\n",
    "        avg_s = np.mean(fold_sharpes) if fold_sharpes else 0.0\n",
    "        fold_pbar.set_postfix_str(\n",
    "            f\"train={train_time:.0f}s, avg_sharpe={avg_s:+.2f}\")\n",
    "        fold_pbar.update(1)\n",
    "\n",
    "        test_start += cfg.test_days\n",
    "\n",
    "        # Memory cleanup\n",
    "        del X_train, y_train, X_train_all, y_train_all, ticker_test_data\n",
    "        gc.collect()\n",
    "\n",
    "    fold_pbar.close()\n",
    "\n",
    "    # ── Assemble per-ticker OOS results ──\n",
    "    oos_returns_dict = {}\n",
    "    oos_signals_dict = {}\n",
    "    for ticker in tickers:\n",
    "        if all_oos_returns[ticker]:\n",
    "            combined_ret = pd.concat(all_oos_returns[ticker])\n",
    "            combined_sig = pd.concat(all_oos_signals[ticker])\n",
    "            oos_returns_dict[ticker] = combined_ret[~combined_ret.index.duplicated(keep='first')]\n",
    "            oos_signals_dict[ticker] = combined_sig[~combined_sig.index.duplicated(keep='first')]\n",
    "\n",
    "    # VSN weights: store under 'universe' key and duplicate per-ticker for viz compatibility\n",
    "    vsn_weights = {}\n",
    "    if latest_vsn_w is not None:\n",
    "        for ticker in tickers:\n",
    "            vsn_weights[ticker] = latest_vsn_w\n",
    "\n",
    "    return {\n",
    "        'oos_returns': pd.DataFrame(oos_returns_dict),\n",
    "        'oos_signals': pd.DataFrame(oos_signals_dict),\n",
    "        'fold_metrics': all_fold_metrics,\n",
    "        'models': all_models,\n",
    "        'vsn_weights': vsn_weights,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9f0386",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# 7. ORCHESTRATION, METRICS & VISUALIZATION\n",
    "\n",
    "def calculate_metrics(returns, costs=0.0):\n",
    "    \"\"\"Full metrics suite for strategy returns.\n",
    "    \n",
    "    Args:\n",
    "        returns: np.ndarray or pd.Series of daily returns (simple, not log)\n",
    "        costs: total transaction costs already deducted (for reporting only)\n",
    "    \n",
    "    Returns: dict with:\n",
    "        total_return, annual_return, sharpe, sortino, calmar, max_dd,\n",
    "        win_rate, profit_factor, n_trades, avg_hold_days\n",
    "    \"\"\"\n",
    "    r = np.asarray(returns, dtype=np.float64)\n",
    "    r = r[~np.isnan(r)]\n",
    "    \n",
    "    n = len(r)\n",
    "    if n < 2:\n",
    "        return {\n",
    "            'total_return': 0.0, 'annual_return': 0.0, 'sharpe': 0.0,\n",
    "            'sortino': 0.0, 'calmar': 0.0, 'max_dd': 0.0,\n",
    "            'win_rate': 0.0, 'profit_factor': 0.0, 'n_days': 0,\n",
    "        }\n",
    "    \n",
    "    # Total and annualized return (simple compounding)\n",
    "    equity = np.cumprod(1.0 + r)\n",
    "    total_return = float(equity[-1] - 1.0)\n",
    "    n_years = n / 252.0\n",
    "    if n_years > 0 and equity[-1] > 0:\n",
    "        annual_return = float(equity[-1] ** (1.0 / n_years) - 1.0)\n",
    "    else:\n",
    "        annual_return = 0.0\n",
    "    \n",
    "    # Sharpe ratio: sqrt(252) * mean / std(ddof=1)\n",
    "    mean_r = np.mean(r)\n",
    "    std_r = np.std(r, ddof=1)\n",
    "    sharpe = float(np.sqrt(252) * mean_r / std_r) if std_r > 1e-9 else 0.0\n",
    "    \n",
    "    # Sortino ratio: sqrt(252) * mean / downside_std(ddof=1)\n",
    "    downside = r[r < 0]\n",
    "    if len(downside) > 1:\n",
    "        downside_std = np.std(downside, ddof=1)\n",
    "        sortino = float(np.sqrt(252) * mean_r / downside_std) if downside_std > 1e-9 else 0.0\n",
    "    else:\n",
    "        sortino = 0.0\n",
    "    \n",
    "    # Maximum drawdown from equity curve\n",
    "    peak = np.maximum.accumulate(equity)\n",
    "    dd_series = (equity - peak) / peak\n",
    "    max_dd = float(np.min(dd_series))\n",
    "    \n",
    "    # Calmar ratio: annual_return / |max_dd|\n",
    "    calmar = float(annual_return / abs(max_dd)) if abs(max_dd) > 1e-9 else 0.0\n",
    "    \n",
    "    # Win rate: fraction of positive return days\n",
    "    win_rate = float(np.mean(r > 0))\n",
    "    \n",
    "    # Profit factor: sum(gains) / |sum(losses)|\n",
    "    gains = r[r > 0]\n",
    "    losses = r[r < 0]\n",
    "    sum_gains = float(np.sum(gains)) if len(gains) > 0 else 0.0\n",
    "    sum_losses = float(np.abs(np.sum(losses))) if len(losses) > 0 else 0.0\n",
    "    profit_factor = float(sum_gains / sum_losses) if sum_losses > 1e-12 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'total_return': total_return,\n",
    "        'annual_return': annual_return,\n",
    "        'sharpe': sharpe,\n",
    "        'sortino': sortino,\n",
    "        'calmar': calmar,\n",
    "        'max_dd': max_dd,\n",
    "        'win_rate': win_rate,\n",
    "        'profit_factor': profit_factor,\n",
    "        'n_days': n,\n",
    "    }\n",
    "\n",
    "\n",
    "def run_enhanced_lab(cfg=None):\n",
    "    \"\"\"Main orchestrator: Data -> Features -> Walk-Forward -> Backtest -> Visualize.\n",
    "    \n",
    "    This is the primary entry point for the notebook.\n",
    "    \"\"\"\n",
    "    if cfg is None:\n",
    "        cfg = CFG\n",
    "    \n",
    "    tickers_to_run = [cfg.tickers[0]] if cfg.quick_mode else cfg.tickers\n",
    "    \n",
    "    # ── PHASE 1: DATA ACQUISITION ──\n",
    "    print(\"=\" * 70)\n",
    "    print(\"  PHASE 1: DATA ACQUISITION\")\n",
    "    print(\"=\" * 70)\n",
    "    auth = KiteAuth()\n",
    "    kite = auth.get_session()\n",
    "    if not kite:\n",
    "        raise RuntimeError(\"Kite authentication failed. Check .env credentials.\")\n",
    "\n",
    "    fetcher = KiteFetcher(kite)\n",
    "    raw_data = {}\n",
    "    for ticker in tqdm(tickers_to_run, desc=\"Fetching data\", unit=\"ticker\"):\n",
    "        exchange = cfg.exchanges.get(ticker, 'NSE')\n",
    "        tqdm.write(f\"  {ticker} ({exchange})...\")\n",
    "        try:\n",
    "            raw_data[ticker] = fetcher.fetch_daily(ticker, exchange, cfg.lookback_days)\n",
    "            tqdm.write(f\"    -> {len(raw_data[ticker])} days \"\n",
    "                       f\"({raw_data[ticker].index[0].date()} to \"\n",
    "                       f\"{raw_data[ticker].index[-1].date()})\")\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"    SKIP: {ticker} failed ({e})\")\n",
    "\n",
    "    if not raw_data:\n",
    "        raise RuntimeError(\"No tickers fetched successfully.\")\n",
    "\n",
    "    # ── PHASE 1.5: CROSS-ASSET DATA (News Sentiment + India VIX) ──\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"  PHASE 1.5: CROSS-ASSET DATA (News Sentiment + India VIX)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Build union of all trading dates for alignment\n",
    "    all_dates = pd.DatetimeIndex([])\n",
    "    for df in raw_data.values():\n",
    "        all_dates = all_dates.union(df.index)\n",
    "    all_dates = all_dates.sort_values()\n",
    "\n",
    "    cross_asset_df = fetch_cross_asset_features(\n",
    "        start_date=all_dates[0].strftime('%Y-%m-%d'),\n",
    "        end_date=all_dates[-1].strftime('%Y-%m-%d'),\n",
    "        date_index=all_dates,\n",
    "        kite=kite,\n",
    "    )\n",
    "\n",
    "    # ── PHASE 2: FEATURE ENGINEERING ──\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"  PHASE 2: FEATURE ENGINEERING ({len(FEATURE_COLUMNS)} features)\")\n",
    "    print(\"=\" * 70)\n",
    "    featured_data = {}\n",
    "    min_required = cfg.min_train_days + 2 * cfg.test_days\n",
    "    for ticker, df in tqdm(raw_data.items(), desc=\"Feature engineering\",\n",
    "                           total=len(raw_data), unit=\"ticker\"):\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            feat_df = build_all_features(df, cfg)\n",
    "            elapsed = time.time() - t0\n",
    "            if len(feat_df) < min_required:\n",
    "                tqdm.write(f\"  SKIP: {ticker} has {len(feat_df)} featured days \"\n",
    "                           f\"(need {min_required})\")\n",
    "                continue\n",
    "            # Merge cross-asset features (same for all tickers)\n",
    "            if not cross_asset_df.empty:\n",
    "                feat_df = feat_df.join(cross_asset_df, how='left')\n",
    "                for col in CROSS_ASSET_COLUMNS:\n",
    "                    if col in feat_df.columns:\n",
    "                        feat_df[col] = feat_df[col].ffill(limit=3).fillna(0.0)\n",
    "            featured_data[ticker] = feat_df\n",
    "            tqdm.write(f\"  {ticker}: {len(feat_df)} days, \"\n",
    "                       f\"{len(FEATURE_COLUMNS)} features ({elapsed:.1f}s)\")\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"  SKIP: {ticker} feature build failed ({e})\")\n",
    "\n",
    "    if not featured_data:\n",
    "        raise RuntimeError(\"No tickers survived feature engineering.\")\n",
    "\n",
    "    # Extend FEATURE_COLUMNS with cross-asset features (if available)\n",
    "    if CROSS_ASSET_COLUMNS:\n",
    "        for col in CROSS_ASSET_COLUMNS:\n",
    "            if col not in FEATURE_COLUMNS:\n",
    "                FEATURE_COLUMNS.append(col)\n",
    "        print(f\"\\n  Features extended: {len(FEATURE_COLUMNS)} total \"\n",
    "              f\"(+{len(CROSS_ASSET_COLUMNS)} cross-asset: \"\n",
    "              f\"{CROSS_ASSET_COLUMNS})\")\n",
    "\n",
    "    print(f\"\\n  Universe: {len(featured_data)} tickers survived \"\n",
    "          f\"(min {min_required} featured days required)\")\n",
    "    \n",
    "    # ── PHASE 3: WALK-FORWARD TRAINING ──\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"  PHASE 3: WALK-FORWARD OOS VALIDATION\")\n",
    "    print(f\"  Train: {cfg.min_train_days}d | Test: {cfg.test_days}d | \"\n",
    "          f\"Purge: {cfg.purge_gap}d | Window: {cfg.window_size}d\")\n",
    "    print(\"=\" * 70)\n",
    "    results = walk_forward_train(featured_data, cfg)\n",
    "    \n",
    "    # ── PHASE 4: STRATEGY CONSTRUCTION & METRICS ──\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"  PHASE 4: OOS STRATEGY METRICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    oos_returns = results['oos_returns']\n",
    "    oos_signals = results['oos_signals']\n",
    "    \n",
    "    if oos_returns.empty:\n",
    "        print(\"  WARNING: No OOS data produced. Check data length vs min_train_days.\")\n",
    "        return {'raw_data': raw_data, 'featured_data': featured_data,\n",
    "                'results': results, 'all_metrics': {}, 'cfg': cfg}\n",
    "    \n",
    "    all_metrics = {}\n",
    "    strat_net_returns = {}  # store for plotting\n",
    "    \n",
    "    for ticker in oos_returns.columns:\n",
    "        ret = oos_returns[ticker].dropna()\n",
    "        sig = oos_signals[ticker].dropna()\n",
    "        \n",
    "        # Align on common dates\n",
    "        common_idx = ret.index.intersection(sig.index)\n",
    "        ret = ret.loc[common_idx]\n",
    "        sig = sig.loc[common_idx]\n",
    "        \n",
    "        if len(ret) < 5:\n",
    "            print(f\"  {ticker}: insufficient OOS data ({len(ret)} days), skipping metrics\")\n",
    "            continue\n",
    "        \n",
    "        # Strategy returns: position = sign(signal), NOT raw magnitude\n",
    "        position = np.sign(sig.values)\n",
    "        strat_ret = position * ret.values\n",
    "        \n",
    "        # Transaction costs: deduct |delta_position| * bps_cost\n",
    "        pos_change = np.abs(np.diff(position, prepend=0))\n",
    "        costs_arr = pos_change * cfg.bps_cost\n",
    "        net_ret = strat_ret - costs_arr\n",
    "        \n",
    "        metrics = calculate_metrics(net_ret, costs=costs_arr.sum())\n",
    "        metrics['ticker'] = ticker\n",
    "        metrics['total_costs'] = float(costs_arr.sum())\n",
    "        all_metrics[ticker] = metrics\n",
    "        strat_net_returns[ticker] = pd.Series(net_ret, index=common_idx)\n",
    "        \n",
    "        print(f\"\\n  {ticker} (OOS: {ret.index[0].date()} to \"\n",
    "              f\"{ret.index[-1].date()}, {len(ret)} days):\")\n",
    "        print(f\"    Sharpe:        {metrics['sharpe']:+.2f}\")\n",
    "        print(f\"    Total Return:  {metrics['total_return']:+.2%}\")\n",
    "        print(f\"    Annual Return: {metrics['annual_return']:+.2%}\")\n",
    "        print(f\"    Max Drawdown:  {metrics['max_dd']:.2%}\")\n",
    "        print(f\"    Sortino:       {metrics['sortino']:+.2f}\")\n",
    "        print(f\"    Calmar:        {metrics['calmar']:+.2f}\")\n",
    "        print(f\"    Win Rate:      {metrics['win_rate']:.1%}\")\n",
    "        print(f\"    Profit Factor: {metrics['profit_factor']:.2f}\")\n",
    "        print(f\"    Total Costs:   {metrics['total_costs']:.4f}\")\n",
    "    \n",
    "    # ── PHASE 5: FOLD-BY-FOLD SUMMARY ──\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"  FOLD-BY-FOLD BREAKDOWN\")\n",
    "    print(\"=\" * 70)\n",
    "    fold_df = pd.DataFrame(results['fold_metrics'])\n",
    "    if not fold_df.empty:\n",
    "        print(fold_df.to_string(index=False))\n",
    "        \n",
    "        avg_sharpe = fold_df.groupby('ticker')['sharpe'].mean()\n",
    "        std_sharpe = fold_df.groupby('ticker')['sharpe'].std(ddof=1)\n",
    "        n_folds = fold_df.groupby('ticker')['sharpe'].count()\n",
    "        print(f\"\\n  Average OOS Sharpe per ticker:\")\n",
    "        for t in avg_sharpe.index:\n",
    "            se = std_sharpe[t] / np.sqrt(n_folds[t]) if n_folds[t] > 1 else 0.0\n",
    "            print(f\"    {t}: {avg_sharpe[t]:+.2f} +/- {std_sharpe[t]:.2f} \"\n",
    "                  f\"(SE={se:.2f}, {int(n_folds[t])} folds)\")\n",
    "    \n",
    "    # ── PHASE 5.5: ENSEMBLE (TFT + Momentum + Mean Reversion) ──\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"  PHASE 5.5: ENSEMBLE (TFT + Momentum + MR — majority vote)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    ensemble_metrics = {}\n",
    "    ensemble_net_returns = {}\n",
    "    for ticker in oos_returns.columns:\n",
    "        ret = oos_returns[ticker].dropna()\n",
    "        sig = oos_signals[ticker].dropna()\n",
    "        common_idx = ret.index.intersection(sig.index)\n",
    "\n",
    "        if len(common_idx) < 10 or ticker not in featured_data:\n",
    "            continue\n",
    "\n",
    "        df_feat = featured_data[ticker]\n",
    "        feat_idx = common_idx.intersection(df_feat.index)\n",
    "        if len(feat_idx) < 10:\n",
    "            continue\n",
    "\n",
    "        ret_e = ret.loc[feat_idx].values\n",
    "        sig_e = sig.loc[feat_idx].values\n",
    "\n",
    "        # Three signals\n",
    "        tft_pos = np.sign(sig_e)\n",
    "        mom_pos = np.sign(df_feat.loc[feat_idx, 'norm_ret_21d'].values)\n",
    "        mr_pos = -np.sign(df_feat.loc[feat_idx, 'mr_zscore'].values)\n",
    "\n",
    "        # Majority vote: sign of sum (2-of-3 or 3-of-3 agree)\n",
    "        ens_signal = tft_pos + mom_pos + mr_pos\n",
    "        ens_pos = np.sign(ens_signal)\n",
    "\n",
    "        # Ensemble returns with costs\n",
    "        ens_strat_ret = ens_pos * ret_e\n",
    "        pos_change = np.abs(np.diff(ens_pos, prepend=0))\n",
    "        costs = pos_change * cfg.bps_cost\n",
    "        ens_net = ens_strat_ret - costs\n",
    "\n",
    "        ens_m = calculate_metrics(ens_net, costs=costs.sum())\n",
    "        ensemble_metrics[ticker] = ens_m\n",
    "        ensemble_net_returns[ticker] = pd.Series(ens_net, index=feat_idx)\n",
    "\n",
    "        # Individual signal Sharpes (gross) for comparison\n",
    "        tft_sr = (float(np.mean(tft_pos * ret_e))\n",
    "                  / (float(np.std(tft_pos * ret_e, ddof=1)) + 1e-9)\n",
    "                  * np.sqrt(252))\n",
    "        mom_sr = (float(np.mean(mom_pos * ret_e))\n",
    "                  / (float(np.std(mom_pos * ret_e, ddof=1)) + 1e-9)\n",
    "                  * np.sqrt(252))\n",
    "        mr_sr = (float(np.mean(mr_pos * ret_e))\n",
    "                 / (float(np.std(mr_pos * ret_e, ddof=1)) + 1e-9)\n",
    "                 * np.sqrt(252))\n",
    "\n",
    "        # Signal agreement rate\n",
    "        agree_all = float(np.mean((tft_pos == mom_pos) & (mom_pos == mr_pos)))\n",
    "        agree_2of3 = float(np.mean(np.abs(ens_signal) >= 2))\n",
    "\n",
    "        print(f\"\\n  {ticker} ({len(ret_e)} days):\")\n",
    "        print(f\"    TFT Sharpe:        {tft_sr:+.2f}\")\n",
    "        print(f\"    Momentum Sharpe:   {mom_sr:+.2f}\")\n",
    "        print(f\"    MR Sharpe:         {mr_sr:+.2f}\")\n",
    "        print(f\"    Ensemble Sharpe:   {ens_m['sharpe']:+.2f}  (net of costs)\")\n",
    "        print(f\"    Ensemble Return:   {ens_m['total_return']:+.2%}\")\n",
    "        print(f\"    Ensemble MaxDD:    {ens_m['max_dd']:.2%}\")\n",
    "        print(f\"    Agreement (3/3):   {agree_all:.1%}\")\n",
    "        print(f\"    Agreement (2+/3):  {agree_2of3:.1%}\")\n",
    "\n",
    "    # ── PHASE 6: EDGE AUDIT ──\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"  PHASE 6: EDGE AUDIT — Is the alpha real?\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\n  Target definition:\")\n",
    "    print(\"    target_ret[t] = (close[t+1] - close[t]) / close[t]  [1-day forward return]\")\n",
    "    print(\"    signal[t] from features[t-W+1 : t]  [causal window, W=21]\")\n",
    "    print(\"    strat_ret[t] = sign(signal[t]) * target_ret[t]  [trade at close of day t]\")\n",
    "\n",
    "    for ticker in oos_returns.columns:\n",
    "        ret = oos_returns[ticker].dropna()\n",
    "        sig = oos_signals[ticker].dropna()\n",
    "        common_idx = ret.index.intersection(sig.index)\n",
    "        ret_a = ret.loc[common_idx]\n",
    "        sig_a = sig.loc[common_idx]\n",
    "\n",
    "        if len(ret_a) < 10:\n",
    "            continue\n",
    "\n",
    "        r = ret_a.values\n",
    "        position = np.sign(sig_a.values)\n",
    "        strat_ret_gross = position * r\n",
    "\n",
    "        # Turnover stats\n",
    "        pos_changes = np.abs(np.diff(position, prepend=0))\n",
    "        n_trades = int(np.sum(pos_changes > 0))\n",
    "        daily_turnover = np.mean(pos_changes)\n",
    "        gross_sharpe = (float(np.mean(strat_ret_gross))\n",
    "                        / (float(np.std(strat_ret_gross, ddof=1)) + 1e-9)\n",
    "                        * np.sqrt(252))\n",
    "\n",
    "        print(f\"\\n  {ticker} ({len(r)} OOS days, {n_trades} position changes, \"\n",
    "              f\"daily turnover {daily_turnover:.3f})\")\n",
    "        print(f\"    Gross Sharpe (no costs): {gross_sharpe:+.2f}\")\n",
    "\n",
    "        # ── Test 1: Sign-flip ──\n",
    "        flip_ret = -position * r\n",
    "        flip_sharpe = (float(np.mean(flip_ret))\n",
    "                       / (float(np.std(flip_ret, ddof=1)) + 1e-9)\n",
    "                       * np.sqrt(252))\n",
    "        verdict_1 = \"PASS\" if flip_sharpe < 0 else \"FAIL (signal is noise or drift)\"\n",
    "        print(f\"\\n    Test 1 — Sign flip:\")\n",
    "        print(f\"      Flipped Sharpe:  {flip_sharpe:+.2f}  [{verdict_1}]\")\n",
    "\n",
    "        # ── Test 2: Delay execution by 1 day ──\n",
    "        if len(r) > 2:\n",
    "            # position[t] applied to target_ret[t+1] instead of target_ret[t]\n",
    "            delay_ret = position[:-1] * r[1:]\n",
    "            delay_sharpe = (float(np.mean(delay_ret))\n",
    "                            / (float(np.std(delay_ret, ddof=1)) + 1e-9)\n",
    "                            * np.sqrt(252))\n",
    "            decay = gross_sharpe - delay_sharpe\n",
    "            if delay_sharpe < gross_sharpe * 0.5:\n",
    "                verdict_2 = \"CLEAN (edge decays with delay)\"\n",
    "            elif delay_sharpe < 0:\n",
    "                verdict_2 = \"CLEAN (edge reverses with delay)\"\n",
    "            else:\n",
    "                verdict_2 = \"SUSPICIOUS (edge persists — possible drift capture)\"\n",
    "            print(f\"\\n    Test 2 — Delay 1 day:\")\n",
    "            print(f\"      Delayed Sharpe:  {delay_sharpe:+.2f}  \"\n",
    "                  f\"(decay: {decay:+.2f})  [{verdict_2}]\")\n",
    "\n",
    "        # ── Test 3: Dumb baselines ──\n",
    "        if ticker in featured_data:\n",
    "            df_feat = featured_data[ticker]\n",
    "            common_feat_idx = common_idx.intersection(df_feat.index)\n",
    "            if len(common_feat_idx) > 10:\n",
    "                ret_bl = ret_a.loc[common_feat_idx].values\n",
    "\n",
    "                # Momentum: sign(yesterday's normalized return)\n",
    "                mom_pos = np.sign(\n",
    "                    df_feat.loc[common_feat_idx, 'norm_ret_1d'].values)\n",
    "                mom_ret = mom_pos * ret_bl\n",
    "                mom_sharpe = (float(np.mean(mom_ret))\n",
    "                              / (float(np.std(mom_ret, ddof=1)) + 1e-9)\n",
    "                              * np.sqrt(252))\n",
    "\n",
    "                # Mean reversion: -sign(yesterday's normalized return)\n",
    "                mr_ret = -mom_pos * ret_bl\n",
    "                mr_sharpe = (float(np.mean(mr_ret))\n",
    "                             / (float(np.std(mr_ret, ddof=1)) + 1e-9)\n",
    "                             * np.sqrt(252))\n",
    "\n",
    "                best_baseline = max(mom_sharpe, mr_sharpe)\n",
    "                advantage = gross_sharpe - best_baseline\n",
    "                if advantage > 0.5:\n",
    "                    verdict_3 = \"PASS\"\n",
    "                elif advantage > 0:\n",
    "                    verdict_3 = \"MARGINAL\"\n",
    "                else:\n",
    "                    verdict_3 = \"FAIL (TFT doesn't beat simple baseline)\"\n",
    "                print(f\"\\n    Test 3 — Baselines:\")\n",
    "                print(f\"      Momentum baseline: {mom_sharpe:+.2f}\")\n",
    "                print(f\"      MR baseline:       {mr_sharpe:+.2f}\")\n",
    "                print(f\"      TFT advantage:     {advantage:+.2f}  [{verdict_3}]\")\n",
    "\n",
    "        # ── Test 4: Cost sensitivity sweep ──\n",
    "        print(f\"\\n    Test 4 — Cost sensitivity:\")\n",
    "        for bps in [0, 5, 10, 15, 20, 30]:\n",
    "            cost_per_change = bps / 10000.0\n",
    "            costs = pos_changes * cost_per_change\n",
    "            net = strat_ret_gross - costs\n",
    "            net_sharpe = (float(np.mean(net))\n",
    "                          / (float(np.std(net, ddof=1)) + 1e-9)\n",
    "                          * np.sqrt(252))\n",
    "            total_cost_pct = costs.sum() * 100\n",
    "            marker = \" <-- current\" if bps == int(cfg.bps_cost * 10000) else \"\"\n",
    "            print(f\"      {bps:2d} bps: Sharpe {net_sharpe:+.2f}  \"\n",
    "                  f\"(total cost: {total_cost_pct:.2f}%){marker}\")\n",
    "\n",
    "        # ── Target alignment: first 5 OOS dates ──\n",
    "        print(f\"\\n    Target alignment (first 5 OOS dates):\")\n",
    "        for i, dt in enumerate(common_idx[:5]):\n",
    "            print(f\"      {dt.date()}: signal={sig_a.loc[dt]:+.4f}  \"\n",
    "                  f\"target_ret={ret_a.loc[dt]:+.6f}  \"\n",
    "                  f\"pos={int(np.sign(sig_a.loc[dt]))}\")\n",
    "\n",
    "    # ── Fold stability ──\n",
    "    if not fold_df.empty and len(fold_df) > 2:\n",
    "        print(f\"\\n  Fold Stability:\")\n",
    "        for ticker in fold_df['ticker'].unique():\n",
    "            t_folds = fold_df[fold_df['ticker'] == ticker]\n",
    "            sharpes = t_folds['sharpe'].values\n",
    "            n_pos = int(np.sum(sharpes > 0))\n",
    "            n_neg = int(np.sum(sharpes <= 0))\n",
    "            print(f\"    {ticker}: {len(sharpes)} folds, \"\n",
    "                  f\"mean={np.mean(sharpes):+.2f}, \"\n",
    "                  f\"std={np.std(sharpes, ddof=1):.2f}, \"\n",
    "                  f\"min={np.min(sharpes):+.2f}, max={np.max(sharpes):+.2f}, \"\n",
    "                  f\"+ve/-ve={n_pos}/{n_neg}\")\n",
    "            if np.std(sharpes, ddof=1) > 3.0:\n",
    "                print(f\"      WARNING: High fold variance — edge is unstable\")\n",
    "\n",
    "    # ── PHASE 7: VISUALIZATION ──\n",
    "    if not all_metrics:\n",
    "        print(\"\\n  No metrics to visualize.\")\n",
    "        return {'raw_data': raw_data, 'featured_data': featured_data,\n",
    "                'results': results, 'all_metrics': all_metrics, 'cfg': cfg}\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "    fig.suptitle('QuantKubera Monolith v2 -- Walk-Forward OOS Results',\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 6a. Equity curves (net of costs)\n",
    "    ax = axes[0, 0]\n",
    "    for ticker, net_s in strat_net_returns.items():\n",
    "        equity = (1.0 + net_s).cumprod()\n",
    "        label = f\"{ticker} (Sharpe={all_metrics[ticker]['sharpe']:+.2f})\"\n",
    "        ax.plot(equity.index, equity.values, label=label, linewidth=1.5)\n",
    "    ax.set_title('OOS Equity Curves (net of costs)')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylabel('Growth of $1')\n",
    "    ax.axhline(1.0, color='gray', linestyle='--', alpha=0.4)\n",
    "    \n",
    "    # 6b. Drawdown\n",
    "    ax = axes[0, 1]\n",
    "    for ticker, net_s in strat_net_returns.items():\n",
    "        equity = (1.0 + net_s).cumprod()\n",
    "        peak = equity.cummax()\n",
    "        dd = (equity - peak) / peak\n",
    "        ax.fill_between(dd.index, dd.values, alpha=0.3, label=ticker)\n",
    "    ax.set_title('Drawdown')\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylabel('Drawdown')\n",
    "    \n",
    "    # 6c. VSN Feature Importance (first ticker with weights)\n",
    "    ax = axes[1, 0]\n",
    "    if results['vsn_weights']:\n",
    "        first_ticker = list(results['vsn_weights'].keys())[0]\n",
    "        vsn_w = results['vsn_weights'][first_ticker]\n",
    "        n_features_to_show = min(15, len(FEATURE_COLUMNS), len(vsn_w))\n",
    "        sorted_idx = np.argsort(vsn_w)[::-1][:n_features_to_show]\n",
    "        ax.barh(range(n_features_to_show),\n",
    "                vsn_w[sorted_idx],\n",
    "                color='steelblue')\n",
    "        ax.set_yticks(range(n_features_to_show))\n",
    "        ax.set_yticklabels([FEATURE_COLUMNS[i] for i in sorted_idx], fontsize=8)\n",
    "        ax.set_title(f'VSN Feature Importance ({first_ticker})')\n",
    "        ax.invert_yaxis()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlabel('Weight')\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, 'No VSN weights available',\n",
    "                ha='center', va='center', transform=ax.transAxes)\n",
    "    \n",
    "    # 6d. Monthly returns bar chart (first ticker)\n",
    "    ax = axes[1, 1]\n",
    "    first_ticker_key = list(strat_net_returns.keys())[0]\n",
    "    monthly = strat_net_returns[first_ticker_key].resample('ME').sum()\n",
    "    colors = ['#d32f2f' if r < 0 else '#388e3c' for r in monthly.values]\n",
    "    x_pos = range(len(monthly))\n",
    "    ax.bar(x_pos, monthly.values * 100, color=colors, alpha=0.7)\n",
    "    ax.set_title(f'Monthly Returns (%) -- {first_ticker_key}')\n",
    "    ax.set_ylabel('Return %')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    # Label x-axis with month abbreviations if not too many\n",
    "    if len(monthly) <= 36:\n",
    "        ax.set_xticks(list(x_pos))\n",
    "        ax.set_xticklabels([d.strftime('%b %y') for d in monthly.index],\n",
    "                           rotation=45, ha='right', fontsize=7)\n",
    "    ax.axhline(0, color='black', linewidth=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Return results for downstream cells (VBT, etc.)\n",
    "    return {\n",
    "        'raw_data': raw_data,\n",
    "        'featured_data': featured_data,\n",
    "        'results': results,\n",
    "        'all_metrics': all_metrics,\n",
    "        'strat_net_returns': strat_net_returns,\n",
    "        'cfg': cfg,\n",
    "    }\n",
    "\n",
    "\n",
    "# ── RUN THE LAB ──\n",
    "lab_output = run_enhanced_lab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72b12ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. VECTORBTPRO TEARSHEET & TRADE ANALYSIS\n",
    "import vectorbtpro as vbt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"  VECTORBTPRO TEARSHEET & TRADE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use results from Cell 7 (lab_output variable)\n",
    "oos_returns = lab_output['results']['oos_returns']\n",
    "oos_signals = lab_output['results']['oos_signals']\n",
    "cfg = lab_output['cfg']\n",
    "\n",
    "for ticker in oos_returns.columns:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"  {ticker} TEARSHEET\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    \n",
    "    sig = oos_signals[ticker].dropna()\n",
    "    ret = oos_returns[ticker].dropna()\n",
    "    \n",
    "    if len(sig) < 10:\n",
    "        print(f\"  Insufficient OOS data for {ticker} ({len(sig)} days), skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Get close prices for the OOS period from featured_data\n",
    "    feat_df = lab_output['featured_data'][ticker]\n",
    "    close = feat_df['close'].reindex(sig.index).dropna()\n",
    "    \n",
    "    # Align all series on common dates\n",
    "    common_idx = sig.index.intersection(close.index)\n",
    "    if len(common_idx) < 10:\n",
    "        print(f\"  Insufficient aligned data for {ticker} ({len(common_idx)} days), skipping.\")\n",
    "        continue\n",
    "    \n",
    "    sig = sig.loc[common_idx]\n",
    "    close = close.loc[common_idx]\n",
    "    \n",
    "    # Build position series with T+1 lag (signal at t -> trade at t+1)\n",
    "    position = np.sign(sig).shift(1).fillna(0)\n",
    "    \n",
    "    # Derive entry/exit signals from position changes\n",
    "    prev_pos = position.shift(1).fillna(0)\n",
    "    long_entries  = (position > 0) & (prev_pos <= 0)\n",
    "    long_exits    = (position <= 0) & (prev_pos > 0)\n",
    "    short_entries = (position < 0) & (prev_pos >= 0)\n",
    "    short_exits   = (position >= 0) & (prev_pos < 0)\n",
    "    \n",
    "    # VBT Portfolio: long/short from signals\n",
    "    pf = vbt.Portfolio.from_signals(\n",
    "        close=close,\n",
    "        long_entries=long_entries,\n",
    "        long_exits=long_exits,\n",
    "        short_entries=short_entries,\n",
    "        short_exits=short_exits,\n",
    "        fees=cfg.bps_cost / 2,       # per-side fee\n",
    "        slippage=cfg.bps_cost / 2,    # per-side slippage\n",
    "        freq='1D',\n",
    "        init_cash=1_000_000,\n",
    "    )\n",
    "    \n",
    "    # Print full stats\n",
    "    print(pf.stats())\n",
    "    \n",
    "    # Trade-level analysis\n",
    "    if hasattr(pf, 'trades') and pf.trades.count > 0:\n",
    "        trades = pf.trades\n",
    "        print(f\"\\n  --- Trade Summary ---\")\n",
    "        print(f\"  Total Trades:   {trades.count}\")\n",
    "        print(f\"  Win Rate:       {trades.win_rate:.2%}\")\n",
    "        print(f\"  Profit Factor:  {trades.profit_factor:.2f}\")\n",
    "        print(f\"  Avg P&L:        {trades.pnl.mean():.2f}\")\n",
    "        print(f\"  Max Win:        {trades.pnl.max():.2f}\")\n",
    "        print(f\"  Max Loss:       {trades.pnl.min():.2f}\")\n",
    "        print(f\"  Expectancy:     {trades.expectancy:.2f}\")\n",
    "        print(f\"  Avg Duration:   {trades.duration.mean()}\")\n",
    "        \n",
    "        # Show recent trades\n",
    "        readable = trades.records_readable\n",
    "        if len(readable) > 0:\n",
    "            print(f\"\\n  --- Last 10 Trades ---\")\n",
    "            print(readable.tail(10).to_string())\n",
    "    else:\n",
    "        print(\"  No trades executed.\")\n",
    "    \n",
    "    # Plot equity curve\n",
    "    try:\n",
    "        fig = pf.plot()\n",
    "        fig.update_layout(\n",
    "            title=f'{ticker} -- VectorBTPro Equity Curve',\n",
    "            height=400,\n",
    "            template='plotly_white'\n",
    "        )\n",
    "        fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"  Plot failed: {e}\")\n",
    "        # Fallback: matplotlib equity curve\n",
    "        equity = pf.value()\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(equity.index, equity.values, linewidth=1.5)\n",
    "        plt.title(f'{ticker} -- VectorBTPro Equity Curve')\n",
    "        plt.ylabel('Portfolio Value')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"  VectorBTPro analysis complete\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qk_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
