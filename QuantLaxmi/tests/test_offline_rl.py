"""Tests for offline RL algorithms: CQL, IQL, TD3+BC.

Tests cover:
  - CQL: conservative penalty, alpha auto-tuning, Q-value suppression, training
  - IQL: expectile regression, advantage weighting, no OOD queries, training
  - TD3+BC: BC regularization, twin critics, training convergence
  - Shared: offline buffer, predict interface, synthetic dataset

At least 18 tests total, all using synthetic offline data.
"""
from __future__ import annotations

import sys

sys.path.insert(0, "/home/ubuntu/Desktop/7hills/QuantLaxmi")

import numpy as np
import pytest
import torch

from quantlaxmi.models.rl.algorithms.offline_rl import (
    CQL,
    IQL,
    TD3BC,
    OfflineReplayBuffer,
)


# ============================================================================
# Fixtures: synthetic offline datasets
# ============================================================================


def _make_linear_dataset(
    n_samples: int = 1000,
    state_dim: int = 4,
    action_dim: int = 2,
    seed: int = 42,
) -> OfflineReplayBuffer:
    """Synthetic offline dataset with linear reward structure.

    Reward = state[0] * action[0] + state[1] * action[1] + noise.
    Optimal policy: action = sign(state[:action_dim]).
    """
    rng = np.random.default_rng(seed)
    states = rng.standard_normal((n_samples, state_dim)).astype(np.float32)
    actions = rng.uniform(-1, 1, (n_samples, action_dim)).astype(np.float32)
    rewards = np.sum(states[:, :action_dim] * actions, axis=1) + rng.normal(0, 0.1, n_samples)
    rewards = rewards.astype(np.float32)
    next_states = states + rng.standard_normal((n_samples, state_dim)).astype(np.float32) * 0.1
    dones = rng.random(n_samples) < 0.05  # 5% terminal
    return OfflineReplayBuffer(states, actions, rewards, next_states, dones.astype(np.float32), seed=seed)


def _make_discrete_dataset(
    n_samples: int = 1000,
    state_dim: int = 4,
    num_actions: int = 3,
    seed: int = 42,
) -> OfflineReplayBuffer:
    """Synthetic discrete-action offline dataset.

    Action 0 is optimal when state[0] > 0, action 1 otherwise.
    """
    rng = np.random.default_rng(seed)
    states = rng.standard_normal((n_samples, state_dim)).astype(np.float32)
    actions = rng.integers(0, num_actions, n_samples).astype(np.float32)
    optimal = (states[:, 0] > 0).astype(np.float32)
    rewards = np.where(actions == optimal, 1.0, -0.5).astype(np.float32)
    next_states = states + rng.standard_normal((n_samples, state_dim)).astype(np.float32) * 0.1
    dones = rng.random(n_samples) < 0.05
    return OfflineReplayBuffer(states, actions, rewards, next_states, dones.astype(np.float32), seed=seed)


@pytest.fixture
def linear_dataset():
    return _make_linear_dataset()


@pytest.fixture
def discrete_dataset():
    return _make_discrete_dataset()


# ============================================================================
# Offline Replay Buffer Tests
# ============================================================================


class TestOfflineReplayBuffer:
    """Tests for OfflineReplayBuffer."""

    def test_buffer_from_transitions(self):
        """Build buffer from list of transition tuples."""
        rng = np.random.default_rng(42)
        transitions = [
            (rng.standard_normal(4), rng.standard_normal(2), float(rng.random()),
             rng.standard_normal(4), bool(rng.random() < 0.1))
            for _ in range(100)
        ]
        buf = OfflineReplayBuffer.from_transitions(transitions)
        assert len(buf) == 100

    def test_buffer_sample_shapes(self, linear_dataset):
        """Verify sample returns correct shapes."""
        s, a, r, ns, d = linear_dataset.sample(32)
        assert s.shape == (32, 4)
        assert a.shape == (32, 2)
        assert r.shape == (32,)
        assert ns.shape == (32, 4)
        assert d.shape == (32,)

    def test_buffer_no_exceeds_size(self):
        """Sampling more than buffer size returns buffer size."""
        buf = _make_linear_dataset(n_samples=50)
        s, a, r, ns, d = buf.sample(100)
        assert s.shape[0] == 50


# ============================================================================
# CQL Tests
# ============================================================================


class TestCQL:
    """Tests for Conservative Q-Learning."""

    def test_cql_init_continuous(self):
        """CQL initializes correctly for continuous actions."""
        cql = CQL(state_dim=4, action_dim=2, device="cpu", seed=42)
        assert cql.state_dim == 4
        assert cql.action_dim == 2
        assert not cql.discrete
        assert cql.alpha == 1.0

    def test_cql_init_discrete(self):
        """CQL initializes correctly for discrete actions."""
        cql = CQL(state_dim=4, action_dim=3, discrete=True, device="cpu", seed=42)
        assert cql.discrete
        assert cql.action_dim == 3

    def test_cql_conservative_penalty_positive(self, linear_dataset):
        """CQL conservative penalty should be positive (logsumexp >= data Q)."""
        cql = CQL(state_dim=4, action_dim=2, device="cpu", seed=42)
        s, a, r, ns, d = linear_dataset.sample(64)
        metrics = cql.train_step(s, a, r, ns, d)
        # The penalty can be negative or positive initially, but total_loss should be finite
        assert np.isfinite(metrics["cql_penalty"])
        assert np.isfinite(metrics["total_loss"])

    def test_cql_penalty_reduces_q_values(self, linear_dataset):
        """CQL with higher alpha should produce lower Q-values than standard TD."""
        # Train two models: one with high alpha, one with low
        cql_high = CQL(state_dim=4, action_dim=2, cql_alpha=10.0,
                       min_q_weight=10.0, device="cpu", seed=42,
                       hidden_layers=(64,))
        cql_low = CQL(state_dim=4, action_dim=2, cql_alpha=0.0,
                      min_q_weight=0.0, device="cpu", seed=42,
                      hidden_layers=(64,))

        # Train both for a few steps
        for _ in range(50):
            s, a, r, ns, d = linear_dataset.sample(64)
            cql_high.train_step(s, a, r, ns, d)
            cql_low.train_step(s, a, r, ns, d)

        # Compare Q-values on random (s, a) pairs â€” CQL with high alpha
        # should generally produce lower Q-values
        test_s = linear_dataset.states[:20]
        test_a = linear_dataset.actions[:20]
        sa = np.concatenate([test_s, test_a], axis=-1)
        with torch.no_grad():
            sa_t = torch.FloatTensor(sa)
            q_high = cql_high.q_network(sa_t).numpy().mean()
            q_low = cql_low.q_network(sa_t).numpy().mean()

        # Q-values with strong CQL penalty should be lower
        assert q_high < q_low, (
            f"CQL with high alpha ({q_high:.4f}) should produce lower Q "
            f"than low alpha ({q_low:.4f})"
        )

    def test_cql_alpha_auto_tuning(self, linear_dataset):
        """Alpha auto-tuning: alpha should change during training."""
        cql = CQL(
            state_dim=4, action_dim=2, cql_alpha=1.0,
            tau_target=5.0,  # Enable auto-tuning
            device="cpu", seed=42, hidden_layers=(64,),
        )
        alpha_before = cql.alpha
        for _ in range(100):
            s, a, r, ns, d = linear_dataset.sample(64)
            cql.train_step(s, a, r, ns, d)
        alpha_after = cql.alpha
        # Alpha should have changed (adjusted to match tau_target)
        assert alpha_before != alpha_after, (
            f"Alpha should change during auto-tuning: before={alpha_before}, after={alpha_after}"
        )

    def test_cql_discrete_training(self, discrete_dataset):
        """CQL works with discrete action spaces."""
        cql = CQL(
            state_dim=4, action_dim=3, discrete=True,
            device="cpu", seed=42, hidden_layers=(64,),
        )
        history = cql.train(discrete_dataset, n_epochs=5, batch_size=64)
        assert len(history) == 5
        assert all(np.isfinite(h["total_loss"]) for h in history)

    def test_cql_predict_continuous(self, linear_dataset):
        """CQL predict returns valid continuous action."""
        cql = CQL(state_dim=4, action_dim=2, device="cpu", seed=42,
                  action_low=-1.0, action_high=1.0)
        # Train briefly
        for _ in range(10):
            s, a, r, ns, d = linear_dataset.sample(64)
            cql.train_step(s, a, r, ns, d)

        obs = np.zeros(4, dtype=np.float32)
        action = cql.predict(obs)
        assert action.shape == (2,)
        assert np.all(action >= -1.0) and np.all(action <= 1.0)

    def test_cql_predict_discrete(self, discrete_dataset):
        """CQL predict returns valid discrete action index."""
        cql = CQL(state_dim=4, action_dim=3, discrete=True, device="cpu", seed=42)
        for _ in range(10):
            s, a, r, ns, d = discrete_dataset.sample(64)
            cql.train_step(s, a, r, ns, d)

        obs = np.zeros(4, dtype=np.float32)
        action = cql.predict(obs)
        assert action.shape == (1,)
        assert action[0] in [0, 1, 2]

    def test_cql_get_q_values_discrete(self, discrete_dataset):
        """get_q_values returns Q for all actions in discrete mode."""
        cql = CQL(state_dim=4, action_dim=3, discrete=True, device="cpu", seed=42)
        obs = np.zeros(4, dtype=np.float32)
        q_vals = cql.get_q_values(obs)
        assert q_vals.shape == (3,)
        assert np.all(np.isfinite(q_vals))

    def test_cql_training_loss_decreases(self, linear_dataset):
        """TD loss should generally decrease over training."""
        cql = CQL(
            state_dim=4, action_dim=2, cql_alpha=0.1, min_q_weight=1.0,
            device="cpu", seed=42, hidden_layers=(64,),
        )
        history = cql.train(linear_dataset, n_epochs=50, batch_size=128)
        # Compare first 5 epochs avg vs last 5 epochs avg
        early_td = np.mean([h["td_loss"] for h in history[:5]])
        late_td = np.mean([h["td_loss"] for h in history[-5:]])
        assert late_td < early_td * 1.5, (
            f"TD loss should not increase much: early={early_td:.4f}, late={late_td:.4f}"
        )


# ============================================================================
# IQL Tests
# ============================================================================


class TestIQL:
    """Tests for Implicit Q-Learning."""

    def test_iql_init(self):
        """IQL initializes correctly."""
        iql = IQL(state_dim=4, action_dim=2, device="cpu", seed=42)
        assert iql.state_dim == 4
        assert iql.action_dim == 2
        assert iql.expectile_tau == 0.7
        assert iql.beta == 3.0

    def test_iql_expectile_loss_asymmetric(self):
        """Expectile loss should be asymmetric: tau > 0.5 penalizes negative diff more."""
        iql = IQL(state_dim=4, action_dim=2, device="cpu", seed=42)

        # Positive diff: Q > V (underestimation of V)
        pos_diff = torch.tensor([1.0, 2.0, 3.0])
        loss_pos = iql._expectile_loss(pos_diff, tau=0.7)

        # Negative diff: Q < V (overestimation of V)
        neg_diff = torch.tensor([-1.0, -2.0, -3.0])
        loss_neg = iql._expectile_loss(neg_diff, tau=0.7)

        # With tau=0.7:
        # For pos diff: weight = 0.7, loss = 0.7 * mean(u^2)
        # For neg diff: weight = 0.3, loss = 0.3 * mean(u^2)
        # So loss_pos > loss_neg (same magnitude, but tau > 1-tau)
        assert loss_pos.item() > loss_neg.item(), (
            f"tau=0.7 should penalize positive diff (underest) more: "
            f"pos={loss_pos.item():.4f}, neg={loss_neg.item():.4f}"
        )

    def test_iql_expectile_loss_symmetric_at_half(self):
        """At tau=0.5, expectile loss equals standard MSE (symmetric)."""
        iql = IQL(state_dim=4, action_dim=2, device="cpu", seed=42)

        diff = torch.tensor([1.0, -2.0, 3.0, -0.5])
        loss_expectile = iql._expectile_loss(diff, tau=0.5)
        loss_mse = (0.5 * diff.pow(2)).mean()

        assert torch.allclose(loss_expectile, loss_mse, atol=1e-6), (
            f"At tau=0.5, expectile should equal 0.5*MSE: "
            f"expectile={loss_expectile.item():.6f}, 0.5*mse={loss_mse.item():.6f}"
        )

    def test_iql_no_ood_action_evaluation(self, linear_dataset):
        """IQL never evaluates Q at actions not in the dataset.

        We verify this by checking that Q-network forward pass during
        training only receives dataset (s,a) pairs.
        """
        iql = IQL(state_dim=4, action_dim=2, device="cpu", seed=42, hidden_layers=(32,))

        # Monkey-patch Q-network to track inputs
        original_forward = iql.q_network.forward
        seen_inputs = []

        def tracking_forward(x):
            seen_inputs.append(x.detach().clone())
            return original_forward(x)

        iql.q_network.forward = tracking_forward

        # Train one step
        s, a, r, ns, d = linear_dataset.sample(32)
        iql.train_step(s, a, r, ns, d)

        # All Q-network inputs should be [s, a] from dataset
        # The Q-network should never be called with random/policy actions
        for inp in seen_inputs:
            # Each input should be (batch, state_dim + action_dim) = (batch, 6)
            assert inp.shape[1] == 6, (
                f"Q-network received input of shape {inp.shape}, "
                f"expected (*, 6) for state_dim=4, action_dim=2"
            )

        # Restore
        iql.q_network.forward = original_forward

    def test_iql_advantage_weighting(self, linear_dataset):
        """Higher-advantage transitions should get larger policy update weights."""
        iql = IQL(state_dim=4, action_dim=2, device="cpu", seed=42, hidden_layers=(32,))

        # Train briefly to get meaningful Q and V
        for _ in range(20):
            s, a, r, ns, d = linear_dataset.sample(64)
            iql.train_step(s, a, r, ns, d)

        # Compute advantages for a batch
        s, a, r, ns, d = linear_dataset.sample(32)
        with torch.no_grad():
            states_t = torch.FloatTensor(s)
            actions_t = torch.FloatTensor(a)
            sa = torch.cat([states_t, actions_t], dim=-1)
            q_vals = iql.q_target(sa).squeeze(-1)
            v_vals = iql.v_network(states_t).squeeze(-1)
            advantages = q_vals - v_vals

            weights = torch.exp(iql.beta * advantages / iql.temperature)
            weights = torch.clamp(weights, max=100.0)

        # Weights should be higher where advantages are higher
        # Sort by advantage and check weight monotonicity (approximately)
        sorted_idx = advantages.argsort()
        sorted_weights = weights[sorted_idx]
        # The top-5 weights should generally be >= bottom-5 weights
        top_mean = sorted_weights[-5:].mean().item()
        bottom_mean = sorted_weights[:5].mean().item()
        assert top_mean >= bottom_mean, (
            f"High-advantage weights ({top_mean:.4f}) should be >= "
            f"low-advantage weights ({bottom_mean:.4f})"
        )

    def test_iql_predict_returns_valid_action(self, linear_dataset):
        """IQL predict returns action within bounds."""
        iql = IQL(state_dim=4, action_dim=2, action_low=-1.0, action_high=1.0,
                  device="cpu", seed=42)
        for _ in range(10):
            s, a, r, ns, d = linear_dataset.sample(64)
            iql.train_step(s, a, r, ns, d)

        obs = np.zeros(4, dtype=np.float32)
        action = iql.predict(obs)
        assert action.shape == (2,)
        assert np.all(action >= -1.0) and np.all(action <= 1.0)

    def test_iql_get_value_and_q(self, linear_dataset):
        """get_value and get_q_value return finite scalars."""
        iql = IQL(state_dim=4, action_dim=2, device="cpu", seed=42)
        obs = np.zeros(4, dtype=np.float32)
        action = np.zeros(2, dtype=np.float32)
        v = iql.get_value(obs)
        q = iql.get_q_value(obs, action)
        assert np.isfinite(v)
        assert np.isfinite(q)

    def test_iql_training_runs(self, linear_dataset):
        """IQL full training loop completes without errors."""
        iql = IQL(
            state_dim=4, action_dim=2, device="cpu", seed=42,
            hidden_layers=(32,),
        )
        history = iql.train(linear_dataset, n_epochs=10, batch_size=64)
        assert len(history) == 10
        assert all(np.isfinite(h["v_loss"]) for h in history)
        assert all(np.isfinite(h["q_loss"]) for h in history)
        assert all(np.isfinite(h["policy_loss"]) for h in history)

    def test_iql_v_approximates_expectile_of_q(self, linear_dataset):
        """After training, V(s) should approximate the tau-expectile of Q(s,a)."""
        iql = IQL(
            state_dim=4, action_dim=2, expectile_tau=0.7,
            device="cpu", seed=42, hidden_layers=(64,),
        )
        iql.train(linear_dataset, n_epochs=30, batch_size=128)

        # Get Q and V for dataset points
        with torch.no_grad():
            states_t = torch.FloatTensor(linear_dataset.states[:200])
            actions_t = torch.FloatTensor(linear_dataset.actions[:200])
            sa = torch.cat([states_t, actions_t], dim=-1)
            q_vals = iql.q_target(sa).squeeze(-1).numpy()
            v_vals = iql.v_network(states_t).squeeze(-1).numpy()

        # V should be between mean(Q) and max(Q) since tau > 0.5
        # (it's an upper expectile)
        diff = q_vals - v_vals
        # With tau=0.7, V should be biased toward higher Q values
        # So the fraction of Q > V should be roughly 1 - tau = 0.3
        frac_above = (diff > 0).mean()
        # Allow generous tolerance for finite samples + training noise
        assert 0.05 < frac_above < 0.8, (
            f"Fraction of Q > V should be roughly 0.3 (tau=0.7), got {frac_above:.2f}"
        )


# ============================================================================
# TD3+BC Tests
# ============================================================================


class TestTD3BC:
    """Tests for TD3 with Behavioral Cloning."""

    def test_td3bc_init(self):
        """TD3+BC initializes correctly."""
        agent = TD3BC(state_dim=4, action_dim=2, device="cpu", seed=42)
        assert agent.state_dim == 4
        assert agent.action_dim == 2
        assert agent.bc_alpha == 2.5
        assert agent.policy_delay == 2

    def test_td3bc_training_runs(self, linear_dataset):
        """TD3+BC full training loop completes."""
        agent = TD3BC(
            state_dim=4, action_dim=2, device="cpu", seed=42,
            hidden_layers=(32,),
        )
        history = agent.train(linear_dataset, n_epochs=5, batch_size=64)
        assert len(history) == 5
        assert all(np.isfinite(h["critic_loss"]) for h in history)

    def test_td3bc_predict(self, linear_dataset):
        """TD3+BC predict returns valid actions."""
        agent = TD3BC(
            state_dim=4, action_dim=2, action_low=-1.0, action_high=1.0,
            device="cpu", seed=42,
        )
        for _ in range(10):
            s, a, r, ns, d = linear_dataset.sample(64)
            agent.train_step(s, a, r, ns, d)

        obs = np.zeros(4, dtype=np.float32)
        action = agent.predict(obs)
        assert action.shape == (2,)
        assert np.all(action >= -1.0) and np.all(action <= 1.0)

    def test_td3bc_delayed_policy_update(self, linear_dataset):
        """Actor loss should be 0 when not updated (delayed update)."""
        agent = TD3BC(
            state_dim=4, action_dim=2, policy_delay=3,
            device="cpu", seed=42, hidden_layers=(32,),
        )
        agent._train_steps = 0  # Reset counter

        s, a, r, ns, d = linear_dataset.sample(64)
        # Step 1: actor should NOT be updated
        m1 = agent.train_step(s, a, r, ns, d)
        assert m1["actor_loss"] == 0.0, "Actor should not update at step 1 (delay=3)"

        # Step 2: still no update
        m2 = agent.train_step(s, a, r, ns, d)
        assert m2["actor_loss"] == 0.0, "Actor should not update at step 2 (delay=3)"

        # Step 3: NOW actor should update
        m3 = agent.train_step(s, a, r, ns, d)
        assert m3["actor_loss"] != 0.0, "Actor should update at step 3 (delay=3)"

    def test_td3bc_bc_regularization_effect(self, linear_dataset):
        """Higher bc_alpha should make policy closer to dataset actions."""
        # Train with high BC weight
        agent_high_bc = TD3BC(
            state_dim=4, action_dim=2, bc_alpha=100.0,
            device="cpu", seed=42, hidden_layers=(32,),
        )
        agent_high_bc.train(linear_dataset, n_epochs=20, batch_size=64)

        # Train with no BC (very low alpha)
        agent_low_bc = TD3BC(
            state_dim=4, action_dim=2, bc_alpha=0.001,
            device="cpu", seed=42, hidden_layers=(32,),
        )
        agent_low_bc.train(linear_dataset, n_epochs=20, batch_size=64)

        # Compare: high BC agent predictions should be closer to dataset
        test_states = linear_dataset.states[:50]
        test_actions = linear_dataset.actions[:50]

        pred_high = np.array([agent_high_bc.predict(s) for s in test_states])
        pred_low = np.array([agent_low_bc.predict(s) for s in test_states])

        mse_high = np.mean((pred_high - test_actions) ** 2)
        mse_low = np.mean((pred_low - test_actions) ** 2)

        # High BC should be closer to data (lower MSE)
        assert mse_high < mse_low * 2.0, (
            f"High BC MSE ({mse_high:.4f}) should be lower than "
            f"low BC MSE ({mse_low:.4f})"
        )


# ============================================================================
# Cross-algorithm tests
# ============================================================================


class TestCrossAlgorithm:
    """Tests that apply to all offline RL algorithms."""

    @pytest.mark.parametrize("AlgClass,kwargs", [
        (CQL, {"state_dim": 4, "action_dim": 2, "device": "cpu", "seed": 42, "hidden_layers": (32,)}),
        (IQL, {"state_dim": 4, "action_dim": 2, "device": "cpu", "seed": 42, "hidden_layers": (32,)}),
        (TD3BC, {"state_dim": 4, "action_dim": 2, "device": "cpu", "seed": 42, "hidden_layers": (32,)}),
    ])
    def test_all_train_and_predict(self, AlgClass, kwargs, linear_dataset):
        """All algorithms complete train+predict cycle."""
        agent = AlgClass(**kwargs)
        agent.train(linear_dataset, n_epochs=3, batch_size=64)
        action = agent.predict(np.zeros(4, dtype=np.float32))
        assert action is not None
        assert np.all(np.isfinite(action))
