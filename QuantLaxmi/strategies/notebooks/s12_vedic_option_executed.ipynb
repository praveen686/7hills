{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18cb647d",
   "metadata": {},
   "source": [
    "# S12: Vedic Fractional Alpha \u2014 Option-Executed Research Notebook\n",
    "\n",
    "**Strategy**: Anomalous diffusion detection via fractional calculus + Vedic mathematics, executed on ATM index options.\n",
    "\n",
    "**Key innovations over base S12**:\n",
    "1. Features computed from **1-minute bars** (not daily closes)\n",
    "2. SANOS calibrated from **nfo_1min CE/PE** bars (316 days vs 125 from bhavcopy)\n",
    "3. Trades executed on **actual ATM options** with prices from nfo_1min\n",
    "4. **Walk-forward OOS validation** (Optuna with train/test splits)\n",
    "\n",
    "**Self-contained**: No QuantLaxmi imports \u2014 all math reimplemented from first principles.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Imports & Data Connection](#1)\n",
    "2. [Data Exploration](#2)\n",
    "3. [SANOS Mathematics](#3)\n",
    "4. [SANOS Implementation](#4)\n",
    "5. [Option Chain from 1-Min Bars](#5)\n",
    "6. [SANOS Calibration (316 Days)](#6)\n",
    "7. [SANOS Visualization](#7)\n",
    "8. [Fractional Calculus Mathematics](#8)\n",
    "9. [Fractional Features](#9)\n",
    "10. [Mock Theta Mathematics](#10)\n",
    "11. [Mock Theta Implementation](#11)\n",
    "12. [Vedic Angular Mathematics](#12)\n",
    "13. [Vedic Angular Implementation](#13)\n",
    "14. [Timothy Masters Indicators](#14)\n",
    "15. [Auxiliary Features](#15)\n",
    "16. [Feature Pipeline](#16)\n",
    "17. [Compute All Features](#17)\n",
    "18. [Feature Visualization](#18)\n",
    "19. [Signal Construction](#19)\n",
    "20. [Optuna Walk-Forward Optimization](#20)\n",
    "21. [Option Execution Mathematics](#21)\n",
    "22. [Option Executor](#22)\n",
    "23. [Run Full Backtest](#23)\n",
    "24. [Equity Curves](#24)\n",
    "25. [Trade-Level Analysis](#25)\n",
    "26. [Validation Gates](#26)\n",
    "27. [Validation Implementation](#27)\n",
    "28. [Feature Importance](#28)\n",
    "29. [Summary & References](#29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca8ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 1: Imports & Data Connection \u2014 zero QuantLaxmi imports.\"\"\"\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import math, time, json, os\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import date, timedelta\n",
    "from functools import lru_cache\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.optimize import linprog\n",
    "from scipy.special import gamma as gamma_fn\n",
    "from scipy.stats import spearmanr, f as f_dist\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    HAS_GPU = torch.cuda.is_available()\n",
    "    if HAS_GPU:\n",
    "        DEVICE = torch.device(\"cuda\")\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError:\n",
    "    HAS_GPU = False\n",
    "\n",
    "try:\n",
    "    import optuna\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "    HAS_OPTUNA = True\n",
    "except ImportError:\n",
    "    HAS_OPTUNA = False\n",
    "    print(\"Optuna not available \u2014 walk-forward disabled\")\n",
    "\n",
    "# \u2500\u2500 DuckDB connection to Hive-partitioned Parquet \u2500\u2500\n",
    "DATA_ROOT = \"/home/ubuntu/Desktop/7hills/QuantLaxmi/data/market\"\n",
    "con = duckdb.connect()\n",
    "for cat in [\"nfo_1min\", \"ticks\", \"instruments\"]:\n",
    "    cat_dir = os.path.join(DATA_ROOT, cat)\n",
    "    if os.path.isdir(cat_dir):\n",
    "        con.execute(\n",
    "            f\"CREATE VIEW {cat} AS SELECT * FROM read_parquet(\"\n",
    "            f\"'{cat_dir}/*/*.parquet', hive_partitioning=true, union_by_name=true)\"\n",
    "        )\n",
    "# nse_index_close needs union_by_name for schema drift\n",
    "cat_dir = os.path.join(DATA_ROOT, \"nse_index_close\")\n",
    "if os.path.isdir(cat_dir):\n",
    "    con.execute(\n",
    "        f\"CREATE VIEW nse_index_close AS SELECT * FROM read_parquet(\"\n",
    "        f\"'{cat_dir}/*/*.parquet', hive_partitioning=true, \"\n",
    "        f\"hive_types_autocast=false, union_by_name=true)\"\n",
    "    )\n",
    "\n",
    "# \u2500\u2500 Configuration \u2500\u2500\n",
    "SYMBOLS = [\"NIFTY\", \"BANKNIFTY\"]\n",
    "INDEX_NAME_MAP = {\"NIFTY\": \"Nifty 50\", \"BANKNIFTY\": \"Nifty Bank\"}\n",
    "STRIKE_STEP = {\"NIFTY\": 50, \"BANKNIFTY\": 100}\n",
    "COST_BPS_FUTURES = 5.0\n",
    "COST_BPS_OPTIONS = 10.0\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"DuckDB connected. DATA_ROOT={DATA_ROOT}\")\n",
    "print(f\"Symbols: {SYMBOLS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf51d0a",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "## 2. Data Exploration\n",
    "\n",
    "Query data availability across nfo_1min, ticks, and nse_index_close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae8f29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 2: Data exploration \u2014 schema, date ranges, instrument counts.\"\"\"\n",
    "print(\"=\" * 70)\n",
    "print(\"DATA AVAILABILITY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# nfo_1min summary\n",
    "df_summary = con.execute(\"\"\"\n",
    "    SELECT MIN(date) AS first_date, MAX(date) AS last_date,\n",
    "           COUNT(DISTINCT date) AS n_days,\n",
    "           COUNT(DISTINCT name) AS n_names\n",
    "    FROM nfo_1min\n",
    "\"\"\").fetchdf()\n",
    "print(f\"\\nnfo_1min: {df_summary['n_days'].iloc[0]} days \"\n",
    "      f\"({df_summary['first_date'].iloc[0]} \u2192 {df_summary['last_date'].iloc[0]})\")\n",
    "\n",
    "# Instrument types\n",
    "print(\"\\nInstrument types:\")\n",
    "print(con.execute(\"\"\"\n",
    "    SELECT instrument_type, COUNT(*) AS rows\n",
    "    FROM nfo_1min GROUP BY instrument_type ORDER BY rows DESC\n",
    "\"\"\").fetchdf().to_string(index=False))\n",
    "\n",
    "# Per-symbol FUT bar counts\n",
    "print(\"\\nFUT bars per symbol:\")\n",
    "print(con.execute(\"\"\"\n",
    "    SELECT name, COUNT(DISTINCT date) AS days, COUNT(*) AS bars\n",
    "    FROM nfo_1min WHERE instrument_type = 'FUT'\n",
    "    GROUP BY name ORDER BY days DESC LIMIT 10\n",
    "\"\"\").fetchdf().to_string(index=False))\n",
    "\n",
    "# CE/PE availability for NIFTY\n",
    "print(\"\\nNIFTY CE/PE daily strike counts (sample):\")\n",
    "print(con.execute(\"\"\"\n",
    "    SELECT date, instrument_type, COUNT(DISTINCT strike) AS n_strikes\n",
    "    FROM nfo_1min\n",
    "    WHERE name = 'NIFTY' AND instrument_type IN ('CE', 'PE')\n",
    "    GROUP BY date, instrument_type\n",
    "    ORDER BY date DESC LIMIT 10\n",
    "\"\"\").fetchdf().to_string(index=False))\n",
    "\n",
    "# nse_index_close\n",
    "idx_summary = con.execute(\"\"\"\n",
    "    SELECT COUNT(DISTINCT date) AS n_days,\n",
    "           COUNT(DISTINCT \"Index Name\") AS n_indices\n",
    "    FROM nse_index_close\n",
    "\"\"\").fetchdf()\n",
    "print(f\"\\nnse_index_close: {idx_summary['n_days'].iloc[0]} days, \"\n",
    "      f\"{idx_summary['n_indices'].iloc[0]} indices\")\n",
    "\n",
    "# Available dates for our symbols\n",
    "all_dates = {}\n",
    "for sym in SYMBOLS:\n",
    "    dates_df = con.execute(\n",
    "        \"SELECT DISTINCT date FROM nfo_1min \"\n",
    "        \"WHERE name = ? AND instrument_type = 'FUT' ORDER BY date\",\n",
    "        [sym]\n",
    "    ).fetchdf()\n",
    "    all_dates[sym] = sorted(dates_df[\"date\"].tolist())\n",
    "    print(f\"\\n{sym} FUT: {len(all_dates[sym])} days\")\n",
    "\n",
    "# Use intersection of dates across symbols\n",
    "common_dates = sorted(set(all_dates[SYMBOLS[0]]) & set(all_dates[SYMBOLS[1]]))\n",
    "print(f\"\\nCommon trading dates: {len(common_dates)} \"\n",
    "      f\"({common_dates[0]} \u2192 {common_dates[-1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf2d1d7",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "## 3. SANOS Mathematics\n",
    "\n",
    "**SANOS** (Smooth strictly Arbitrage-free Non-parametric Option Surfaces) from Buehler et al. (2026).\n",
    "\n",
    "Option prices are convex combinations of Black-Scholes calls anchored at model strikes,\n",
    "with martingale density weights found via linear programming.\n",
    "\n",
    "### Black-Scholes Call (normalised)\n",
    "$$C(K, v) = \\Phi(d_1) - K \\cdot \\Phi(d_2), \\quad d_{1,2} = \\frac{-\\ln K \\pm v/2}{\\sqrt{v}}$$\n",
    "where $v = \\sigma^2 T$ (total variance), $K$ is normalised strike $K/F$.\n",
    "\n",
    "### LP Formulation\n",
    "Decision variables: discrete densities $q_j \\in \\mathbb{R}^N$ for each expiry $j$.\n",
    "\n",
    "**Objective**: minimise weighted L1 fitting error\n",
    "$$\\min \\sum_j \\sum_\\ell w_{j\\ell} |C^{\\text{model}}_{j\\ell} - C^{\\text{market}}_{j\\ell}|$$\n",
    "\n",
    "**Constraints**:\n",
    "1. **Normalisation**: $\\mathbf{1}^\\top q_j = 1$ (sum to 1 \u2014 discrete probabilities)\n",
    "2. **Martingale**: $K^\\top q_j = 1$ (unit forward \u2014 no-arbitrage)\n",
    "3. **Calendar**: $U_j q_j \\geq R_j q_{j-1}$ (option prices increase with maturity)\n",
    "4. **Non-negativity**: $q_j \\geq 0$\n",
    "\n",
    "### ATM Variance (Brenner-Subrahmanyam)\n",
    "$$\\sigma^2 T \\approx \\left(\\frac{\\text{Straddle}}{2F} \\cdot \\sqrt{2\\pi}\\right)^2$$\n",
    "\n",
    "### Density Extraction (Breeden-Litzenberger)\n",
    "$$q(K) = e^{rT} \\frac{\\partial^2 C}{\\partial K^2}$$\n",
    "Moments: $\\mu, \\sigma^2, \\text{skew}, \\text{kurt}$ from discrete density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca44fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 4: SANOS implementation \u2014 LP solver from first principles.\"\"\"\n",
    "\n",
    "def bs_call(K: np.ndarray, k_strike: float, v: float) -> np.ndarray:\n",
    "    \"\"\"Black-Scholes call price in normalised coordinates (F=1).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    K : model strike grid (normalised by forward)\n",
    "    k_strike : market strike (normalised)\n",
    "    v : \u03b7 \u00d7 \u03c3\u00b2T (smoothed total variance)\n",
    "\n",
    "    Returns C(K_i, k_strike, v) for each model strike K_i.\n",
    "    \"\"\"\n",
    "    sqrt_v = np.sqrt(np.maximum(v, 1e-12))\n",
    "    log_ratio = np.log(np.maximum(K / k_strike, 1e-30))\n",
    "    d1 = (log_ratio + 0.5 * v) / sqrt_v\n",
    "    d2 = d1 - sqrt_v\n",
    "    from scipy.stats import norm\n",
    "    return K * norm.cdf(d1) - k_strike * norm.cdf(d2)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SANOSResult:\n",
    "    \"\"\"Result of SANOS calibration.\"\"\"\n",
    "    densities: list          # q_j arrays (discrete weights, sum to 1)\n",
    "    model_strikes: np.ndarray\n",
    "    variances: np.ndarray    # ATM variances per expiry\n",
    "    eta: float\n",
    "    expiry_labels: list\n",
    "    market_strikes: list\n",
    "    market_mids: list\n",
    "    fit_errors: list\n",
    "    max_fit_error: float\n",
    "    lp_success: bool\n",
    "\n",
    "    def density(self, expiry_idx: int, K_eval: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Interpolate density onto evaluation grid.\"\"\"\n",
    "        return np.interp(K_eval, self.model_strikes, self.densities[expiry_idx])\n",
    "\n",
    "\n",
    "def fit_sanos(\n",
    "    market_strikes: list[np.ndarray],\n",
    "    market_calls: list[np.ndarray],\n",
    "    atm_variances: np.ndarray | None = None,\n",
    "    market_spreads: list | None = None,\n",
    "    eta: float = 0.50,\n",
    "    K_min: float = 0.70,\n",
    "    K_max: float = 1.35,\n",
    "    n_model_strikes: int = 80,\n",
    "    expiry_labels: list | None = None,\n",
    ") -> SANOSResult:\n",
    "    \"\"\"Fit SANOS surface via LP. Reimplemented from core/pricing/sanos.py.\"\"\"\n",
    "    M = len(market_strikes)\n",
    "    if expiry_labels is None:\n",
    "        expiry_labels = [f\"T{j}\" for j in range(M)]\n",
    "\n",
    "    K_model = np.linspace(K_min, K_max, n_model_strikes)\n",
    "    N = len(K_model)\n",
    "\n",
    "    # Estimate ATM variances if not provided\n",
    "    if atm_variances is None:\n",
    "        atm_variances = np.zeros(M)\n",
    "        for j in range(M):\n",
    "            atm_idx = np.argmin(np.abs(market_strikes[j] - 1.0))\n",
    "            c_atm = market_calls[j][atm_idx]\n",
    "            atm_variances[j] = max((c_atm * math.sqrt(2.0 * math.pi)) ** 2, 1e-6)\n",
    "\n",
    "    # Ensure variances increasing (no calendar arb)\n",
    "    for j in range(1, M):\n",
    "        atm_variances[j] = max(atm_variances[j], atm_variances[j - 1] + 1e-8)\n",
    "\n",
    "    # Build LP\n",
    "    total_market = sum(len(mk) for mk in market_strikes)\n",
    "    total_vars = M * N\n",
    "    n_extended = total_vars + 2 * total_market\n",
    "\n",
    "    c_obj = np.zeros(n_extended)\n",
    "    slack_offset = total_vars\n",
    "    for j in range(M):\n",
    "        n_j = len(market_strikes[j])\n",
    "        offset = sum(len(market_strikes[jj]) for jj in range(j))\n",
    "        for l in range(n_j):\n",
    "            w = 1.0\n",
    "            if market_spreads and market_spreads[j] is not None:\n",
    "                spread = max(market_spreads[j][l], 1e-8)\n",
    "                w = 1.0 / spread\n",
    "            c_obj[slack_offset + offset + l] = w\n",
    "            c_obj[slack_offset + total_market + offset + l] = w\n",
    "\n",
    "    A_eq_rows, b_eq_rows = [], []\n",
    "    A_ub_rows, b_ub_rows = [], []\n",
    "\n",
    "    for j in range(M):\n",
    "        q_start = j * N\n",
    "        v_j = eta * atm_variances[j]\n",
    "\n",
    "        # Sum to 1\n",
    "        row = np.zeros(n_extended); row[q_start:q_start + N] = 1.0\n",
    "        A_eq_rows.append(row); b_eq_rows.append(1.0)\n",
    "\n",
    "        # Martingale: K' \u00b7 q = 1\n",
    "        row = np.zeros(n_extended); row[q_start:q_start + N] = K_model\n",
    "        A_eq_rows.append(row); b_eq_rows.append(1.0)\n",
    "\n",
    "        # Fitting: model_price - s+ + s- = market_mid\n",
    "        n_j = len(market_strikes[j])\n",
    "        offset = sum(len(market_strikes[jj]) for jj in range(j))\n",
    "        for l in range(n_j):\n",
    "            row = np.zeros(n_extended)\n",
    "            k_mkt = market_strikes[j][l]\n",
    "            row[q_start:q_start + N] = bs_call(K_model, k_mkt, v_j)\n",
    "            row[slack_offset + offset + l] = -1.0\n",
    "            row[slack_offset + total_market + offset + l] = 1.0\n",
    "            A_eq_rows.append(row); b_eq_rows.append(market_calls[j][l])\n",
    "\n",
    "    # Calendar arbitrage constraints\n",
    "    for j in range(1, M):\n",
    "        v_j = eta * atm_variances[j]\n",
    "        v_prev = eta * atm_variances[j - 1]\n",
    "        q_start_j = j * N\n",
    "        q_start_prev = (j - 1) * N\n",
    "        for l in range(N):\n",
    "            row = np.zeros(n_extended)\n",
    "            k_l = K_model[l]\n",
    "            row[q_start_prev:q_start_prev + N] = bs_call(K_model, k_l, v_prev)\n",
    "            row[q_start_j:q_start_j + N] = -bs_call(K_model, k_l, v_j)\n",
    "            A_ub_rows.append(row); b_ub_rows.append(0.0)\n",
    "\n",
    "    bounds = [(0, None)] * n_extended\n",
    "    A_eq = np.array(A_eq_rows) if A_eq_rows else None\n",
    "    b_eq = np.array(b_eq_rows) if b_eq_rows else None\n",
    "    A_ub = np.array(A_ub_rows) if A_ub_rows else None\n",
    "    b_ub = np.array(b_ub_rows) if b_ub_rows else None\n",
    "\n",
    "    result = linprog(c=c_obj, A_ub=A_ub, b_ub=b_ub, A_eq=A_eq, b_eq=b_eq,\n",
    "                     bounds=bounds, method=\"highs\",\n",
    "                     options={\"presolve\": True, \"time_limit\": 60,\n",
    "                              \"dual_feasibility_tolerance\": 1e-10,\n",
    "                              \"primal_feasibility_tolerance\": 1e-10})\n",
    "\n",
    "    densities, fit_errors_list = [], []\n",
    "    max_error = 0.0\n",
    "\n",
    "    if result.success:\n",
    "        x = result.x\n",
    "        for j in range(M):\n",
    "            densities.append(x[j * N:(j + 1) * N])\n",
    "    else:\n",
    "        for j in range(M):\n",
    "            q_j = np.zeros(N); q_j[N // 2] = 1.0\n",
    "            densities.append(q_j)\n",
    "\n",
    "    for j in range(M):\n",
    "        v_j = eta * atm_variances[j]\n",
    "        n_j = len(market_strikes[j])\n",
    "        model_prices = np.array([\n",
    "            np.dot(densities[j], bs_call(K_model, market_strikes[j][l], v_j))\n",
    "            for l in range(n_j)\n",
    "        ])\n",
    "        errors = model_prices - market_calls[j]\n",
    "        fit_errors_list.append(errors)\n",
    "        max_error = max(max_error, np.max(np.abs(errors)))\n",
    "\n",
    "    return SANOSResult(\n",
    "        densities=densities, model_strikes=K_model, variances=atm_variances,\n",
    "        eta=eta, expiry_labels=expiry_labels, market_strikes=market_strikes,\n",
    "        market_mids=market_calls, fit_errors=fit_errors_list,\n",
    "        max_fit_error=max_error, lp_success=result.success,\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_density(result: SANOSResult, expiry_idx: int = 0,\n",
    "                    n_points: int = 500, K_lo: float = 0.80, K_hi: float = 1.25):\n",
    "    \"\"\"Extract risk-neutral density from SANOS surface.\"\"\"\n",
    "    K = np.linspace(K_lo, K_hi, n_points)\n",
    "    q = result.density(expiry_idx, K)\n",
    "    q = np.maximum(q, 0.0)\n",
    "    dK = K[1] - K[0]\n",
    "    total = np.sum(q) * dK\n",
    "    if total > 1e-12:\n",
    "        q /= total\n",
    "    return K, q\n",
    "\n",
    "\n",
    "def compute_moments(K, q):\n",
    "    \"\"\"Mean, variance, skewness, excess kurtosis from density.\"\"\"\n",
    "    dK = K[1] - K[0]\n",
    "    mu = float(np.sum(K * q) * dK)\n",
    "    var = float(np.sum((K - mu) ** 2 * q) * dK)\n",
    "    if var < 1e-14:\n",
    "        return mu, var, 0.0, 0.0\n",
    "    std = math.sqrt(var)\n",
    "    z = (K - mu) / std\n",
    "    skew = float(np.sum(z ** 3 * q) * dK)\n",
    "    kurt = float(np.sum(z ** 4 * q) * dK) - 3.0\n",
    "    return mu, var, skew, kurt\n",
    "\n",
    "\n",
    "def shannon_entropy(q, dK):\n",
    "    \"\"\"Shannon entropy H = -\u222b q ln q dK.\"\"\"\n",
    "    mask = q > 1e-15\n",
    "    if not np.any(mask):\n",
    "        return 0.0\n",
    "    return float(-np.sum(q[mask] * np.log(q[mask])) * dK)\n",
    "\n",
    "\n",
    "def kl_divergence(q_new, q_old, dK):\n",
    "    \"\"\"KL divergence D_KL(q_new \u2016 q_old).\"\"\"\n",
    "    eps = 1e-15\n",
    "    qn = np.maximum(q_new, eps)\n",
    "    qo = np.maximum(q_old, eps)\n",
    "    return float(np.sum(qn * np.log(qn / qo)) * dK)\n",
    "\n",
    "\n",
    "def tail_weights(K, q, mu, std):\n",
    "    \"\"\"P(K < \u03bc-\u03c3), P(K > \u03bc+\u03c3).\"\"\"\n",
    "    dK = K[1] - K[0]\n",
    "    left = float(np.sum(q[K < mu - std]) * dK)\n",
    "    right = float(np.sum(q[K > mu + std]) * dK)\n",
    "    return left, right\n",
    "\n",
    "print(f\"SANOS solver ready (linprog/highs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc09d48",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "## 5. Option Chain from 1-Min Bars\n",
    "\n",
    "**Key innovation**: Build SANOS-ready option chain from nfo_1min EOD snapshots\n",
    "instead of nse_fo_bhavcopy. This extends SANOS coverage from ~125 days to 316 days.\n",
    "\n",
    "Steps per date:\n",
    "1. Query last close per (strike, instrument_type, expiry) from nfo_1min\n",
    "2. Pivot CE/PE \u2192 compute implied forward via put-call parity\n",
    "3. Select OTM options, normalise by forward\n",
    "4. Estimate ATM variance from straddle (Brenner-Subrahmanyam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e02a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 5: Option chain builder from nfo_1min.\"\"\"\n",
    "\n",
    "def prepare_chain_from_1min(con, symbol: str, d: str, max_expiries: int = 2):\n",
    "    \"\"\"Build SANOS-ready option chain from nfo_1min last-bar snapshot.\n",
    "\n",
    "    Returns dict matching fit_sanos() input format, or None if insufficient data.\n",
    "    \"\"\"\n",
    "    # EOD snapshot: last close per (strike, instrument_type, expiry)\n",
    "    df = con.execute(\"\"\"\n",
    "        SELECT strike, instrument_type, expiry,\n",
    "               LAST(close) AS close,\n",
    "               LAST(oi) AS oi\n",
    "        FROM nfo_1min\n",
    "        WHERE date = ? AND name = ? AND instrument_type IN ('CE', 'PE')\n",
    "        GROUP BY strike, instrument_type, expiry\n",
    "        ORDER BY expiry, strike\n",
    "    \"\"\", [d, symbol]).fetchdf()\n",
    "\n",
    "    if df is None or len(df) < 20:\n",
    "        return None\n",
    "\n",
    "    # Get spot from index close (or approximate from futures)\n",
    "    try:\n",
    "        idx_name = INDEX_NAME_MAP.get(symbol, symbol)\n",
    "        spot_df = con.execute(\n",
    "            'SELECT \"Closing Index Value\" AS spot FROM nse_index_close '\n",
    "            'WHERE \"Index Name\" = ? AND date = ?', [idx_name, d]\n",
    "        ).fetchdf()\n",
    "        if not spot_df.empty and spot_df[\"spot\"].iloc[0] is not None:\n",
    "            spot = float(spot_df[\"spot\"].iloc[0])\n",
    "        else:\n",
    "            # Fallback: use FUT close\n",
    "            fut_df = con.execute(\n",
    "                \"SELECT LAST(close) AS c FROM nfo_1min \"\n",
    "                \"WHERE name = ? AND date = ? AND instrument_type = 'FUT' \"\n",
    "                \"AND expiry = (SELECT MIN(expiry) FROM nfo_1min \"\n",
    "                \"WHERE name = ? AND date = ? AND instrument_type = 'FUT')\",\n",
    "                [symbol, d, symbol, d]\n",
    "            ).fetchdf()\n",
    "            if fut_df.empty:\n",
    "                return None\n",
    "            spot = float(fut_df[\"c\"].iloc[0])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    # Select up to max_expiries\n",
    "    expiries = sorted(df[\"expiry\"].unique())[:max_expiries]\n",
    "\n",
    "    market_strikes_list = []\n",
    "    market_calls_list = []\n",
    "    atm_vars = []\n",
    "    expiry_labels = []\n",
    "\n",
    "    for exp in expiries:\n",
    "        sub = df[df[\"expiry\"] == exp].copy()\n",
    "        calls = sub[sub[\"instrument_type\"] == \"CE\"].set_index(\"strike\")[\"close\"]\n",
    "        puts = sub[sub[\"instrument_type\"] == \"PE\"].set_index(\"strike\")[\"close\"]\n",
    "\n",
    "        common = sorted(set(calls.index) & set(puts.index))\n",
    "        if len(common) < 10:\n",
    "            continue\n",
    "\n",
    "        # Implied forward from put-call parity: F = K + C - P\n",
    "        best_F, best_diff = spot, float(\"inf\")\n",
    "        for K in common:\n",
    "            C, P = float(calls[K]), float(puts[K])\n",
    "            if C <= 0 or P <= 0:\n",
    "                continue\n",
    "            F = K + C - P\n",
    "            diff = abs(C - P)\n",
    "            if diff < best_diff:\n",
    "                best_diff = diff\n",
    "                best_F = F\n",
    "        forward = best_F\n",
    "\n",
    "        # OTM options only: calls K >= F, puts K <= F \u2192 convert to call prices\n",
    "        rows = []\n",
    "        for K in common:\n",
    "            C_val = float(calls[K])\n",
    "            P_val = float(puts[K])\n",
    "            k_norm = K / forward\n",
    "            if K >= forward:\n",
    "                # OTM call\n",
    "                c_norm = C_val / forward\n",
    "                if c_norm > 0.001:\n",
    "                    rows.append((k_norm, c_norm))\n",
    "            else:\n",
    "                # OTM put \u2192 synthetic call: C = P + 1 - K/F\n",
    "                c_norm = P_val / forward + 1.0 - k_norm\n",
    "                if c_norm > 0.001:\n",
    "                    rows.append((k_norm, c_norm))\n",
    "\n",
    "        if len(rows) < 5:\n",
    "            continue\n",
    "\n",
    "        rows.sort(key=lambda x: x[0])\n",
    "        k_arr = np.array([r[0] for r in rows])\n",
    "        c_arr = np.array([r[1] for r in rows])\n",
    "\n",
    "        # ATM variance from straddle (Brenner-Subrahmanyam)\n",
    "        atm_K = min(common, key=lambda K: abs(K - forward))\n",
    "        straddle = float(calls[atm_K]) + float(puts[atm_K])\n",
    "        straddle_norm = straddle / forward\n",
    "        atm_var = max((straddle_norm / 2.0 * math.sqrt(2.0 * math.pi)) ** 2, 1e-6)\n",
    "\n",
    "        market_strikes_list.append(k_arr)\n",
    "        market_calls_list.append(c_arr)\n",
    "        atm_vars.append(atm_var)\n",
    "        expiry_labels.append(str(exp))\n",
    "\n",
    "    if not market_strikes_list:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"market_strikes\": market_strikes_list,\n",
    "        \"market_calls\": market_calls_list,\n",
    "        \"atm_variances\": np.array(atm_vars),\n",
    "        \"expiry_labels\": expiry_labels,\n",
    "        \"forward\": forward,\n",
    "        \"spot\": spot,\n",
    "    }\n",
    "\n",
    "print(\"Option chain builder ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afdc422",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "## 6. SANOS Calibration (316 Days)\n",
    "\n",
    "Loop over all trading dates, build option chains from nfo_1min, calibrate SANOS.\n",
    "Cache results: skew, left_tail, entropy, KL divergence per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9918ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 6: SANOS calibration over all trading dates.\"\"\"\n",
    "\n",
    "def calibrate_sanos_all_dates(con, symbol, dates):\n",
    "    \"\"\"Calibrate SANOS for each date, return cache dict.\"\"\"\n",
    "    cache = {}\n",
    "    prev_density = None\n",
    "    prev_K = None\n",
    "    ok_count = 0\n",
    "    fail_count = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for i, d in enumerate(dates):\n",
    "        chain = prepare_chain_from_1min(con, symbol, d, max_expiries=2)\n",
    "        if chain is None:\n",
    "            fail_count += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            result = fit_sanos(\n",
    "                market_strikes=chain[\"market_strikes\"],\n",
    "                market_calls=chain[\"market_calls\"],\n",
    "                atm_variances=chain[\"atm_variances\"],\n",
    "                expiry_labels=chain[\"expiry_labels\"],\n",
    "                eta=0.50, n_model_strikes=80,\n",
    "            )\n",
    "        except Exception:\n",
    "            fail_count += 1\n",
    "            continue\n",
    "\n",
    "        if not result.lp_success:\n",
    "            fail_count += 1\n",
    "            continue\n",
    "\n",
    "        K, q = extract_density(result, 0)\n",
    "        dK = K[1] - K[0]\n",
    "        mu, var, skew, kurt = compute_moments(K, q)\n",
    "        std = math.sqrt(max(var, 1e-14))\n",
    "        H = shannon_entropy(q, dK)\n",
    "        lt, rt = tail_weights(K, q, mu, std)\n",
    "\n",
    "        # KL divergence from previous day\n",
    "        kl = 0.0\n",
    "        if prev_density is not None and prev_K is not None:\n",
    "            # Interpolate prev density onto current grid\n",
    "            prev_q_interp = np.interp(K, prev_K, prev_density)\n",
    "            prev_q_interp = np.maximum(prev_q_interp, 0)\n",
    "            total_prev = np.sum(prev_q_interp) * dK\n",
    "            if total_prev > 1e-12:\n",
    "                prev_q_interp /= total_prev\n",
    "            kl = kl_divergence(q, prev_q_interp, dK)\n",
    "\n",
    "        density_ok = var > 1e-8 and (np.sum(q) * dK) > 0.90\n",
    "\n",
    "        cache[d] = {\n",
    "            \"skew\": skew, \"kurtosis\": kurt, \"left_tail\": lt, \"right_tail\": rt,\n",
    "            \"entropy\": H, \"kl\": kl, \"density_ok\": density_ok,\n",
    "            \"forward\": chain[\"forward\"], \"spot\": chain.get(\"spot\", 0),\n",
    "            \"max_fit_error\": result.max_fit_error,\n",
    "        }\n",
    "\n",
    "        prev_density = q\n",
    "        prev_K = K\n",
    "        ok_count += 1\n",
    "\n",
    "        if (i + 1) % 50 == 0:\n",
    "            elapsed = time.time() - t0\n",
    "            print(f\"  {symbol} {i+1}/{len(dates)} ({elapsed:.1f}s) \u2014 \"\n",
    "                  f\"{ok_count} ok, {fail_count} fail\")\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"  {symbol} DONE: {ok_count}/{len(dates)} calibrated in {elapsed:.1f}s\")\n",
    "    return cache\n",
    "\n",
    "\n",
    "# Run calibration for both symbols\n",
    "sanos_cache = {}\n",
    "for sym in SYMBOLS:\n",
    "    dates_for_sym = con.execute(\n",
    "        \"SELECT DISTINCT CAST(date AS VARCHAR) AS date FROM nfo_1min \"\n",
    "        \"WHERE name = ? AND instrument_type = 'CE' ORDER BY date\", [sym]\n",
    "    ).fetchdf()[\"date\"].tolist()\n",
    "    print(f\"\\nCalibrating SANOS for {sym} ({len(dates_for_sym)} dates)...\")\n",
    "    sanos_cache[sym] = calibrate_sanos_all_dates(con, sym, dates_for_sym)\n",
    "    print(f\"  Cached {len(sanos_cache[sym])} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74f009b",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "## 7. SANOS Visualization\n",
    "\n",
    "Density plots, skew/tail/entropy time series, IV smile comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb00d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 7: SANOS visualisation.\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for idx, sym in enumerate(SYMBOLS):\n",
    "    cache = sanos_cache[sym]\n",
    "    if not cache:\n",
    "        continue\n",
    "    dates_sorted = sorted(cache.keys())\n",
    "    skews = [cache[d][\"skew\"] for d in dates_sorted]\n",
    "    lefts = [cache[d][\"left_tail\"] for d in dates_sorted]\n",
    "    entropies = [cache[d][\"entropy\"] for d in dates_sorted]\n",
    "    kls = [cache[d][\"kl\"] for d in dates_sorted]\n",
    "\n",
    "    # Parse dates for x-axis\n",
    "    from datetime import datetime\n",
    "    date_objs = [datetime.strptime(d, \"%Y-%m-%d\") if isinstance(d, str) else d\n",
    "                 for d in dates_sorted]\n",
    "\n",
    "    ax = axes[0, idx]\n",
    "    ax.plot(date_objs, skews, linewidth=0.8, color=\"steelblue\")\n",
    "    ax.axhline(0, color=\"gray\", linewidth=0.5, linestyle=\"--\")\n",
    "    ax.set_title(f\"{sym} \u2014 Risk-Neutral Skewness\")\n",
    "    ax.set_ylabel(\"Skew\")\n",
    "    ax.tick_params(axis=\"x\", rotation=30)\n",
    "\n",
    "    ax = axes[1, idx]\n",
    "    ax.plot(date_objs, lefts, linewidth=0.8, color=\"crimson\", label=\"Left tail\")\n",
    "    ax.plot(date_objs, entropies, linewidth=0.8, color=\"teal\", label=\"Entropy\")\n",
    "    ax.set_title(f\"{sym} \u2014 Left Tail & Entropy\")\n",
    "    ax.set_ylabel(\"Value\")\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.tick_params(axis=\"x\", rotation=30)\n",
    "\n",
    "plt.suptitle(\"SANOS Density Features (from nfo_1min, 316 days)\", fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary stats\n",
    "for sym in SYMBOLS:\n",
    "    cache = sanos_cache[sym]\n",
    "    n_ok = sum(1 for v in cache.values() if v[\"density_ok\"])\n",
    "    skews = [v[\"skew\"] for v in cache.values() if v[\"density_ok\"]]\n",
    "    print(f\"\\n{sym}: {n_ok}/{len(cache)} good densities\")\n",
    "    if skews:\n",
    "        print(f\"  Skew: mean={np.mean(skews):.4f}, std={np.std(skews):.4f}\")\n",
    "        print(f\"  Max fit error (median): {np.median([v['max_fit_error'] for v in cache.values()]):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba58ea47",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "## 8. Fractional Calculus Mathematics\n",
    "\n",
    "### Mittag-Leffler Function\n",
    "$$E_{\\alpha,\\beta}(z) = \\sum_{k=0}^{\\infty} \\frac{z^k}{\\Gamma(\\alpha k + \\beta)}$$\n",
    "\n",
    "Special cases: $E_{1,1}(z) = e^z$, $E_{2,1}(-z^2) = \\cos(z)$.\n",
    "\n",
    "### Mean Square Displacement (MSD) Scaling\n",
    "$$\\text{MSD}(\\tau) = E[X^2(\\tau)] \\sim \\tau^{2H}$$\n",
    "- $H < 0.5$: subdiffusive (mean-reverting)\n",
    "- $H = 0.5$: normal (Brownian)\n",
    "- $H > 0.5$: superdiffusive (trending)\n",
    "\n",
    "Fractional order: $\\alpha = 2H$.\n",
    "\n",
    "### Fractional Differencing (Hosking 1981)\n",
    "Binomial expansion weights:\n",
    "$$w_0 = 1, \\quad w_k = -w_{k-1} \\frac{d - k + 1}{k}$$\n",
    "\n",
    "Optimal $d = 0.226$ preserves long memory while achieving stationarity.\n",
    "\n",
    "### Fractional Fokker-Planck Equation (FFPE)\n",
    "$$\\partial_t^\\alpha p = -\\partial_x(f \\cdot p) + D \\, \\partial_x^2 p$$\n",
    "\n",
    "Time-stepping via Mittag-Leffler matrix exponential:\n",
    "$$p_{n+1} = E_\\alpha(-A \\cdot \\Delta t^\\alpha) \\cdot p_n$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 9: Fractional calculus features \u2014 from first principles.\"\"\"\n",
    "\n",
    "def mittag_leffler(z, alpha, beta=1.0, n_terms=100):\n",
    "    \"\"\"Generalised Mittag-Leffler function E_{\u03b1,\u03b2}(z).\"\"\"\n",
    "    scalar = np.isscalar(z)\n",
    "    z = np.atleast_1d(np.asarray(z, dtype=np.float64))\n",
    "    result = np.zeros_like(z, dtype=np.float64)\n",
    "    for k in range(n_terms):\n",
    "        denom = gamma_fn(alpha * k + beta)\n",
    "        if denom == 0 or np.isinf(denom):\n",
    "            break\n",
    "        term = z**k / denom\n",
    "        result += term\n",
    "        if np.all(np.abs(term) < 1e-15):\n",
    "            break\n",
    "    return float(result[0]) if scalar else result\n",
    "\n",
    "\n",
    "def estimate_alpha_msd(returns, max_lag=50):\n",
    "    \"\"\"Estimate \u03b1 via MSD scaling: MSD(\u03c4) ~ \u03c4^{2H}, \u03b1 = 2H.\"\"\"\n",
    "    cumret = np.cumsum(returns)\n",
    "    n = len(cumret)\n",
    "    max_lag = min(max_lag, n // 3)\n",
    "    if max_lag < 4:\n",
    "        return 1.0\n",
    "\n",
    "    lags = np.arange(2, max_lag + 1)\n",
    "    msds = np.empty(len(lags))\n",
    "    for i, tau in enumerate(lags):\n",
    "        displacements = cumret[tau:] - cumret[:-tau]\n",
    "        msds[i] = np.mean(displacements**2)\n",
    "\n",
    "    valid = msds > 0\n",
    "    if valid.sum() < 3:\n",
    "        return 1.0\n",
    "    log_lags = np.log(lags[valid])\n",
    "    log_msds = np.log(msds[valid])\n",
    "    slope, _ = np.polyfit(log_lags, log_msds, 1)\n",
    "    hurst = np.clip(slope / 2.0, 0.01, 1.5)\n",
    "    return float(2.0 * hurst)\n",
    "\n",
    "\n",
    "def estimate_alpha_waiting(returns, threshold=0.0):\n",
    "    \"\"\"Estimate \u03b1 from waiting-time distribution (Hill estimator).\"\"\"\n",
    "    if threshold <= 0:\n",
    "        threshold = np.std(returns, ddof=1) * 0.5 if len(returns) > 2 else 1e-8\n",
    "    exceedance_idx = np.where(np.abs(returns) > threshold)[0]\n",
    "    if len(exceedance_idx) < 10:\n",
    "        return 1.0\n",
    "    waiting_times = np.diff(exceedance_idx).astype(float)\n",
    "    waiting_times = waiting_times[waiting_times > 0]\n",
    "    if len(waiting_times) < 5:\n",
    "        return 1.0\n",
    "    sorted_wt = np.sort(waiting_times)[::-1]\n",
    "    k = max(int(len(sorted_wt) * 0.1), 5)\n",
    "    k = min(k, len(sorted_wt) - 1)\n",
    "    log_ratios = np.log(sorted_wt[:k]) - np.log(sorted_wt[k])\n",
    "    if np.sum(log_ratios) == 0:\n",
    "        return 1.0\n",
    "    return float(np.clip(k / np.sum(log_ratios), 0.1, 3.0))\n",
    "\n",
    "\n",
    "def estimate_alpha(returns, max_lag=50):\n",
    "    \"\"\"Consensus \u03b1: 0.6 \u00d7 MSD + 0.4 \u00d7 waiting-time.\"\"\"\n",
    "    return 0.6 * estimate_alpha_msd(returns, max_lag) + 0.4 * estimate_alpha_waiting(returns)\n",
    "\n",
    "\n",
    "def fractional_differentiation(series, d, threshold=1e-5, max_window=None):\n",
    "    \"\"\"Fractionally difference a series of order d (Hosking weights).\"\"\"\n",
    "    n = len(series)\n",
    "    if max_window is None:\n",
    "        max_window = n\n",
    "    weights = [1.0]\n",
    "    k = 1\n",
    "    while True:\n",
    "        w = -weights[-1] * (d - k + 1) / k\n",
    "        if abs(w) < threshold:\n",
    "            break\n",
    "        weights.append(w)\n",
    "        k += 1\n",
    "        if k > max_window:\n",
    "            break\n",
    "    weights = np.array(weights[::-1])\n",
    "    width = len(weights)\n",
    "    result = np.full(n, np.nan)\n",
    "    for i in range(width - 1, n):\n",
    "        result[i] = np.dot(weights, series[i - width + 1: i + 1])\n",
    "    return result\n",
    "\n",
    "\n",
    "def solve_ffpe_1d(alpha, drift, diffusion, x_grid, dt, n_steps):\n",
    "    \"\"\"Solve 1-D fractional Fokker-Planck via Mittag-Leffler time evolution.\"\"\"\n",
    "    nx = len(x_grid)\n",
    "    dx = x_grid[1] - x_grid[0] if nx > 1 else 1.0\n",
    "\n",
    "    A = np.zeros((nx, nx))\n",
    "    for i in range(1, nx - 1):\n",
    "        A[i, i - 1] += diffusion / dx**2\n",
    "        A[i, i]     -= 2 * diffusion / dx**2\n",
    "        A[i, i + 1] += diffusion / dx**2\n",
    "        if drift >= 0:\n",
    "            A[i, i]     -= drift / dx\n",
    "            A[i, i - 1] += drift / dx\n",
    "        else:\n",
    "            A[i, i]     += drift / dx\n",
    "            A[i, i + 1] -= drift / dx\n",
    "\n",
    "    mean_x = 0.5 * (x_grid[0] + x_grid[-1])\n",
    "    sigma0 = 0.1 * (x_grid[-1] - x_grid[0])\n",
    "    p = np.exp(-0.5 * ((x_grid - mean_x) / max(sigma0, 1e-8))**2)\n",
    "    p /= (np.sum(p) * dx + 1e-30)\n",
    "\n",
    "    dt_alpha = dt**alpha\n",
    "    for _ in range(n_steps):\n",
    "        p_new = p.copy()\n",
    "        Ak_p = p.copy()\n",
    "        for k in range(1, 11):\n",
    "            Ak_p = A @ Ak_p * (-dt_alpha)\n",
    "            denom = gamma_fn(alpha * k + 1)\n",
    "            if denom == 0 or np.isinf(denom):\n",
    "                break\n",
    "            p_new += Ak_p / denom\n",
    "        p_new = np.maximum(p_new, 0)\n",
    "        total = np.sum(p_new) * dx\n",
    "        if total > 1e-30:\n",
    "            p_new /= total\n",
    "        p = p_new\n",
    "    return p\n",
    "\n",
    "print(\"Fractional calculus features ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88286207",
   "metadata": {},
   "source": [
    "<a id=\"10\"></a>\n",
    "## 10. Mock Theta Mathematics\n",
    "\n",
    "Ramanujan's third-order mock theta functions detect quasi-periodic structures invisible to FFT:\n",
    "\n",
    "$$f(q) = \\sum_{n=0}^{\\infty} \\frac{q^{n^2}}{\\prod_{k=1}^{n}(1+q^k)^2}, \\quad\n",
    "\\varphi(q) = \\sum_{n=0}^{\\infty} \\frac{q^{n^2}}{\\prod_{k=1}^{n}(1+q^k)}$$\n",
    "\n",
    "### Return-to-q Mapping\n",
    "$$q(r) = \\exp\\left(\\frac{-\\pi}{1 + |r|/\\sigma}\\right)$$\n",
    "\n",
    "Properties: $q \\in (0, e^{-\\pi})$ for $r=0$; $q \\to 1$ as $|r|/\\sigma \\to \\infty$.\n",
    "\n",
    "### Ramanujan Continued-Fraction Volatility Distortion\n",
    "$$R(V) = \\frac{e^{-\\alpha V}}{1 + \\frac{V}{1 + \\frac{2V}{1 + \\frac{3V}{\\cdots}}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37f9c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 11: Mock theta functions \u2014 from first principles.\"\"\"\n",
    "\n",
    "def mock_theta_f(q, n_terms=50):\n",
    "    \"\"\"Third-order mock theta f(q) = \u03a3 q^{n\u00b2} / \u03a0(1+q^k)\u00b2.\"\"\"\n",
    "    if abs(q) >= 1.0:\n",
    "        return 0.0\n",
    "    total = 1.0\n",
    "    prod = 1.0\n",
    "    for n in range(1, n_terms + 1):\n",
    "        prod *= (1.0 + q**n) ** 2\n",
    "        if prod == 0 or abs(prod) < 1e-300:\n",
    "            break\n",
    "        qn2 = q ** (n * n)\n",
    "        if abs(qn2) < 1e-300:\n",
    "            break\n",
    "        total += qn2 / prod\n",
    "    return total\n",
    "\n",
    "\n",
    "def mock_theta_phi(q, n_terms=50):\n",
    "    \"\"\"Third-order mock theta \u03c6(q) = \u03a3 q^{n\u00b2} / \u03a0(1+q^k).\"\"\"\n",
    "    if abs(q) >= 1.0:\n",
    "        return 0.0\n",
    "    total = 1.0\n",
    "    prod = 1.0\n",
    "    for n in range(1, n_terms + 1):\n",
    "        prod *= (1.0 + q**n)\n",
    "        if prod == 0 or abs(prod) < 1e-300:\n",
    "            break\n",
    "        qn2 = q ** (n * n)\n",
    "        if abs(qn2) < 1e-300:\n",
    "            break\n",
    "        total += qn2 / prod\n",
    "    return total\n",
    "\n",
    "\n",
    "def return_to_q(returns, sigma):\n",
    "    \"\"\"Map returns to nome domain: q = exp(-\u03c0 / (1 + |r|/\u03c3)).\"\"\"\n",
    "    sigma = max(sigma, 1e-10)\n",
    "    return np.exp(-math.pi / (1.0 + np.abs(returns) / sigma))\n",
    "\n",
    "\n",
    "def ramanujan_volatility_distortion(vol, alpha=0.2, depth=20):\n",
    "    \"\"\"Continued-fraction volatility transform R(V).\"\"\"\n",
    "    cf = 1.0\n",
    "    for k in range(depth, 0, -1):\n",
    "        cf = 1.0 + k * vol / max(cf, 1e-30)\n",
    "    return math.exp(-alpha * vol) / max(cf, 1e-30)\n",
    "\n",
    "print(\"Mock theta features ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731e9dcc",
   "metadata": {},
   "source": [
    "<a id=\"12\"></a>\n",
    "## 12. Vedic Angular Mathematics\n",
    "\n",
    "### Madhava Angular Kernel (Kerala School, ~1400 CE)\n",
    "$$K_M(x,y) = \\sum_{n=0}^{P} \\frac{(-1)^n \\theta^{2n}}{(2n)!}, \\quad \\theta = \\arccos(\\hat{x} \\cdot \\hat{y})$$\n",
    "\n",
    "For $P \\to \\infty$ converges to $\\cos(\\theta)$, but finite-order captures higher-order curvature.\n",
    "\n",
    "### Aryabhata Sine-Difference Phase (499 CE)\n",
    "$$\\Delta^2 \\sin(nh) \\approx -\\sin(nh) \\cdot h^2$$\n",
    "\n",
    "The oldest known discrete second-order recurrence for trig functions, used to track\n",
    "oscillation phase in detrended prices for entry timing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6226498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 13: Vedic angular features \u2014 Madhava kernel + Aryabhata phase.\"\"\"\n",
    "\n",
    "def madhava_kernel(x, y, order=4):\n",
    "    \"\"\"Madhava angular kernel on unit hypersphere.\"\"\"\n",
    "    x_norm = np.linalg.norm(x)\n",
    "    y_norm = np.linalg.norm(y)\n",
    "    if x_norm < 1e-30 or y_norm < 1e-30:\n",
    "        return 0.0\n",
    "    cos_theta = np.clip(np.dot(x / x_norm, y / y_norm), -1.0, 1.0)\n",
    "    theta = math.acos(cos_theta)\n",
    "    result = 0.0\n",
    "    for n in range(order + 1):\n",
    "        result += ((-1) ** n) * (theta ** (2 * n)) / math.factorial(2 * n)\n",
    "    return float(result)\n",
    "\n",
    "\n",
    "def angular_coherence(features, regime_centroids, order=4):\n",
    "    \"\"\"Madhava kernel coherence to best regime centroid.\"\"\"\n",
    "    if not regime_centroids:\n",
    "        return (\"unknown\", 0.0, 0.0)\n",
    "    best_regime, best_coh, best_resid = \"unknown\", -2.0, 0.0\n",
    "    for regime, centroid in regime_centroids.items():\n",
    "        if len(centroid) != len(features):\n",
    "            continue\n",
    "        k_full = madhava_kernel(features, centroid, order=order)\n",
    "        k_cos = madhava_kernel(features, centroid, order=1)\n",
    "        if k_full > best_coh:\n",
    "            best_coh = k_full\n",
    "            best_regime = regime\n",
    "            best_resid = abs(k_full - k_cos)\n",
    "    return (best_regime, float(best_coh), float(best_resid))\n",
    "\n",
    "\n",
    "def update_regime_centroids(centroids, features, regime, decay=0.99):\n",
    "    \"\"\"EMA update of regime centroid vectors.\"\"\"\n",
    "    if regime not in centroids:\n",
    "        centroids[regime] = features.copy()\n",
    "    else:\n",
    "        centroids[regime] = decay * centroids[regime] + (1 - decay) * features\n",
    "        norm = np.linalg.norm(centroids[regime])\n",
    "        if norm > 1e-30:\n",
    "            centroids[regime] /= norm\n",
    "    return centroids\n",
    "\n",
    "\n",
    "# \u2500\u2500 Ramanujan periodogram (for Aryabhata) \u2500\u2500\n",
    "\n",
    "@lru_cache(maxsize=4096)\n",
    "def _euler_phi(n):\n",
    "    \"\"\"Euler's totient.\"\"\"\n",
    "    result = n; temp = n; p = 2\n",
    "    while p * p <= temp:\n",
    "        if temp % p == 0:\n",
    "            while temp % p == 0: temp //= p\n",
    "            result -= result // p\n",
    "        p += 1\n",
    "    if temp > 1: result -= result // temp\n",
    "    return result\n",
    "\n",
    "@lru_cache(maxsize=4096)\n",
    "def _mobius(n):\n",
    "    \"\"\"Mobius function.\"\"\"\n",
    "    if n == 1: return 1\n",
    "    count = 0; temp = n; p = 2\n",
    "    while p * p <= temp:\n",
    "        if temp % p == 0:\n",
    "            temp //= p; count += 1\n",
    "            if temp % p == 0: return 0\n",
    "        p += 1\n",
    "    if temp > 1: count += 1\n",
    "    return (-1) ** count\n",
    "\n",
    "def ramanujan_sum(q_val, n):\n",
    "    if q_val <= 0: return 0.0\n",
    "    d = math.gcd(abs(n), q_val)\n",
    "    q_over_d = q_val // d\n",
    "    mu_val = _mobius(q_over_d)\n",
    "    if mu_val == 0: return 0.0\n",
    "    return mu_val * _euler_phi(q_val) / max(_euler_phi(q_over_d), 1)\n",
    "\n",
    "def dominant_periods(signal, max_period=64, top_k=3):\n",
    "    \"\"\"Find top-k dominant periods via Ramanujan periodogram.\"\"\"\n",
    "    N = len(signal)\n",
    "    energies = np.zeros(max_period)\n",
    "    for q in range(1, max_period + 1):\n",
    "        cq = np.array([ramanujan_sum(q, n) for n in range(N)], dtype=np.float64)\n",
    "        projection = np.dot(signal, cq)\n",
    "        phi_q = _euler_phi(q)\n",
    "        if phi_q > 0:\n",
    "            energies[q - 1] = (projection**2) / (N * phi_q**2)\n",
    "    energies[0] = 0\n",
    "    indices = np.argsort(energies)[::-1][:top_k]\n",
    "    return [int(i + 1) for i in indices if energies[i] > 0]\n",
    "\n",
    "\n",
    "def aryabhata_phase(prices, period, window_mult=2, min_window=None):\n",
    "    \"\"\"Causal phase estimator at a given period via least-squares sinusoid fit.\n",
    "\n",
    "    Model: y = c + m*t + a*sin(w*t) + b*cos(w*t)\n",
    "    Phase: phi = atan2(b, a), normalised to [0, 1).\n",
    "\n",
    "    Optimisation: only computes the final-index phase (since Cell 17 only uses ph[-1]).\n",
    "    Returns full-length arrays for API compatibility, with phase set only at the last index.\n",
    "\n",
    "    Time t is centered at i (the fit point) for numerical conditioning.\n",
    "\n",
    "    Args:\n",
    "        prices: price series (typically ~60 points from ALPHA_WINDOW)\n",
    "        period: target oscillation period (from Ramanujan periodogram)\n",
    "        window_mult: fit window = window_mult * period (default 2 for ~60-pt series)\n",
    "        min_window: minimum fit window (default: period + 2)\n",
    "\n",
    "    Returns:\n",
    "        phase:     normalised phase in [0, 1) per index (only last index populated)\n",
    "        phase_vel: wrapped phase velocity (only last index populated if possible)\n",
    "    \"\"\"\n",
    "    y = np.asarray(prices, dtype=np.float64)\n",
    "    n = len(y)\n",
    "    phase = np.full(n, np.nan, dtype=np.float64)\n",
    "    phase_vel = np.full(n, np.nan, dtype=np.float64)\n",
    "\n",
    "    if period is None or period < 2 or n < period + 2:\n",
    "        return phase, phase_vel\n",
    "\n",
    "    if min_window is None:\n",
    "        min_window = period + 2\n",
    "    W = int(max(min_window, window_mult * period))\n",
    "    W = min(W, n)\n",
    "\n",
    "    w = 2.0 * math.pi / float(period)\n",
    "\n",
    "    # Only compute phase at the last index (i = n-1)\n",
    "    i = n - 1\n",
    "    start = max(0, i - W + 1)\n",
    "    t_raw = np.arange(start, i + 1, dtype=np.float64)\n",
    "    t = t_raw - float(i)  # center at i for numerical conditioning\n",
    "    yy = y[start:i+1]\n",
    "\n",
    "    if len(yy) < 6:\n",
    "        return phase, phase_vel\n",
    "\n",
    "    S = np.sin(w * t)\n",
    "    C = np.cos(w * t)\n",
    "    X = np.column_stack([np.ones_like(t), t, S, C])\n",
    "\n",
    "    beta, *_ = np.linalg.lstsq(X, yy, rcond=None)\n",
    "    a = beta[2]\n",
    "    b = beta[3]\n",
    "\n",
    "    amp = math.hypot(a, b)\n",
    "    if amp < 1e-12:\n",
    "        return phase, phase_vel\n",
    "\n",
    "    phi = math.atan2(b, a)\n",
    "    phase[i] = (phi % (2.0 * math.pi)) / (2.0 * math.pi)\n",
    "\n",
    "    return phase, phase_vel\n",
    "\n",
    "print(\"Vedic angular features ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56827b16",
   "metadata": {},
   "source": [
    "<a id=\"14\"></a>\n",
    "## 14. Timothy Masters Indicators\n",
    "\n",
    "Ported from C++ reference: FTI, Mutual Information, ADX, Detrended RSI, Price Variance Ratio, Price Intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f575aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 14: Timothy Masters indicators \u2014 reimplemented.\"\"\"\n",
    "\n",
    "_BH_D = np.array([0.35577019, 0.2436983, 0.07211497, 0.00630165])\n",
    "\n",
    "def _fti_coefs(period, half_length):\n",
    "    \"\"\"FIR lowpass filter coefficients (Blackman-Harris windowed sinc).\"\"\"\n",
    "    H = half_length\n",
    "    c = np.zeros(H + 1)\n",
    "    fact = 2.0 / period\n",
    "    c[0] = fact\n",
    "    fact_pi = fact * math.pi\n",
    "    for i in range(1, H + 1):\n",
    "        c[i] = math.sin(i * fact_pi) / (i * math.pi)\n",
    "    c[H] *= 0.5\n",
    "    sumg = c[0]\n",
    "    for i in range(1, H + 1):\n",
    "        w = _BH_D[0]\n",
    "        f = i * math.pi / H\n",
    "        for j in range(1, 4):\n",
    "            w += 2.0 * _BH_D[j] * math.cos(j * f)\n",
    "        c[i] *= w\n",
    "        sumg += 2.0 * c[i]\n",
    "    c /= sumg\n",
    "    return c\n",
    "\n",
    "\n",
    "def rolling_fti(closes, lookback=128, min_period=5, max_period=65, half_length=32):\n",
    "    \"\"\"Rolling Follow Through Index.\"\"\"\n",
    "    n = len(closes)\n",
    "    fti_best = np.full(n, np.nan)\n",
    "    max_period = min(max_period, 2 * half_length)\n",
    "\n",
    "    for idx in range(lookback - 1, n):\n",
    "        prices = closes[idx - lookback + 1: idx + 1]\n",
    "        if len(prices) < half_length + 2:\n",
    "            continue\n",
    "\n",
    "        y = np.empty(lookback + half_length)\n",
    "        y[:lookback] = np.log(np.maximum(prices, 1e-12))\n",
    "        H = half_length\n",
    "        L = lookback\n",
    "\n",
    "        seg = y[L - 1 - H: L]\n",
    "        xs = -np.arange(H + 1, dtype=np.float64)\n",
    "        xm, ym = xs.mean(), seg.mean()\n",
    "        xd, yd = xs - xm, seg - ym\n",
    "        slope = np.dot(xd, yd) / (np.dot(xd, xd) + 1e-30)\n",
    "        for k in range(H):\n",
    "            y[L + k] = (k + 1.0 - xm) * slope + ym\n",
    "\n",
    "        best_fti = 0.0\n",
    "        for period in range(min_period, max_period + 1):\n",
    "            c = _fti_coefs(period, H)\n",
    "            legs = []\n",
    "            extreme_type = 0\n",
    "            extreme_value = 0.0\n",
    "            longest_leg = 0.0\n",
    "            prior = 0.0\n",
    "            width_diffs = np.empty(L - H)\n",
    "\n",
    "            for iy in range(H, L):\n",
    "                s = c[0] * y[iy]\n",
    "                for i in range(1, H + 1):\n",
    "                    s += c[i] * (y[iy + i] + y[iy - i])\n",
    "                width_diffs[iy - H] = abs(y[iy] - s)\n",
    "\n",
    "                if iy == H:\n",
    "                    extreme_type = 0; extreme_value = s\n",
    "                elif extreme_type == 0:\n",
    "                    if s > extreme_value: extreme_type = -1\n",
    "                    elif s < extreme_value: extreme_type = 1\n",
    "                elif iy == L - 1:\n",
    "                    if extreme_type != 0:\n",
    "                        leg = abs(extreme_value - s)\n",
    "                        legs.append(leg)\n",
    "                        longest_leg = max(longest_leg, leg)\n",
    "                else:\n",
    "                    if extreme_type == 1 and s > prior:\n",
    "                        leg = extreme_value - prior\n",
    "                        legs.append(leg); longest_leg = max(longest_leg, leg)\n",
    "                        extreme_type = -1; extreme_value = prior\n",
    "                    elif extreme_type == -1 and s < prior:\n",
    "                        leg = prior - extreme_value\n",
    "                        legs.append(leg); longest_leg = max(longest_leg, leg)\n",
    "                        extreme_type = 1; extreme_value = prior\n",
    "                prior = s\n",
    "\n",
    "            width_sorted = np.sort(width_diffs)\n",
    "            w_idx = min(max(int(0.95 * (L - H + 1)) - 1, 0), L - H - 1)\n",
    "            width = width_sorted[w_idx]\n",
    "\n",
    "            if legs and longest_leg > 0:\n",
    "                noise_level = 0.20 * longest_leg\n",
    "                big_legs = [lg for lg in legs if lg > noise_level]\n",
    "                if big_legs:\n",
    "                    fti = sum(big_legs) / len(big_legs) / (width + 1e-5)\n",
    "                    best_fti = max(best_fti, fti)\n",
    "\n",
    "        fti_best[idx] = best_fti\n",
    "    return fti_best\n",
    "\n",
    "\n",
    "def rolling_mutual_information(closes, window=100, word_length=2):\n",
    "    \"\"\"Rolling mutual information (nonlinear serial dependency).\"\"\"\n",
    "    n = len(closes)\n",
    "    out = np.full(n, np.nan)\n",
    "    for i in range(window - 1, n):\n",
    "        seg = closes[i - window + 1: i + 1][::-1]  # newest first\n",
    "        nx = len(seg)\n",
    "        nn = nx - word_length - 1\n",
    "        if nn < 4:\n",
    "            continue\n",
    "        m = 2 ** word_length; nb_ = 2 * m\n",
    "        bins = np.zeros(nb_, dtype=np.int64)\n",
    "        dep_marg = np.zeros(2)\n",
    "        for ii in range(nn):\n",
    "            k = 1 if seg[ii] > seg[ii + 1] else 0\n",
    "            dep_marg[k] += 1\n",
    "            for j in range(1, word_length + 1):\n",
    "                k *= 2\n",
    "                if seg[ii + j] > seg[ii + j + 1]: k += 1\n",
    "            bins[k] += 1\n",
    "        dep_marg /= nn\n",
    "        MI = 0.0\n",
    "        for ii in range(m):\n",
    "            hist_marg = (bins[ii] + bins[ii + m]) / nn\n",
    "            if hist_marg < 1e-15: continue\n",
    "            p0 = bins[ii] / nn\n",
    "            if p0 > 0: MI += p0 * math.log(p0 / (hist_marg * max(dep_marg[0], 1e-15)))\n",
    "            p1 = bins[ii + m] / nn\n",
    "            if p1 > 0: MI += p1 * math.log(p1 / (hist_marg * max(dep_marg[1], 1e-15)))\n",
    "        out[i] = MI\n",
    "    return out\n",
    "\n",
    "\n",
    "def rolling_adx(highs, lows, closes, lookback=14):\n",
    "    \"\"\"Average Directional Index (Wilder smoothing).\"\"\"\n",
    "    n = len(closes)\n",
    "    out = np.full(n, np.nan)\n",
    "    if n < 2 * lookback + 1:\n",
    "        return out\n",
    "    lb = lookback; alpha = (lb - 1.0) / lb\n",
    "    DMSp = DMSm = ATR = 0.0\n",
    "    for i in range(1, lb + 1):\n",
    "        dm_up = highs[i] - highs[i - 1]; dm_dn = lows[i - 1] - lows[i]\n",
    "        if dm_up >= dm_dn: dm_dn = 0.0\n",
    "        else: dm_up = 0.0\n",
    "        dm_up = max(dm_up, 0); dm_dn = max(dm_dn, 0)\n",
    "        DMSp += dm_up; DMSm += dm_dn\n",
    "        tr = max(highs[i] - lows[i], abs(highs[i] - closes[i-1]), abs(closes[i-1] - lows[i]))\n",
    "        ATR += tr\n",
    "    ADX_sum = 0.0; adx_count = 0\n",
    "    for i in range(lb + 1, 2 * lb):\n",
    "        dm_up = highs[i] - highs[i-1]; dm_dn = lows[i-1] - lows[i]\n",
    "        if dm_up >= dm_dn: dm_dn = 0.0\n",
    "        else: dm_up = 0.0\n",
    "        dm_up = max(dm_up, 0); dm_dn = max(dm_dn, 0)\n",
    "        DMSp = alpha * DMSp + dm_up; DMSm = alpha * DMSm + dm_dn\n",
    "        tr = max(highs[i]-lows[i], abs(highs[i]-closes[i-1]), abs(closes[i-1]-lows[i]))\n",
    "        ATR = alpha * ATR + tr\n",
    "        DIp = DMSp / (ATR + 1e-10); DIm = DMSm / (ATR + 1e-10)\n",
    "        DX = abs(DIp - DIm) / (DIp + DIm + 1e-10)\n",
    "        ADX_sum += DX; adx_count += 1\n",
    "    ADX = ADX_sum / max(adx_count, 1)\n",
    "    for i in range(2 * lb, n):\n",
    "        dm_up = highs[i] - highs[i-1]; dm_dn = lows[i-1] - lows[i]\n",
    "        if dm_up >= dm_dn: dm_dn = 0.0\n",
    "        else: dm_up = 0.0\n",
    "        dm_up = max(dm_up, 0); dm_dn = max(dm_dn, 0)\n",
    "        DMSp = alpha * DMSp + dm_up; DMSm = alpha * DMSm + dm_dn\n",
    "        tr = max(highs[i]-lows[i], abs(highs[i]-closes[i-1]), abs(closes[i-1]-lows[i]))\n",
    "        ATR = alpha * ATR + tr\n",
    "        DIp = DMSp / (ATR + 1e-10); DIm = DMSm / (ATR + 1e-10)\n",
    "        DX = abs(DIp - DIm) / (DIp + DIm + 1e-10)\n",
    "        ADX = alpha * ADX + DX / lb\n",
    "        out[i] = 100.0 * ADX\n",
    "    return out\n",
    "\n",
    "\n",
    "def rolling_detrended_rsi(closes, short_period=5, long_period=14, reg_length=60):\n",
    "    \"\"\"Detrended RSI: residual of short RSI regressed on long RSI.\"\"\"\n",
    "    n = len(closes)\n",
    "    out = np.full(n, np.nan)\n",
    "    # Wilder RSI helper\n",
    "    def _rsi(cl, period):\n",
    "        r = np.full(len(cl), np.nan)\n",
    "        if len(cl) < period + 1: return r\n",
    "        diff = np.diff(cl)\n",
    "        up = dn = 0.0\n",
    "        for i in range(period):\n",
    "            if diff[i] > 0: up += diff[i]\n",
    "            else: dn -= diff[i]\n",
    "        up /= period; dn /= period\n",
    "        denom = up + dn\n",
    "        r[period] = 100.0 * up / denom if denom > 1e-15 else 50.0\n",
    "        for i in range(period, len(cl) - 1):\n",
    "            d = diff[i]\n",
    "            if d > 0: up = ((period-1)*up + d)/period; dn = (period-1)*dn/period\n",
    "            else: dn = ((period-1)*dn - d)/period; up = (period-1)*up/period\n",
    "            denom = up + dn\n",
    "            r[i+1] = 100.0 * up / denom if denom > 1e-15 else 50.0\n",
    "        return r\n",
    "\n",
    "    rsi_s = _rsi(closes, short_period)\n",
    "    rsi_l = _rsi(closes, long_period)\n",
    "    warmup = long_period + reg_length\n",
    "    for i in range(warmup, n):\n",
    "        xs = rsi_l[i-reg_length+1:i+1]; ys = rsi_s[i-reg_length+1:i+1]\n",
    "        if np.any(np.isnan(xs)) or np.any(np.isnan(ys)): continue\n",
    "        xm, ym = xs.mean(), ys.mean()\n",
    "        xd = xs - xm; coef = np.dot(xd, ys - ym) / (np.dot(xd, xd) + 1e-30)\n",
    "        out[i] = (rsi_s[i] - ym) - coef * (rsi_l[i] - xm)\n",
    "    return out\n",
    "\n",
    "\n",
    "def rolling_price_variance_ratio(closes, short_len=10, mult=4):\n",
    "    \"\"\"Price variance ratio (F-CDF centred).\"\"\"\n",
    "    n = len(closes); long_len = short_len * mult\n",
    "    out = np.full(n, np.nan)\n",
    "    if n < long_len + 1: return out\n",
    "    log_ret = np.diff(np.log(np.maximum(closes, 1e-12)))\n",
    "    for i in range(long_len, n - 1):\n",
    "        vs = np.var(log_ret[i-short_len+1:i+1])\n",
    "        vl = np.var(log_ret[i-long_len+1:i+1])\n",
    "        if vl < 1e-20: continue\n",
    "        out[i+1] = 100.0 * f_dist.cdf(vs/vl, 4, 4*mult) - 50.0\n",
    "    return out\n",
    "\n",
    "\n",
    "def rolling_price_intensity(opens, highs, lows, closes, smooth=10):\n",
    "    \"\"\"Price intensity: (close-open)/TR, EMA smoothed.\"\"\"\n",
    "    n = len(closes); raw = np.full(n, np.nan)\n",
    "    denom = max(highs[0] - lows[0], 1e-10)\n",
    "    raw[0] = (closes[0] - opens[0]) / denom\n",
    "    for i in range(1, n):\n",
    "        denom = max(highs[i]-lows[i], abs(highs[i]-closes[i-1]), abs(closes[i-1]-lows[i]), 1e-10)\n",
    "        raw[i] = (closes[i] - opens[i]) / denom\n",
    "    if smooth > 1:\n",
    "        alpha = 2.0 / (smooth + 1.0); sm = raw[0]\n",
    "        for i in range(1, n):\n",
    "            if np.isnan(raw[i]): continue\n",
    "            sm = alpha * raw[i] + (1 - alpha) * sm\n",
    "            raw[i] = sm\n",
    "    return raw\n",
    "\n",
    "print(\"Timothy Masters indicators ready (6 indicators)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26082b",
   "metadata": {},
   "source": [
    "<a id=\"15\"></a>\n",
    "## 15. Auxiliary Features\n",
    "\n",
    "Yang-Zhang vol, normalised ATR, VPIN, Shannon entropy, Ramanujan period energy, RMT absorption ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855088fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 15: Auxiliary features.\"\"\"\n",
    "\n",
    "def yang_zhang_vol(opens, highs, lows, closes, window=20):\n",
    "    \"\"\"Yang-Zhang volatility estimator (overnight + Rogers-Satchell).\"\"\"\n",
    "    n = len(closes); out = np.full(n, np.nan)\n",
    "    for i in range(window, n):\n",
    "        o = opens[i-window+1:i+1]; h = highs[i-window+1:i+1]\n",
    "        l = lows[i-window+1:i+1]; c = closes[i-window+1:i+1]\n",
    "        c_prev = closes[i-window:i]\n",
    "        log_oc = np.log(o / np.maximum(c_prev, 1e-10))\n",
    "        log_co = np.log(c / np.maximum(o, 1e-10))\n",
    "        log_ho = np.log(h / np.maximum(o, 1e-10))\n",
    "        log_lo = np.log(l / np.maximum(o, 1e-10))\n",
    "        log_hc = np.log(h / np.maximum(c, 1e-10))\n",
    "        log_lc = np.log(l / np.maximum(c, 1e-10))\n",
    "        overnight = np.var(log_oc, ddof=1)\n",
    "        rs = np.mean(log_ho * log_hc + log_lo * log_lc)\n",
    "        close_var = np.var(log_co, ddof=1)\n",
    "        k = 0.34 / (1.34 + (window + 1) / (window - 1))\n",
    "        yz = overnight + k * close_var + (1 - k) * rs\n",
    "        out[i] = math.sqrt(max(yz, 0)) * math.sqrt(252)\n",
    "    return out\n",
    "\n",
    "\n",
    "def rolling_atr_norm(highs, lows, closes, lookback=14):\n",
    "    \"\"\"Normalised ATR.\"\"\"\n",
    "    n = len(closes); out = np.full(n, np.nan)\n",
    "    for i in range(lookback, n):\n",
    "        trs = np.empty(lookback)\n",
    "        for j in range(lookback):\n",
    "            idx = i - lookback + 1 + j\n",
    "            tr = highs[idx] - lows[idx]\n",
    "            if idx > 0:\n",
    "                tr = max(tr, abs(highs[idx]-closes[idx-1]), abs(closes[idx-1]-lows[idx]))\n",
    "            trs[j] = tr\n",
    "        out[i] = np.mean(trs) / max(closes[i], 1e-10)\n",
    "    return out\n",
    "\n",
    "\n",
    "def rolling_vpin(closes, volume, n_buckets=50, window=100):\n",
    "    \"\"\"Volume-synchronised PIN (bulk volume classification).\"\"\"\n",
    "    n = len(closes); out = np.full(n, np.nan)\n",
    "    if n < window + 1: return out\n",
    "    log_ret = np.diff(np.log(np.maximum(closes, 1e-12)))\n",
    "    sigma = np.std(log_ret[:window], ddof=1) if window < n else 0.01\n",
    "    sigma = max(sigma, 1e-10)\n",
    "    for i in range(window, n - 1):\n",
    "        rets = log_ret[i-window+1:i+1]\n",
    "        vols = volume[i-window+1:i+1].astype(float)\n",
    "        total_vol = max(vols.sum(), 1)\n",
    "        bucket_vol = total_vol / n_buckets\n",
    "        buy_vol = sell_vol = current_vol = 0.0\n",
    "        for j in range(len(rets)):\n",
    "            from scipy.stats import norm as norm_dist\n",
    "            z = rets[j] / sigma\n",
    "            buy_frac = norm_dist.cdf(z)\n",
    "            buy_vol += vols[j] * buy_frac\n",
    "            sell_vol += vols[j] * (1 - buy_frac)\n",
    "        out[i+1] = abs(buy_vol - sell_vol) / max(buy_vol + sell_vol, 1e-10)\n",
    "    return out\n",
    "\n",
    "\n",
    "def rolling_entropy(closes, word_length=2, window=100):\n",
    "    \"\"\"Shannon entropy of discretised returns.\"\"\"\n",
    "    n = len(closes); out = np.full(n, np.nan)\n",
    "    for i in range(window, n):\n",
    "        rets = np.diff(closes[i-window:i+1])\n",
    "        up = (rets > 0).astype(int)\n",
    "        words = {}\n",
    "        for j in range(len(up) - word_length + 1):\n",
    "            w = tuple(up[j:j+word_length])\n",
    "            words[w] = words.get(w, 0) + 1\n",
    "        total = sum(words.values())\n",
    "        H = 0.0\n",
    "        for cnt in words.values():\n",
    "            p = cnt / total\n",
    "            if p > 0: H -= p * math.log(p)\n",
    "        out[i] = H\n",
    "    return out\n",
    "\n",
    "\n",
    "def rolling_period_energy(closes, max_period=32, window=64):\n",
    "    \"\"\"Ramanujan periodogram peak energy.\"\"\"\n",
    "    n = len(closes); out = np.full(n, np.nan)\n",
    "    for i in range(window, n):\n",
    "        rets = np.diff(np.log(np.maximum(closes[i-window:i+1], 1e-8)))\n",
    "        rets = rets - rets.mean()\n",
    "        N_ = len(rets)\n",
    "        best_e = 0.0\n",
    "        for q in range(2, max_period + 1):\n",
    "            cq = np.array([ramanujan_sum(q, nn) for nn in range(N_)], dtype=np.float64)\n",
    "            proj = np.dot(rets, cq)\n",
    "            phi_q = _euler_phi(q)\n",
    "            if phi_q > 0:\n",
    "                e = (proj**2) / (N_ * phi_q**2)\n",
    "                best_e = max(best_e, e)\n",
    "        out[i] = best_e\n",
    "    return out\n",
    "\n",
    "\n",
    "def rmt_absorption_ratio(returns_matrix, top_k=5):\n",
    "    \"\"\"Absorption ratio from correlation matrix eigenvalues.\"\"\"\n",
    "    n_obs, n_assets = returns_matrix.shape\n",
    "    if n_obs < 3 or n_assets < 2:\n",
    "        return np.nan, np.nan\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        corr = np.corrcoef(returns_matrix.T)\n",
    "    if not np.isfinite(corr).all():\n",
    "        corr = np.nan_to_num(corr, nan=0.0); np.fill_diagonal(corr, 1.0)\n",
    "    eigenvalues = np.linalg.eigvalsh(corr)[::-1]\n",
    "    eigenvalues = np.maximum(eigenvalues, 0.0)\n",
    "    total = eigenvalues.sum()\n",
    "    if total < 1e-12:\n",
    "        return np.nan, np.nan\n",
    "    k = min(top_k, len(eigenvalues))\n",
    "    absorption = float(eigenvalues[:k].sum() / total)\n",
    "    q = n_assets / n_obs\n",
    "    mp_upper = (1.0 + np.sqrt(q))**2\n",
    "    mp_excess = float(eigenvalues[0] / mp_upper) if mp_upper > 0 else 0.0\n",
    "    return absorption, mp_excess\n",
    "\n",
    "print(\"Auxiliary features ready (6 features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b1745",
   "metadata": {},
   "source": [
    "<a id=\"16\"></a>\n",
    "## 16. Feature Pipeline\n",
    "\n",
    "Architecture: DuckDB \u2192 1-min bars \u2192 24 features \u2192 z-score \u2192 signal.\n",
    "\n",
    "| # | Feature | Source | Frequency |\n",
    "|---|---------|--------|-----------|\n",
    "| 1 | frac_alpha | MSD + waiting-time | daily (from 1-min bars) |\n",
    "| 2 | frac_hurst | \u03b1/2 | daily |\n",
    "| 3 | frac_d_series | Hosking differencing (d=0.226) | daily |\n",
    "| 4 | mock_theta_ratio | f(q)/\u03c6(q) | daily |\n",
    "| 5 | mock_theta_div | |\u0394(f/\u03c6)| | daily |\n",
    "| 6 | vol_distortion | Ramanujan CF transform | daily |\n",
    "| 7 | angular_coherence | Madhava kernel to centroid | daily |\n",
    "| 8 | aryabhata_phase | Phase in [0,1] | daily |\n",
    "| 9 | period_energy | Ramanujan periodogram peak | daily |\n",
    "| 10 | entropy | Shannon entropy of returns | daily |\n",
    "| 11 | yang_zhang_vol | YZ estimator | daily |\n",
    "| 12 | atr_norm | Normalised ATR | daily |\n",
    "| 13 | vpin | Volume-synced PIN | daily |\n",
    "| 14-15 | rmt_absorption, rmt_mp_excess | RMT eigenvalue | daily |\n",
    "| 16-19 | sanos_skew, left_tail, entropy, kl | SANOS density | daily |\n",
    "| 20 | fti | Follow Through Index | daily |\n",
    "| 21 | mutual_info | Shannon MI | daily |\n",
    "| 22 | adx | Average Directional Index | daily |\n",
    "| 23 | detrended_rsi | Detrended RSI | daily |\n",
    "| 24 | price_intensity | (C-O)/TR smoothed | daily |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0388b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 17: Compute all 24 features from 1-min bars + SANOS cache.\"\"\"\n",
    "\n",
    "def load_futures_bars(con, symbol, dates):\n",
    "    \"\"\"Load 1-min futures bars for all dates. Returns dict[date] -> {open,high,low,close,volume}.\"\"\"\n",
    "    bars = {}\n",
    "    for d in dates:\n",
    "        df = con.execute(\n",
    "            \"SELECT open, high, low, close, volume \"\n",
    "            \"FROM nfo_1min \"\n",
    "            \"WHERE name = ? AND date = ? AND instrument_type = 'FUT' \"\n",
    "            \"AND expiry = (\"\n",
    "            \"  SELECT MIN(expiry) FROM nfo_1min \"\n",
    "            \"  WHERE name = ? AND date = ? AND instrument_type = 'FUT'\"\n",
    "            \")\",\n",
    "            [symbol, d, symbol, d]\n",
    "        ).fetchdf()\n",
    "        if df is not None and len(df) >= 30:\n",
    "            bars[d] = {\n",
    "                \"open\": df[\"open\"].values.astype(np.float64),\n",
    "                \"high\": df[\"high\"].values.astype(np.float64),\n",
    "                \"low\": df[\"low\"].values.astype(np.float64),\n",
    "                \"close\": df[\"close\"].values.astype(np.float64),\n",
    "                \"volume\": df[\"volume\"].values.astype(np.float64),\n",
    "            }\n",
    "    return bars\n",
    "\n",
    "\n",
    "def _zscore(arr):\n",
    "    \"\"\"Rolling z-score (expanding window, causal).\"\"\"\n",
    "    out = np.full(len(arr), np.nan)\n",
    "    for i in range(20, len(arr)):\n",
    "        window = arr[max(0, i-250):i+1]\n",
    "        valid = window[~np.isnan(window)]\n",
    "        if len(valid) < 10:\n",
    "            continue\n",
    "        mu, sigma = np.mean(valid), np.std(valid, ddof=1)\n",
    "        if sigma > 1e-10 and not np.isnan(arr[i]):\n",
    "            out[i] = (arr[i] - mu) / sigma\n",
    "    return out\n",
    "\n",
    "\n",
    "def compute_all_features(con, symbol, dates, futures_bars, sanos_cache_sym):\n",
    "    \"\"\"Master feature computation. Returns dict of arrays (n_days,).\"\"\"\n",
    "    n = len(dates)\n",
    "    date_to_idx = {d: i for i, d in enumerate(dates)}\n",
    "\n",
    "    # Daily close from last 1-min bar\n",
    "    daily_close = np.full(n, np.nan)\n",
    "    daily_open = np.full(n, np.nan)\n",
    "    daily_high = np.full(n, np.nan)\n",
    "    daily_low = np.full(n, np.nan)\n",
    "    daily_volume = np.full(n, np.nan)\n",
    "\n",
    "    for d, b in futures_bars.items():\n",
    "        if d in date_to_idx:\n",
    "            idx = date_to_idx[d]\n",
    "            daily_close[idx] = b[\"close\"][-1]\n",
    "            daily_open[idx] = b[\"open\"][0]\n",
    "            daily_high[idx] = np.max(b[\"high\"])\n",
    "            daily_low[idx] = np.min(b[\"low\"])\n",
    "            daily_volume[idx] = np.sum(b[\"volume\"])\n",
    "\n",
    "    # Forward-fill close for gaps\n",
    "    for i in range(1, n):\n",
    "        if np.isnan(daily_close[i]) and not np.isnan(daily_close[i-1]):\n",
    "            daily_close[i] = daily_close[i-1]\n",
    "\n",
    "    # Log returns\n",
    "    log_ret = np.full(n, np.nan)\n",
    "    for i in range(1, n):\n",
    "        if daily_close[i] > 0 and daily_close[i-1] > 0:\n",
    "            log_ret[i] = math.log(daily_close[i] / daily_close[i-1])\n",
    "\n",
    "    features = {}\n",
    "    ALPHA_WINDOW = 60\n",
    "\n",
    "    # \u2500\u2500 Fractional features (from daily closes) \u2500\u2500\n",
    "    # NOTE: \"alpha\" is an MSD/waiting-time diffusion-exponent proxy (0.6*MSD + 0.4*Hill),\n",
    "    # NOT the time-fractional order in the CTRW/FFPE sense (which requires \u03b1 \u2208 (0,1]).\n",
    "    alpha_arr = np.full(n, np.nan)\n",
    "    hurst_arr = np.full(n, np.nan)\n",
    "    for i in range(ALPHA_WINDOW, n):\n",
    "        rets = log_ret[i-ALPHA_WINDOW+1:i+1]\n",
    "        valid = rets[~np.isnan(rets)]\n",
    "        if len(valid) < 20:\n",
    "            continue\n",
    "        a = estimate_alpha(valid, max_lag=min(30, len(valid)//2))\n",
    "        alpha_arr[i] = a\n",
    "        hurst_arr[i] = a / 2.0\n",
    "    features[\"alpha\"] = alpha_arr\n",
    "    features[\"hurst\"] = hurst_arr\n",
    "\n",
    "    # Fractional differencing\n",
    "    # Forward-fill NaNs (replacing with 0 injects discontinuities of magnitude P_t)\n",
    "    valid_close = daily_close.copy()\n",
    "    mask = np.isnan(valid_close)\n",
    "    if mask.any():\n",
    "        for i in range(1, len(valid_close)):\n",
    "            if mask[i] and not mask[i-1]:\n",
    "                valid_close[i] = valid_close[i-1]\n",
    "            elif mask[i]:\n",
    "                valid_close[i] = valid_close[i-1] if i > 0 and not np.isnan(valid_close[i-1]) else 0.0\n",
    "    features[\"frac_d_series\"] = fractional_differentiation(valid_close, d=0.226, max_window=100)\n",
    "\n",
    "    # \u2500\u2500 Mock theta features \u2500\u2500\n",
    "    mock_f_arr = np.full(n, np.nan)\n",
    "    mock_phi_arr = np.full(n, np.nan)\n",
    "    mock_ratio_arr = np.full(n, np.nan)\n",
    "    mock_div_arr = np.full(n, np.nan)\n",
    "    vol_dist_arr = np.full(n, np.nan)\n",
    "    prev_ratio = None\n",
    "    MOCK_WINDOW = 20\n",
    "\n",
    "    for i in range(MOCK_WINDOW, n):\n",
    "        rets = log_ret[i-MOCK_WINDOW+1:i+1]\n",
    "        valid = rets[~np.isnan(rets)]\n",
    "        if len(valid) < 5:\n",
    "            continue\n",
    "        sigma = max(np.std(valid, ddof=1), 1e-8)\n",
    "        mean_abs = np.mean(np.abs(valid))\n",
    "        q_val = math.exp(-math.pi / (1.0 + mean_abs / sigma))\n",
    "        f_val = mock_theta_f(q_val)\n",
    "        phi_val = mock_theta_phi(q_val)\n",
    "        mock_f_arr[i] = f_val\n",
    "        mock_phi_arr[i] = phi_val\n",
    "        ratio = f_val / max(phi_val, 1e-30)\n",
    "        mock_ratio_arr[i] = ratio\n",
    "        if prev_ratio is not None:\n",
    "            mock_div_arr[i] = abs(ratio - prev_ratio)\n",
    "        prev_ratio = ratio\n",
    "        vol_ann = sigma * math.sqrt(252)\n",
    "        vol_dist_arr[i] = ramanujan_volatility_distortion(vol_ann)\n",
    "\n",
    "    features[\"mock_theta_ratio\"] = mock_ratio_arr\n",
    "    features[\"mock_theta_div\"] = mock_div_arr\n",
    "    features[\"vol_distortion\"] = vol_dist_arr\n",
    "\n",
    "    # \u2500\u2500 Vedic angular features \u2500\u2500\n",
    "    coherence_arr = np.full(n, np.nan)\n",
    "    phase_arr = np.full(n, np.nan)\n",
    "    centroids = {}\n",
    "    regime_names = [\"subdiffusive\", \"normal\", \"superdiffusive\"]\n",
    "\n",
    "    for i in range(ALPHA_WINDOW, n):\n",
    "        rets = log_ret[i-ALPHA_WINDOW+1:i+1]\n",
    "        valid = rets[~np.isnan(rets)]\n",
    "        if len(valid) < 10:\n",
    "            continue\n",
    "        vol = np.std(valid, ddof=1)\n",
    "        mom = np.sum(valid)\n",
    "        mean_r = np.mean(valid)\n",
    "        feat_vec = np.array([vol, mom, mean_r])\n",
    "\n",
    "        if centroids:\n",
    "            _, coh, _ = angular_coherence(feat_vec, centroids, order=4)\n",
    "            coherence_arr[i] = coh\n",
    "\n",
    "        # Classify regime\n",
    "        autocorr = np.corrcoef(valid[:-1], valid[1:])[0, 1] if len(valid) > 2 else 0\n",
    "        if autocorr < -0.1: regime = \"subdiffusive\"\n",
    "        elif autocorr > 0.1: regime = \"superdiffusive\"\n",
    "        else: regime = \"normal\"\n",
    "        centroids = update_regime_centroids(centroids, feat_vec, regime)\n",
    "\n",
    "    features[\"coherence\"] = coherence_arr\n",
    "\n",
    "    # Aryabhata phase\n",
    "    for i in range(ALPHA_WINDOW * 2, n):\n",
    "        window_close = daily_close[i-ALPHA_WINDOW:i+1]\n",
    "        valid_c = window_close[~np.isnan(window_close)]\n",
    "        if len(valid_c) < 20:\n",
    "            continue\n",
    "        rets_w = np.diff(np.log(np.maximum(valid_c, 1e-8)))\n",
    "        rets_w = rets_w - rets_w.mean()\n",
    "        periods = dominant_periods(rets_w, max_period=min(32, len(rets_w)//2), top_k=1)\n",
    "        period = periods[0] if periods else ALPHA_WINDOW // 4\n",
    "        ph, _ = aryabhata_phase(valid_c, period)\n",
    "        if not np.isnan(ph[-1]):\n",
    "            phase_arr[i] = ph[-1]\n",
    "\n",
    "    features[\"phase\"] = phase_arr\n",
    "\n",
    "    # \u2500\u2500 Masters + auxiliary features (from daily OHLCV) \u2500\u2500\n",
    "    # Forward-fill NaNs in OHLCV (replacing with 0 injects price-magnitude discontinuities)\n",
    "    def _ffill(arr):\n",
    "        out = arr.copy()\n",
    "        for i in range(1, len(out)):\n",
    "            if np.isnan(out[i]):\n",
    "                out[i] = out[i-1]\n",
    "        return out\n",
    "\n",
    "    c_valid = _ffill(daily_close)\n",
    "    o_valid = _ffill(daily_open)\n",
    "    # Where open is still NaN after ffill (e.g. leading NaNs), use close\n",
    "    o_valid = np.where(np.isnan(o_valid), c_valid, o_valid)\n",
    "    h_valid = _ffill(daily_high)\n",
    "    h_valid = np.where(np.isnan(h_valid), c_valid, h_valid)\n",
    "    l_valid = _ffill(daily_low)\n",
    "    l_valid = np.where(np.isnan(l_valid), c_valid, l_valid)\n",
    "    v_valid = np.where(np.isnan(daily_volume), 0, daily_volume)  # volume: 0 is valid (no trading)\n",
    "\n",
    "    features[\"fti\"] = rolling_fti(c_valid, lookback=128)\n",
    "    features[\"mutual_info\"] = rolling_mutual_information(c_valid, window=100)\n",
    "    features[\"adx\"] = rolling_adx(h_valid, l_valid, c_valid, lookback=14)\n",
    "    features[\"detrended_rsi\"] = rolling_detrended_rsi(c_valid, short_period=5, long_period=14)\n",
    "    features[\"price_intensity\"] = rolling_price_intensity(o_valid, h_valid, l_valid, c_valid)\n",
    "    features[\"yang_zhang_vol\"] = yang_zhang_vol(o_valid, h_valid, l_valid, c_valid, window=20)\n",
    "    features[\"atr_norm\"] = rolling_atr_norm(h_valid, l_valid, c_valid, lookback=14)\n",
    "    features[\"entropy\"] = rolling_entropy(c_valid, word_length=2, window=100)\n",
    "    features[\"period_energy\"] = rolling_period_energy(c_valid, max_period=32, window=64)\n",
    "\n",
    "    # VPIN (needs volume)\n",
    "    features[\"vpin\"] = rolling_vpin(c_valid, v_valid, n_buckets=50, window=100)\n",
    "\n",
    "    # \u2500\u2500 SANOS features (from cache) \u2500\u2500\n",
    "    sanos_skew = np.full(n, np.nan)\n",
    "    sanos_lt = np.full(n, np.nan)\n",
    "    sanos_ent = np.full(n, np.nan)\n",
    "    sanos_kl = np.full(n, np.nan)\n",
    "\n",
    "    for d, v in sanos_cache_sym.items():\n",
    "        if d in date_to_idx and v[\"density_ok\"]:\n",
    "            idx = date_to_idx[d]\n",
    "            sanos_skew[idx] = v[\"skew\"]\n",
    "            sanos_lt[idx] = v[\"left_tail\"]\n",
    "            sanos_ent[idx] = v[\"entropy\"]\n",
    "            sanos_kl[idx] = v[\"kl\"]\n",
    "\n",
    "    features[\"sanos_skew\"] = sanos_skew\n",
    "    features[\"sanos_left_tail\"] = sanos_lt\n",
    "    features[\"sanos_entropy\"] = sanos_ent\n",
    "    features[\"sanos_kl\"] = sanos_kl\n",
    "\n",
    "    # \u2500\u2500 RMT (need multi-asset returns \u2014 use NIFTY sectors if available) \u2500\u2500\n",
    "    # Simplified: use rolling eigenvalue of auto-correlation matrix of features\n",
    "    rmt_abs = np.full(n, np.nan)\n",
    "    rmt_mp = np.full(n, np.nan)\n",
    "    rmt_window = 60\n",
    "    feat_matrix_keys = [\"alpha\", \"mock_theta_ratio\", \"vol_distortion\", \"coherence\", \"fti\"]\n",
    "    for i in range(rmt_window + ALPHA_WINDOW, n):\n",
    "        mat = np.column_stack([\n",
    "            features[k][i-rmt_window:i] for k in feat_matrix_keys\n",
    "        ])\n",
    "        if np.isnan(mat).any():\n",
    "            # Fill NaN with column mean\n",
    "            for col in range(mat.shape[1]):\n",
    "                col_mean = np.nanmean(mat[:, col])\n",
    "                mat[np.isnan(mat[:, col]), col] = col_mean if not np.isnan(col_mean) else 0\n",
    "        ab, mp = rmt_absorption_ratio(mat, top_k=2)\n",
    "        rmt_abs[i] = ab\n",
    "        rmt_mp[i] = mp\n",
    "\n",
    "    features[\"rmt_absorption\"] = rmt_abs\n",
    "    features[\"rmt_mp_excess\"] = rmt_mp\n",
    "\n",
    "    features[\"daily_close\"] = daily_close\n",
    "    features[\"log_ret\"] = log_ret\n",
    "\n",
    "    # Coverage report\n",
    "    print(f\"\\nFeature coverage for {symbol}:\")\n",
    "    for k, v in sorted(features.items()):\n",
    "        if k in (\"daily_close\", \"log_ret\"):\n",
    "            continue\n",
    "        valid = np.sum(~np.isnan(v))\n",
    "        print(f\"  {k:<20} {valid:4d}/{n} ({valid/n*100:.0f}%)\")\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "# \u2500\u2500 Load data & compute features \u2500\u2500\n",
    "all_features = {}\n",
    "all_futures_bars = {}\n",
    "\n",
    "for sym in SYMBOLS:\n",
    "    dates_for_sym = sorted(con.execute(\n",
    "        \"SELECT DISTINCT CAST(date AS VARCHAR) AS date FROM nfo_1min \"\n",
    "        \"WHERE name = ? AND instrument_type = 'FUT' ORDER BY date\", [sym]\n",
    "    ).fetchdf()[\"date\"].tolist())\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Loading 1-min bars for {sym} ({len(dates_for_sym)} dates)...\")\n",
    "    t0 = time.time()\n",
    "    futures_bars = load_futures_bars(con, sym, dates_for_sym)\n",
    "    print(f\"  Loaded {len(futures_bars)} days of bars in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    print(f\"Computing features for {sym}...\")\n",
    "    t0 = time.time()\n",
    "    features = compute_all_features(\n",
    "        con, sym, dates_for_sym, futures_bars,\n",
    "        sanos_cache.get(sym, {}),\n",
    "    )\n",
    "    print(f\"  Features computed in {time.time()-t0:.1f}s\")\n",
    "\n",
    "    all_features[sym] = features\n",
    "    all_futures_bars[sym] = futures_bars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f6aaeb",
   "metadata": {},
   "source": [
    "<a id=\"18\"></a>\n",
    "## 18. Feature Visualization\n",
    "\n",
    "Correlation heatmap, distributions, top-feature time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65f27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 18: Feature visualisation.\"\"\"\n",
    "\n",
    "sym = SYMBOLS[0]  # Lead symbol for visualisation\n",
    "feat = all_features[sym]\n",
    "feat_keys = [k for k in feat if k not in (\"daily_close\", \"log_ret\")]\n",
    "\n",
    "# Build feature matrix\n",
    "n = len(feat[\"daily_close\"])\n",
    "feat_matrix = np.column_stack([feat[k] for k in feat_keys])\n",
    "\n",
    "# Correlation heatmap (pairwise, ignoring NaN)\n",
    "n_feats = len(feat_keys)\n",
    "corr_mat = np.full((n_feats, n_feats), np.nan)\n",
    "for i in range(n_feats):\n",
    "    for j in range(n_feats):\n",
    "        mask = ~(np.isnan(feat_matrix[:, i]) | np.isnan(feat_matrix[:, j]))\n",
    "        if mask.sum() > 20:\n",
    "            corr_mat[i, j] = np.corrcoef(feat_matrix[mask, i], feat_matrix[mask, j])[0, 1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "im = ax.imshow(corr_mat, cmap=\"RdBu_r\", vmin=-1, vmax=1, aspect=\"auto\")\n",
    "ax.set_xticks(range(n_feats)); ax.set_xticklabels(feat_keys, rotation=90, fontsize=7)\n",
    "ax.set_yticks(range(n_feats)); ax.set_yticklabels(feat_keys, fontsize=7)\n",
    "plt.colorbar(im, ax=ax, shrink=0.8)\n",
    "ax.set_title(f\"{sym} \u2014 Feature Correlation Heatmap (24 features)\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Feature distributions (top 8)\n",
    "top_feats = [\"alpha\", \"mock_theta_ratio\", \"vol_distortion\", \"coherence\",\n",
    "             \"sanos_skew\", \"fti\", \"adx\", \"yang_zhang_vol\"]\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
    "for idx, k in enumerate(top_feats):\n",
    "    ax = axes[idx // 4, idx % 4]\n",
    "    vals = feat[k][~np.isnan(feat[k])]\n",
    "    if len(vals) > 0:\n",
    "        ax.hist(vals, bins=40, color=\"steelblue\", alpha=0.7, edgecolor=\"white\")\n",
    "    ax.set_title(k, fontsize=9)\n",
    "    ax.tick_params(labelsize=7)\n",
    "plt.suptitle(f\"{sym} \u2014 Feature Distributions\", fontsize=12)\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4e84b5",
   "metadata": {},
   "source": [
    "<a id=\"19\"></a>\n",
    "## 19. Signal Construction\n",
    "\n",
    "Composite signal: $S = \\sum_i w_i \\cdot z(f_i)$ where $z$ = causal z-score normalisation.\n",
    "\n",
    "**Walk-forward protocol**:\n",
    "- Train window: 190 days\n",
    "- Test window: 63 days\n",
    "- Advance by 63 days per fold\n",
    "- Optuna: 200 trials per fold, maximise Sharpe on train\n",
    "- OOS evaluation: best params on test (no re-optimisation)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\"\"\"Cell 19b: Common backtest utilities \u2014 equity curve, costs, Sharpe.\"\"\"\n",
    "\n",
    "def _apply_roundtrip_cost(ret: float, cost_bps: float) -> float:\n",
    "    \"\"\"Round-trip cost in basis points of notional (entry + exit combined).\"\"\"\n",
    "    return ret - (cost_bps / 1e4)\n",
    "\n",
    "\n",
    "def equity_curve_from_returns(returns, start_equity=1.0):\n",
    "    \"\"\"Multiplicative equity curve from simple returns.\"\"\"\n",
    "    r = np.asarray(returns, dtype=float)\n",
    "    eq = np.empty(len(r) + 1, dtype=float)\n",
    "    eq[0] = start_equity\n",
    "    for i in range(len(r)):\n",
    "        eq[i + 1] = eq[i] * (1.0 + r[i])\n",
    "    return eq\n",
    "\n",
    "\n",
    "def sharpe_daily(returns, ann_factor=252.0):\n",
    "    \"\"\"Annualised Sharpe from daily returns. ddof=1.\"\"\"\n",
    "    r = np.asarray(returns, dtype=float)\n",
    "    if len(r) < 2:\n",
    "        return 0.0\n",
    "    mu = np.mean(r)\n",
    "    sd = np.std(r, ddof=1)\n",
    "    if sd == 0:\n",
    "        return 0.0\n",
    "    return float((mu / sd) * np.sqrt(ann_factor))\n",
    "\n",
    "\n",
    "def max_drawdown_from_equity(equity):\n",
    "    \"\"\"Max drawdown (positive number, e.g. 0.15 = 15%).\"\"\"\n",
    "    eq = np.asarray(equity, dtype=float)\n",
    "    peak = np.maximum.accumulate(eq)\n",
    "    dd = (eq - peak) / np.where(peak > 0, peak, 1.0)\n",
    "    return float(abs(dd.min()))\n",
    "\n",
    "\n",
    "print(\"Utilities ready: _apply_roundtrip_cost, equity_curve_from_returns, sharpe_daily, max_drawdown_from_equity\")"
   ],
   "outputs": [],
   "execution_count": null,
   "id": "f62b7fdc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9761b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 20: Optuna walk-forward optimisation \u2014 CORRECTED.\n",
    "\n",
    "Fixes applied:\n",
    "  - Signal on day i \u2192 position held on day i+1 \u2192 return = close[i+1]/close[i] - 1\n",
    "  - Multiplicative equity curve (not additive sum)\n",
    "  - Round-trip cost applied on entry AND exit (position flips)\n",
    "  - Entry day accrues the first holding-day return (not 0)\n",
    "\"\"\"\n",
    "\n",
    "FEATURE_KEYS = [\n",
    "    \"alpha\", \"hurst\", \"frac_d_series\", \"mock_theta_ratio\", \"mock_theta_div\",\n",
    "    \"vol_distortion\", \"coherence\", \"phase\", \"period_energy\", \"entropy\",\n",
    "    \"yang_zhang_vol\", \"atr_norm\", \"vpin\", \"rmt_absorption\", \"rmt_mp_excess\",\n",
    "    \"sanos_skew\", \"sanos_left_tail\", \"sanos_entropy\", \"sanos_kl\",\n",
    "    \"fti\", \"mutual_info\", \"adx\", \"detrended_rsi\", \"price_intensity\",\n",
    "]\n",
    "\n",
    "\n",
    "def backtest_signal(features, daily_close, weights, entry_threshold=0.3,\n",
    "                    hold_days=5, cost_bps=5.0, start_idx=0, end_idx=None):\n",
    "    \"\"\"Backtest: weighted z-scored features -> daily signal -> PnL.\n",
    "\n",
    "    Fully causal T+1 execution:\n",
    "      - Signal computed at EOD of day i\n",
    "      - Position taken on day i+1\n",
    "      - Return for day i+1 = close[i+1]/close[i] - 1\n",
    "      - Round-trip cost applied on every position change\n",
    "    \"\"\"\n",
    "    n = len(daily_close)\n",
    "    if end_idx is None:\n",
    "        end_idx = n\n",
    "    cost_rt = cost_bps / 1e4  # round-trip cost fraction\n",
    "\n",
    "    # Z-score all features (causal, expanding window)\n",
    "    z_features = {}\n",
    "    for k in FEATURE_KEYS:\n",
    "        z_features[k] = _zscore(features[k])\n",
    "\n",
    "    # Pre-compute daily returns: dr[i] = close[i+1]/close[i] - 1\n",
    "    # dr[i] is the return earned by holding from close[i] to close[i+1]\n",
    "    dr = np.full(n, np.nan)\n",
    "    for i in range(n - 1):\n",
    "        if daily_close[i] > 0 and daily_close[i+1] > 0:\n",
    "            dr[i] = (daily_close[i+1] - daily_close[i]) / daily_close[i]\n",
    "\n",
    "    warmup = 130  # FTI(128) + z-score(20)\n",
    "\n",
    "    # Phase 1: compute target position for each day from signal\n",
    "    # signal[i] -> desired position on day i+1\n",
    "    desired_pos = np.zeros(n, dtype=float)  # +1, -1, or 0\n",
    "    position = 0\n",
    "    entry_day = 0\n",
    "\n",
    "    for i in range(max(warmup, start_idx), min(end_idx, n)):\n",
    "        # Compute signal from day i features\n",
    "        signal = 0.0\n",
    "        for k, w in zip(FEATURE_KEYS, weights):\n",
    "            if abs(w) < 1e-10:\n",
    "                continue\n",
    "            v = z_features[k][i]\n",
    "            if np.isnan(v):\n",
    "                continue\n",
    "            signal += w * v\n",
    "\n",
    "        direction = 1 if signal > 0 else (-1 if signal < 0 else 0)\n",
    "        conviction = abs(signal)\n",
    "\n",
    "        if position != 0:\n",
    "            days_held = i - entry_day\n",
    "            should_exit = days_held >= hold_days\n",
    "            should_exit |= (direction != 0 and direction != position\n",
    "                            and conviction >= entry_threshold)\n",
    "            if should_exit:\n",
    "                position = 0\n",
    "                # Check if we should also enter the opposite direction\n",
    "                if conviction >= entry_threshold and direction != 0:\n",
    "                    position = direction\n",
    "                    entry_day = i\n",
    "            # else: continue holding\n",
    "        else:\n",
    "            if conviction >= entry_threshold and direction != 0:\n",
    "                position = direction\n",
    "                entry_day = i\n",
    "\n",
    "        desired_pos[i] = position\n",
    "\n",
    "    # Phase 2: compute daily PnL stream\n",
    "    # Signal on day i -> position on day i+1 -> earns dr[i] (close[i]->close[i+1])\n",
    "    # So: pos_on_day_t = desired_pos[t-1], return_on_day_t = dr[t-1]\n",
    "    # PnL for day t = desired_pos[t-1] * dr[t-1] - cost_if_flip\n",
    "    daily_pnl = []\n",
    "    trades = []\n",
    "    prev_pos = 0.0\n",
    "\n",
    "    for i in range(max(warmup, start_idx), min(end_idx, n - 1)):\n",
    "        pos = desired_pos[i]       # signal from EOD day i\n",
    "        ret = dr[i]                # return close[i] -> close[i+1]\n",
    "\n",
    "        if np.isnan(ret):\n",
    "            daily_pnl.append(0.0)\n",
    "            prev_pos = pos\n",
    "            continue\n",
    "\n",
    "        pnl = pos * ret\n",
    "\n",
    "        # Apply round-trip cost on position change\n",
    "        if pos != prev_pos:\n",
    "            pnl -= cost_rt\n",
    "            if pos != 0 and prev_pos == 0:\n",
    "                trades.append({\"entry_day\": i, \"direction\": pos, \"rets\": []})\n",
    "            elif pos == 0 and prev_pos != 0:\n",
    "                # closing trade\n",
    "                if trades:\n",
    "                    trades[-1][\"exit_day\"] = i\n",
    "                    trades[-1][\"total_ret\"] = float(np.sum(trades[-1][\"rets\"]) + pnl)\n",
    "\n",
    "        if pos != 0 and trades:\n",
    "            trades[-1][\"rets\"].append(pnl)\n",
    "\n",
    "        daily_pnl.append(pnl)\n",
    "        prev_pos = pos\n",
    "\n",
    "    if not daily_pnl:\n",
    "        return {\"sharpe\": 0.0, \"trades\": 0, \"daily_pnl\": [], \"equity\": np.array([1.0]),\n",
    "                \"total_return\": 0.0, \"max_dd\": 0.0, \"trade_list\": []}\n",
    "\n",
    "    pnl_arr = np.array(daily_pnl)\n",
    "    equity = equity_curve_from_returns(pnl_arr)\n",
    "    sharpe = sharpe_daily(pnl_arr)\n",
    "    mdd = max_drawdown_from_equity(equity)\n",
    "\n",
    "    # Compute trade-level stats\n",
    "    completed_trades = [t for t in trades if \"exit_day\" in t]\n",
    "    wins = sum(1 for t in completed_trades if t.get(\"total_ret\", 0) > 0)\n",
    "\n",
    "    return {\n",
    "        \"sharpe\": sharpe,\n",
    "        \"trades\": len(completed_trades),\n",
    "        \"daily_pnl\": daily_pnl,\n",
    "        \"equity\": equity,\n",
    "        \"total_return\": float(equity[-1] - 1.0),\n",
    "        \"max_dd\": mdd,\n",
    "        \"win_rate\": wins / max(len(completed_trades), 1),\n",
    "        \"trade_list\": completed_trades,\n",
    "    }\n",
    "\n",
    "\n",
    "def walk_forward_optuna(features, daily_close, train_size=190, test_size=63,\n",
    "                        n_trials=200, cost_bps=5.0):\n",
    "    \"\"\"Walk-forward Optuna optimisation. Returns OOS results + best weights.\"\"\"\n",
    "    if not HAS_OPTUNA:\n",
    "        print(\"Optuna not available \u2014 skipping walk-forward\")\n",
    "        return None\n",
    "\n",
    "    n = len(daily_close)\n",
    "    folds = []\n",
    "    start = 0\n",
    "    while start + train_size + test_size <= n:\n",
    "        folds.append((start, start + train_size, start + train_size + test_size))\n",
    "        start += test_size\n",
    "\n",
    "    if not folds:\n",
    "        print(\"Insufficient data for walk-forward\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Walk-forward: {len(folds)} folds, {train_size} train / {test_size} test\")\n",
    "\n",
    "    all_oos_pnl = []\n",
    "    all_oos_trades = []\n",
    "    fold_results = []\n",
    "\n",
    "    for fold_idx, (train_start, train_end, test_end) in enumerate(folds):\n",
    "        t0 = time.time()\n",
    "\n",
    "        def objective(trial):\n",
    "            weights = []\n",
    "            for k in FEATURE_KEYS:\n",
    "                w = trial.suggest_float(f\"w_{k}\", -1.0, 1.0)\n",
    "                weights.append(w)\n",
    "            threshold = trial.suggest_float(\"threshold\", 0.1, 1.0)\n",
    "            hold = trial.suggest_int(\"hold_days\", 2, 10)\n",
    "\n",
    "            result = backtest_signal(\n",
    "                features, daily_close, weights,\n",
    "                entry_threshold=threshold, hold_days=hold,\n",
    "                cost_bps=cost_bps, start_idx=train_start, end_idx=train_end,\n",
    "            )\n",
    "            if result[\"trades\"] < 3:\n",
    "                return -10.0\n",
    "            return result[\"sharpe\"]\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\",\n",
    "                                     sampler=optuna.samplers.TPESampler(seed=42 + fold_idx))\n",
    "        study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "\n",
    "        # Extract best params\n",
    "        best = study.best_params\n",
    "        best_weights = [best[f\"w_{k}\"] for k in FEATURE_KEYS]\n",
    "        best_threshold = best[\"threshold\"]\n",
    "        best_hold = best[\"hold_days\"]\n",
    "\n",
    "        # In-sample result\n",
    "        is_result = backtest_signal(\n",
    "            features, daily_close, best_weights,\n",
    "            entry_threshold=best_threshold, hold_days=best_hold,\n",
    "            cost_bps=cost_bps, start_idx=train_start, end_idx=train_end,\n",
    "        )\n",
    "\n",
    "        # OOS result (NO re-optimisation)\n",
    "        oos_result = backtest_signal(\n",
    "            features, daily_close, best_weights,\n",
    "            entry_threshold=best_threshold, hold_days=best_hold,\n",
    "            cost_bps=cost_bps, start_idx=train_end, end_idx=test_end,\n",
    "        )\n",
    "\n",
    "        all_oos_pnl.extend(oos_result[\"daily_pnl\"])\n",
    "        all_oos_trades.extend(oos_result.get(\"trade_list\", []))\n",
    "\n",
    "        elapsed = time.time() - t0\n",
    "        fold_results.append({\n",
    "            \"fold\": fold_idx, \"is_sharpe\": is_result[\"sharpe\"],\n",
    "            \"oos_sharpe\": oos_result[\"sharpe\"],\n",
    "            \"is_trades\": is_result[\"trades\"], \"oos_trades\": oos_result[\"trades\"],\n",
    "        })\n",
    "        print(f\"  Fold {fold_idx}: IS Sharpe={is_result['sharpe']:.2f} \"\n",
    "              f\"({is_result['trades']} trades) -> OOS Sharpe={oos_result['sharpe']:.2f} \"\n",
    "              f\"({oos_result['trades']} trades) [{elapsed:.1f}s]\")\n",
    "\n",
    "    # Aggregate OOS\n",
    "    oos_pnl = np.array(all_oos_pnl)\n",
    "    oos_eq = equity_curve_from_returns(oos_pnl)\n",
    "    oos_sharpe = sharpe_daily(oos_pnl)\n",
    "\n",
    "    print(f\"\\n  Aggregate OOS: Sharpe={oos_sharpe:.2f}, \"\n",
    "          f\"Trades={len(all_oos_trades)}, \"\n",
    "          f\"Return={float(oos_eq[-1] - 1.0)*100:.2f}%\")\n",
    "\n",
    "    # Return last fold's best weights for option backtest\n",
    "    return {\n",
    "        \"weights\": best_weights, \"threshold\": best_threshold,\n",
    "        \"hold_days\": best_hold, \"fold_results\": fold_results,\n",
    "        \"oos_sharpe\": oos_sharpe, \"oos_pnl\": all_oos_pnl,\n",
    "        \"oos_trades\": all_oos_trades,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run walk-forward for lead symbol\n",
    "sym = SYMBOLS[0]\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Walk-Forward Optimisation: {sym}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "wf_result = walk_forward_optuna(\n",
    "    all_features[sym], all_features[sym][\"daily_close\"],\n",
    "    train_size=190, test_size=63, n_trials=200,\n",
    "    cost_bps=COST_BPS_FUTURES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21050a8c",
   "metadata": {},
   "source": [
    "<a id=\"21\"></a>\n",
    "## 21. Option Execution Mathematics\n",
    "\n",
    "### ATM Option Selection\n",
    "- Strike: `round(spot / step) * step` (NIFTY: 50pt, BANKNIFTY: 100pt)\n",
    "- Nearest expiry: `min(expiry) WHERE expiry > date`\n",
    "- Long signal \u2192 buy ATM CE; Short signal \u2192 buy ATM PE\n",
    "\n",
    "### Cost Model\n",
    "- ATM index options: **10 bps** roundtrip (liquid)\n",
    "- OTM options: 15-20 bps (wider spreads)\n",
    "\n",
    "### PnL Calculation\n",
    "$$\\text{PnL} = \\frac{\\text{exit\\_premium} - \\text{entry\\_premium}}{\\text{entry\\_premium}} - \\text{cost}$$\n",
    "\n",
    "### Exit Rules\n",
    "1. **Target**: premium \u00d7 target_mult\n",
    "2. **Stop**: premium \u00d7 stop_mult (loss limit)\n",
    "3. **Time-stop**: last 15 bars of day\n",
    "4. **Signal flip**: opposite signal with sufficient conviction\n",
    "5. **EOD**: forced exit at last bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db99efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 22: Option executor \u2014 T+1 causal, actual option prices from nfo_1min.\n",
    "\n",
    "CORRECTED:\n",
    "  - Signal computed at EOD day i -> trade enters on day i+1 at bar 30\n",
    "  - ATM strike selected using spot at bar 30 on day i+1 (NOT day-i close)\n",
    "  - Round-trip cost (entry + exit)\n",
    "  - Multiplicative equity curve\n",
    "  - Option loss capped at -100% (premium paid)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptionTrade:\n",
    "    signal_date: str          # day signal was computed (day i)\n",
    "    trade_date: str           # day trade executes (day i+1)\n",
    "    entry_bar: int\n",
    "    entry_price: float        # option premium at entry\n",
    "    direction: str            # \"long_ce\" or \"long_pe\"\n",
    "    strike: float\n",
    "    expiry: str\n",
    "    exit_date: str = \"\"\n",
    "    exit_bar: int = -1\n",
    "    exit_price: float = 0.0   # option premium at exit\n",
    "    exit_reason: str = \"\"\n",
    "    pnl: float = 0.0          # net return after round-trip cost\n",
    "\n",
    "\n",
    "def get_spot_at_bar(futures_bars, d, bar_idx=30):\n",
    "    \"\"\"Get underlying futures close at a specific 1-min bar on a date.\n",
    "\n",
    "    Uses pre-loaded futures_bars dict (already sorted by time).\n",
    "    \"\"\"\n",
    "    if d not in futures_bars:\n",
    "        return None\n",
    "    closes = futures_bars[d][\"close\"]\n",
    "    if len(closes) <= bar_idx:\n",
    "        return None\n",
    "    return float(closes[bar_idx])\n",
    "\n",
    "\n",
    "def select_atm_option(con, symbol, d, spot, direction):\n",
    "    \"\"\"Select ATM CE (long) or PE (short) from nfo_1min.\n",
    "\n",
    "    Returns (strike, expiry, inst_type) or None.\n",
    "    \"\"\"\n",
    "    step = STRIKE_STEP.get(symbol, 50)\n",
    "    atm_strike = round(spot / step) * step\n",
    "    inst_type = \"CE\" if direction == \"long\" else \"PE\"\n",
    "\n",
    "    # Get nearest expiry\n",
    "    expiry_df = con.execute(\n",
    "        \"SELECT DISTINCT expiry FROM nfo_1min \"\n",
    "        \"WHERE name = ? AND date = ? AND instrument_type = ? AND strike = ? \"\n",
    "        \"ORDER BY expiry LIMIT 1\",\n",
    "        [symbol, d, inst_type, atm_strike]\n",
    "    ).fetchdf()\n",
    "\n",
    "    if expiry_df.empty:\n",
    "        for offset in [step, -step, 2*step, -2*step]:\n",
    "            expiry_df = con.execute(\n",
    "                \"SELECT DISTINCT expiry FROM nfo_1min \"\n",
    "                \"WHERE name = ? AND date = ? AND instrument_type = ? AND strike = ? \"\n",
    "                \"ORDER BY expiry LIMIT 1\",\n",
    "                [symbol, d, inst_type, atm_strike + offset]\n",
    "            ).fetchdf()\n",
    "            if not expiry_df.empty:\n",
    "                atm_strike += offset\n",
    "                break\n",
    "\n",
    "    if expiry_df.empty:\n",
    "        return None\n",
    "\n",
    "    expiry = str(expiry_df[\"expiry\"].iloc[0])\n",
    "    return atm_strike, expiry, inst_type\n",
    "\n",
    "\n",
    "def get_option_bars(con, symbol, d, strike, expiry, inst_type):\n",
    "    \"\"\"Get all 1-min close prices for an option on a date.\"\"\"\n",
    "    df = con.execute(\n",
    "        \"SELECT close FROM nfo_1min \"\n",
    "        \"WHERE name = ? AND date = ? AND strike = ? AND expiry = ? \"\n",
    "        \"AND instrument_type = ?\",\n",
    "        [symbol, d, strike, expiry, inst_type]\n",
    "    ).fetchdf()\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    return df[\"close\"].values.astype(np.float64)\n",
    "\n",
    "\n",
    "def run_option_backtest(con, symbol, features, dates, futures_bars,\n",
    "                        weights, entry_threshold=0.3, hold_days=5,\n",
    "                        cost_bps=10.0, entry_bar=30):\n",
    "    \"\"\"Full T+1 option backtest with actual nfo_1min prices.\n",
    "\n",
    "    Causal execution protocol:\n",
    "      1. Signal computed at EOD of day i using z_features[k][i]\n",
    "      2. Trade enters on day i+1 at bar entry_bar\n",
    "      3. ATM strike selected using spot at entry_bar on day i+1\n",
    "      4. Exit after hold_days or on signal flip (at entry_bar on exit day)\n",
    "      5. Round-trip cost applied per trade\n",
    "      6. Option loss capped at -100% of premium\n",
    "    \"\"\"\n",
    "    n = len(dates)\n",
    "    daily_close = features[\"daily_close\"]\n",
    "    cost_rt = cost_bps / 1e4  # round-trip cost\n",
    "    warmup = 130\n",
    "\n",
    "    z_features = {k: _zscore(features[k]) for k in FEATURE_KEYS}\n",
    "\n",
    "    trades = []\n",
    "    # Daily return stream aligned to dates (nonzero only on trade exit day)\n",
    "    daily_rets = np.zeros(n, dtype=float)\n",
    "    position = 0       # 0=flat, 1=long_ce, -1=long_pe\n",
    "    signal_day = 0     # day index when signal was computed\n",
    "    active_trade = None\n",
    "\n",
    "    for i in range(warmup, n - 1):\n",
    "        # Compute signal from EOD day i features\n",
    "        signal = 0.0\n",
    "        for k, w in zip(FEATURE_KEYS, weights):\n",
    "            if abs(w) < 1e-10:\n",
    "                continue\n",
    "            v = z_features[k][i]\n",
    "            if np.isnan(v):\n",
    "                continue\n",
    "            signal += w * v\n",
    "\n",
    "        direction_int = 1 if signal > 0 else (-1 if signal < 0 else 0)\n",
    "        conviction = abs(signal)\n",
    "\n",
    "        # \u2500\u2500 Exit logic (if in position) \u2500\u2500\n",
    "        # Check if we should exit on day i+1 (the trade day after signal day)\n",
    "        if position != 0 and active_trade is not None:\n",
    "            days_held = i - signal_day\n",
    "            should_exit = days_held >= hold_days\n",
    "            should_exit |= (direction_int != 0 and direction_int != position\n",
    "                            and conviction >= entry_threshold)\n",
    "            if should_exit:\n",
    "                d_exit = dates[i + 1]  # exit on next trading day\n",
    "                if d_exit not in futures_bars:\n",
    "                    continue  # can't exit today, try next day\n",
    "                inst_type = \"CE\" if position == 1 else \"PE\"\n",
    "                opt_bars = get_option_bars(\n",
    "                    con, symbol, d_exit, active_trade.strike,\n",
    "                    active_trade.expiry, inst_type\n",
    "                )\n",
    "                if opt_bars is not None and len(opt_bars) > entry_bar:\n",
    "                    exit_price = opt_bars[entry_bar]\n",
    "                    raw_ret = (exit_price - active_trade.entry_price) / max(active_trade.entry_price, 1e-8)\n",
    "                    net_ret = _apply_roundtrip_cost(raw_ret, cost_bps)\n",
    "                    # Cap option loss at -100% of premium\n",
    "                    net_ret = max(net_ret, -1.0)\n",
    "\n",
    "                    active_trade.exit_date = d_exit\n",
    "                    active_trade.exit_bar = entry_bar\n",
    "                    active_trade.exit_price = exit_price\n",
    "                    active_trade.exit_reason = \"hold\" if days_held >= hold_days else \"flip\"\n",
    "                    active_trade.pnl = net_ret\n",
    "                    trades.append(active_trade)\n",
    "                    daily_rets[i + 1] = net_ret\n",
    "                # else: option expired/no data, mark as total loss\n",
    "                elif active_trade is not None:\n",
    "                    active_trade.exit_date = d_exit\n",
    "                    active_trade.exit_reason = \"no_data\"\n",
    "                    active_trade.pnl = _apply_roundtrip_cost(-1.0, cost_bps)\n",
    "                    trades.append(active_trade)\n",
    "                    daily_rets[i + 1] = active_trade.pnl\n",
    "\n",
    "                position = 0\n",
    "                active_trade = None\n",
    "\n",
    "        # \u2500\u2500 Entry logic (if flat, signal day i -> enter day i+1) \u2500\u2500\n",
    "        if position == 0 and conviction >= entry_threshold and direction_int != 0:\n",
    "            d_trade = dates[i + 1]  # enter on NEXT trading day\n",
    "            if d_trade not in futures_bars:\n",
    "                continue\n",
    "\n",
    "            # Get spot at entry_bar on trade day (NOT day-i close)\n",
    "            spot_entry = get_spot_at_bar(futures_bars, d_trade, entry_bar)\n",
    "            if spot_entry is None or spot_entry <= 0:\n",
    "                continue\n",
    "\n",
    "            direction_str = \"long\" if direction_int == 1 else \"short\"\n",
    "            opt_info = select_atm_option(con, symbol, d_trade, spot_entry, direction_str)\n",
    "            if opt_info is None:\n",
    "                continue\n",
    "\n",
    "            strike, expiry, inst_type = opt_info\n",
    "            opt_bars = get_option_bars(con, symbol, d_trade, strike, expiry, inst_type)\n",
    "            if opt_bars is None or len(opt_bars) <= entry_bar:\n",
    "                continue\n",
    "\n",
    "            entry_price = opt_bars[entry_bar]\n",
    "            if entry_price <= 0:\n",
    "                continue\n",
    "\n",
    "            position = direction_int\n",
    "            signal_day = i\n",
    "            active_trade = OptionTrade(\n",
    "                signal_date=dates[i], trade_date=d_trade,\n",
    "                entry_bar=entry_bar, entry_price=entry_price,\n",
    "                direction=f\"long_{inst_type.lower()}\", strike=strike, expiry=expiry,\n",
    "            )\n",
    "\n",
    "    # Close any remaining position at end\n",
    "    if position != 0 and active_trade is not None:\n",
    "        active_trade.exit_reason = \"eod_final\"\n",
    "        active_trade.pnl = _apply_roundtrip_cost(-1.0, cost_bps)  # conservative: total loss\n",
    "        trades.append(active_trade)\n",
    "\n",
    "    if not trades:\n",
    "        return {\"trades\": [], \"daily_pnl\": daily_rets.tolist(), \"sharpe\": 0.0,\n",
    "                \"equity\": np.array([1.0]), \"total_return\": 0.0, \"max_dd\": 0.0,\n",
    "                \"win_rate\": 0.0, \"n_trades\": 0}\n",
    "\n",
    "    # Only use days with actual activity for Sharpe (sparse signal)\n",
    "    pnl_arr = daily_rets[warmup:]\n",
    "    equity = equity_curve_from_returns(pnl_arr)\n",
    "    sharpe = sharpe_daily(pnl_arr)\n",
    "    mdd = max_drawdown_from_equity(equity)\n",
    "    total_return = float(equity[-1] - 1.0)\n",
    "\n",
    "    wins = sum(1 for t in trades if t.pnl > 0)\n",
    "\n",
    "    return {\n",
    "        \"trades\": trades,\n",
    "        \"daily_pnl\": pnl_arr.tolist(),\n",
    "        \"sharpe\": sharpe,\n",
    "        \"equity\": equity,\n",
    "        \"total_return\": total_return,\n",
    "        \"max_dd\": mdd,\n",
    "        \"win_rate\": wins / max(len(trades), 1),\n",
    "        \"n_trades\": len(trades),\n",
    "    }\n",
    "\n",
    "print(\"Option executor ready (T+1 causal, round-trip costs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa00ea7",
   "metadata": {},
   "source": [
    "<a id=\"23\"></a>\n",
    "## 23. Run Full Backtest\n",
    "\n",
    "Run both futures and options backtests with walk-forward-validated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e417a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 23: Run full backtest \u2014 futures vs options comparison.\n",
    "\n",
    "CORRECTED:\n",
    "  - Uses compounded equity curves for returns and drawdowns\n",
    "  - Consistent Sharpe calculation via sharpe_daily()\n",
    "  - Futures drawdown from equity curve (not cumsum)\n",
    "\"\"\"\n",
    "\n",
    "sym = SYMBOLS[0]\n",
    "features = all_features[sym]\n",
    "dates_for_sym = sorted(con.execute(\n",
    "    \"SELECT DISTINCT CAST(date AS VARCHAR) AS date FROM nfo_1min \"\n",
    "    \"WHERE name = ? AND instrument_type = 'FUT' ORDER BY date\", [sym]\n",
    ").fetchdf()[\"date\"].tolist())\n",
    "\n",
    "# Use walk-forward best weights, or default equal weights if WF failed\n",
    "if wf_result is not None:\n",
    "    best_weights = wf_result[\"weights\"]\n",
    "    best_threshold = wf_result[\"threshold\"]\n",
    "    best_hold = wf_result[\"hold_days\"]\n",
    "    print(f\"Using walk-forward optimised params (OOS Sharpe: {wf_result['oos_sharpe']:.2f})\")\n",
    "else:\n",
    "    best_weights = [0.1] * len(FEATURE_KEYS)\n",
    "    best_threshold = 0.3\n",
    "    best_hold = 5\n",
    "    print(\"Using default equal weights (no Optuna)\")\n",
    "\n",
    "# \u2500\u2500 Futures backtest \u2500\u2500\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FUTURES BACKTEST: {sym}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "fut_result = backtest_signal(\n",
    "    features, features[\"daily_close\"], best_weights,\n",
    "    entry_threshold=best_threshold, hold_days=best_hold,\n",
    "    cost_bps=COST_BPS_FUTURES,\n",
    ")\n",
    "print(f\"  Trades:   {fut_result['trades']}\")\n",
    "print(f\"  Return:   {fut_result['total_return']*100:+.2f}%\")\n",
    "print(f\"  Sharpe:   {fut_result['sharpe']:.2f}\")\n",
    "print(f\"  Max DD:   {fut_result['max_dd']*100:.2f}%\")\n",
    "print(f\"  Win Rate: {fut_result['win_rate']*100:.1f}%\")\n",
    "\n",
    "# \u2500\u2500 Options backtest \u2500\u2500\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"OPTIONS BACKTEST: {sym} (T+1 causal)\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "opt_result = run_option_backtest(\n",
    "    con, sym, features, dates_for_sym, all_futures_bars[sym],\n",
    "    weights=best_weights, entry_threshold=best_threshold,\n",
    "    hold_days=best_hold, cost_bps=COST_BPS_OPTIONS,\n",
    ")\n",
    "print(f\"  Trades:   {opt_result['n_trades']}\")\n",
    "print(f\"  Return:   {opt_result['total_return']*100:+.2f}%\")\n",
    "print(f\"  Sharpe:   {opt_result['sharpe']:.2f}\")\n",
    "print(f\"  Max DD:   {opt_result['max_dd']*100:.2f}%\")\n",
    "print(f\"  Win Rate: {opt_result['win_rate']*100:.1f}%\")\n",
    "\n",
    "# \u2500\u2500 OOS from walk-forward \u2500\u2500\n",
    "if wf_result:\n",
    "    oos_pnl = np.array(wf_result[\"oos_pnl\"])\n",
    "    oos_eq = equity_curve_from_returns(oos_pnl)\n",
    "    oos_mdd = max_drawdown_from_equity(oos_eq)\n",
    "\n",
    "# \u2500\u2500 Comparison table \u2500\u2500\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"{'Variant':<16} {'Trades':>7} {'Return':>9} {'Sharpe':>7} {'MaxDD':>7} {'WinRate':>8}\")\n",
    "print(f\"{'-'*60}\")\n",
    "\n",
    "print(f\"{'Futures':<16} {fut_result['trades']:>7} \"\n",
    "      f\"{fut_result['total_return']*100:>+8.2f}% \"\n",
    "      f\"{fut_result['sharpe']:>7.2f} \"\n",
    "      f\"{fut_result['max_dd']*100:>6.2f}% \"\n",
    "      f\"{fut_result['win_rate']*100:>7.1f}%\")\n",
    "\n",
    "print(f\"{'Options (T+1)':<16} {opt_result['n_trades']:>7} \"\n",
    "      f\"{opt_result['total_return']*100:>+8.2f}% \"\n",
    "      f\"{opt_result['sharpe']:>7.2f} \"\n",
    "      f\"{opt_result['max_dd']*100:>6.2f}% \"\n",
    "      f\"{opt_result['win_rate']*100:>7.1f}%\")\n",
    "\n",
    "if wf_result:\n",
    "    print(f\"{'OOS (WF)':<16} {len(wf_result['oos_trades']):>7} \"\n",
    "          f\"{float(oos_eq[-1] - 1.0)*100:>+8.2f}% \"\n",
    "          f\"{wf_result['oos_sharpe']:>7.2f} \"\n",
    "          f\"{oos_mdd*100:>6.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fdb296",
   "metadata": {},
   "source": [
    "<a id=\"24\"></a>\n",
    "## 24. Equity Curves\n",
    "\n",
    "Side-by-side: futures vs options execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836900b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 24: Equity curves & drawdown charts \u2014 CORRECTED (multiplicative).\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# \u2500\u2500 Equity curves (multiplicative) \u2500\u2500\n",
    "ax = axes[0]\n",
    "eq_fut = fut_result[\"equity\"]\n",
    "eq_opt = opt_result[\"equity\"]\n",
    "\n",
    "# Plot as % return from initial equity\n",
    "ax.plot((eq_fut - 1) * 100, linewidth=1.2, color=\"steelblue\",\n",
    "        label=f\"Futures (Sharpe {fut_result['sharpe']:.2f})\")\n",
    "ax.plot((eq_opt - 1) * 100, linewidth=1.2, color=\"crimson\",\n",
    "        label=f\"Options T+1 (Sharpe {opt_result['sharpe']:.2f})\")\n",
    "if wf_result:\n",
    "    oos_eq = equity_curve_from_returns(np.array(wf_result[\"oos_pnl\"]))\n",
    "    ax.plot((oos_eq - 1) * 100, linewidth=1.2, color=\"forestgreen\", linestyle=\"--\",\n",
    "            label=f\"OOS Walk-Forward (Sharpe {wf_result['oos_sharpe']:.2f})\")\n",
    "ax.legend(fontsize=9)\n",
    "ax.set_ylabel(\"Cumulative Return (%)\")\n",
    "ax.set_title(f\"{sym} \u2014 S12 Vedic Fractional Alpha: Equity Curves (Compounded)\")\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# \u2500\u2500 Drawdown (from equity peak) \u2500\u2500\n",
    "ax = axes[1]\n",
    "peak_fut = np.maximum.accumulate(eq_fut)\n",
    "dd_fut = (eq_fut - peak_fut) / np.where(peak_fut > 0, peak_fut, 1.0) * 100\n",
    "peak_opt = np.maximum.accumulate(eq_opt)\n",
    "dd_opt = (eq_opt - peak_opt) / np.where(peak_opt > 0, peak_opt, 1.0) * 100\n",
    "\n",
    "ax.fill_between(range(len(dd_fut)), dd_fut, 0, alpha=0.3, color=\"steelblue\", label=\"Futures DD\")\n",
    "ax.fill_between(range(len(dd_opt)), dd_opt, 0, alpha=0.3, color=\"crimson\", label=\"Options DD\")\n",
    "ax.legend(fontsize=9)\n",
    "ax.set_ylabel(\"Drawdown (%)\")\n",
    "ax.set_xlabel(\"Trading Day\")\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Monthly return heatmap (options)\n",
    "if opt_result[\"trades\"]:\n",
    "    trade_dates = [t.trade_date for t in opt_result[\"trades\"]]\n",
    "    trade_pnls = [t.pnl for t in opt_result[\"trades\"]]\n",
    "\n",
    "    # Group by month\n",
    "    monthly = {}\n",
    "    for d, p in zip(trade_dates, trade_pnls):\n",
    "        ym = str(d)[:7]\n",
    "        monthly[ym] = monthly.get(ym, 0) + p\n",
    "\n",
    "    if monthly:\n",
    "        months = sorted(monthly.keys())\n",
    "        returns = [monthly[m] * 100 for m in months]\n",
    "        colors = [\"green\" if r > 0 else \"red\" for r in returns]\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(14, 4))\n",
    "        ax.bar(range(len(months)), returns, color=colors, alpha=0.7)\n",
    "        ax.set_xticks(range(len(months)))\n",
    "        ax.set_xticklabels(months, rotation=45, fontsize=7)\n",
    "        ax.set_ylabel(\"Return (%)\")\n",
    "        ax.set_title(f\"{sym} \u2014 Monthly Option Returns\")\n",
    "        ax.axhline(0, color=\"black\", linewidth=0.5)\n",
    "        ax.grid(alpha=0.3, axis=\"y\")\n",
    "        plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56e3fe7",
   "metadata": {},
   "source": [
    "<a id=\"25\"></a>\n",
    "## 25. Trade-Level Analysis\n",
    "\n",
    "Individual trade details, PnL distribution, hold-time analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b875f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 25: Trade-level analysis.\"\"\"\n",
    "\n",
    "trades = opt_result[\"trades\"]\n",
    "if not trades:\n",
    "    print(\"No option trades to analyse\")\n",
    "else:\n",
    "    # Trade table\n",
    "    print(f\"{'Date':<12} {'Direction':<12} {'Strike':>8} {'Entry':>8} {'Exit':>8} \"\n",
    "          f\"{'PnL%':>7} {'Reason':<8}\")\n",
    "    print(\"-\" * 70)\n",
    "    for t in trades[:30]:  # Show first 30\n",
    "        print(f\"{t.trade_date:<12} {t.direction:<12} {t.strike:>8.0f} \"\n",
    "              f\"{t.entry_price:>8.1f} {t.exit_price:>8.1f} \"\n",
    "              f\"{t.pnl*100:>+6.2f}% {t.exit_reason:<8}\")\n",
    "    if len(trades) > 30:\n",
    "        print(f\"  ... ({len(trades) - 30} more trades)\")\n",
    "\n",
    "    # PnL histogram\n",
    "    pnls = [t.pnl * 100 for t in trades]\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.hist(pnls, bins=30, color=\"steelblue\", alpha=0.7, edgecolor=\"white\")\n",
    "    ax.axvline(0, color=\"red\", linewidth=1, linestyle=\"--\")\n",
    "    ax.set_xlabel(\"PnL (%)\"); ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Trade PnL Distribution\")\n",
    "\n",
    "    # Exit reason breakdown\n",
    "    ax = axes[1]\n",
    "    reasons = {}\n",
    "    for t in trades:\n",
    "        reasons[t.exit_reason] = reasons.get(t.exit_reason, 0) + 1\n",
    "    labels = list(reasons.keys())\n",
    "    sizes = list(reasons.values())\n",
    "    ax.pie(sizes, labels=labels, autopct=\"%1.0f%%\", startangle=90)\n",
    "    ax.set_title(\"Exit Reason Breakdown\")\n",
    "\n",
    "    # Hold time distribution\n",
    "    ax = axes[2]\n",
    "    hold_times = [t.exit_bar - t.entry_bar for t in trades]\n",
    "    ax.hist(hold_times, bins=20, color=\"teal\", alpha=0.7, edgecolor=\"white\")\n",
    "    ax.set_xlabel(\"Hold Time (bars)\"); ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"Hold Time Distribution\")\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Direction breakdown\n",
    "    long_trades = [t for t in trades if \"call\" in t.direction]\n",
    "    short_trades = [t for t in trades if \"put\" in t.direction]\n",
    "    print(f\"\\nDirection Breakdown:\")\n",
    "    print(f\"  Long (CE):  {len(long_trades)} trades, \"\n",
    "          f\"avg PnL={np.mean([t.pnl for t in long_trades])*100:+.2f}%\" if long_trades else \"  Long: 0\")\n",
    "    print(f\"  Short (PE): {len(short_trades)} trades, \"\n",
    "          f\"avg PnL={np.mean([t.pnl for t in short_trades])*100:+.2f}%\" if short_trades else \"  Short: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313d2171",
   "metadata": {},
   "source": [
    "<a id=\"26\"></a>\n",
    "## 26. Validation Gates\n",
    "\n",
    "Two statistical tests to verify signal quality:\n",
    "\n",
    "1. **Placebo test**: Shuffle signal dates, run 100 permutations. Real must beat 90th percentile.\n",
    "2. **Time-shift test**: Shift signals \u00b11,\u00b12,\u00b13 days. Original must beat all shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e08f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 27: Validation \u2014 placebo + time-shift tests.\"\"\"\n",
    "\n",
    "def placebo_test(features, daily_close, weights, n_perms=100,\n",
    "                 entry_threshold=0.3, hold_days=5, cost_bps=5.0):\n",
    "    \"\"\"Shuffle signal dates and run backtests. Real must beat 90th percentile.\"\"\"\n",
    "    # Real backtest\n",
    "    real = backtest_signal(features, daily_close, weights,\n",
    "                           entry_threshold=entry_threshold, hold_days=hold_days,\n",
    "                           cost_bps=cost_bps)\n",
    "    real_sharpe = real[\"sharpe\"]\n",
    "\n",
    "    # Permutation tests\n",
    "    perm_sharpes = []\n",
    "    for p in range(n_perms):\n",
    "        # Shuffle weights randomly\n",
    "        perm_weights = [w * np.random.choice([-1, 1]) * np.random.uniform(0.5, 1.5)\n",
    "                        for w in weights]\n",
    "        np.random.shuffle(perm_weights)\n",
    "        result = backtest_signal(features, daily_close, perm_weights,\n",
    "                                  entry_threshold=entry_threshold, hold_days=hold_days,\n",
    "                                  cost_bps=cost_bps)\n",
    "        perm_sharpes.append(result[\"sharpe\"])\n",
    "\n",
    "    perm_sharpes = np.array(perm_sharpes)\n",
    "    p90 = np.percentile(perm_sharpes, 90)\n",
    "    passed = real_sharpe > p90\n",
    "\n",
    "    return {\n",
    "        \"real_sharpe\": real_sharpe,\n",
    "        \"perm_mean\": float(np.mean(perm_sharpes)),\n",
    "        \"perm_p90\": float(p90),\n",
    "        \"passed\": passed,\n",
    "        \"perm_sharpes\": perm_sharpes,\n",
    "    }\n",
    "\n",
    "\n",
    "def time_shift_test(features, daily_close, weights, shifts=[-3,-2,-1,1,2,3],\n",
    "                     entry_threshold=0.3, hold_days=5, cost_bps=5.0):\n",
    "    \"\"\"Shift feature arrays by N days. Original must beat all shifts.\"\"\"\n",
    "    n = len(daily_close)\n",
    "\n",
    "    # Real backtest\n",
    "    real = backtest_signal(features, daily_close, weights,\n",
    "                           entry_threshold=entry_threshold, hold_days=hold_days,\n",
    "                           cost_bps=cost_bps)\n",
    "    real_sharpe = real[\"sharpe\"]\n",
    "\n",
    "    shift_results = {}\n",
    "    for s in shifts:\n",
    "        shifted_features = {}\n",
    "        for k in FEATURE_KEYS:\n",
    "            arr = features[k].copy()\n",
    "            if s > 0:\n",
    "                shifted_features[k] = np.concatenate([np.full(s, np.nan), arr[:-s]])\n",
    "            else:\n",
    "                shifted_features[k] = np.concatenate([arr[-s:], np.full(-s, np.nan)])\n",
    "\n",
    "        # Also need daily_close and log_ret unshifted\n",
    "        shifted_features[\"daily_close\"] = features[\"daily_close\"]\n",
    "        shifted_features[\"log_ret\"] = features[\"log_ret\"]\n",
    "\n",
    "        result = backtest_signal(shifted_features, daily_close, weights,\n",
    "                                  entry_threshold=entry_threshold, hold_days=hold_days,\n",
    "                                  cost_bps=cost_bps)\n",
    "        shift_results[s] = result[\"sharpe\"]\n",
    "\n",
    "    beats_all = all(real_sharpe > s_sharpe for s_sharpe in shift_results.values())\n",
    "\n",
    "    return {\n",
    "        \"real_sharpe\": real_sharpe,\n",
    "        \"shift_sharpes\": shift_results,\n",
    "        \"beats_all\": beats_all,\n",
    "    }\n",
    "\n",
    "\n",
    "# Run validation\n",
    "sym = SYMBOLS[0]\n",
    "if wf_result is not None:\n",
    "    weights = wf_result[\"weights\"]\n",
    "    threshold = wf_result[\"threshold\"]\n",
    "    hold = wf_result[\"hold_days\"]\n",
    "else:\n",
    "    weights = [0.1] * len(FEATURE_KEYS)\n",
    "    threshold = 0.3\n",
    "    hold = 5\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"VALIDATION GATES: {sym}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Placebo test\n",
    "print(\"\\n1. Placebo Test (100 permutations):\")\n",
    "placebo = placebo_test(\n",
    "    all_features[sym], all_features[sym][\"daily_close\"],\n",
    "    weights, n_perms=100, entry_threshold=threshold,\n",
    "    hold_days=hold, cost_bps=COST_BPS_FUTURES,\n",
    ")\n",
    "status = \"PASS\" if placebo[\"passed\"] else \"FAIL\"\n",
    "print(f\"   Real Sharpe:  {placebo['real_sharpe']:.2f}\")\n",
    "print(f\"   Perm mean:    {placebo['perm_mean']:.2f}\")\n",
    "print(f\"   Perm 90th %%:  {placebo['perm_p90']:.2f}\")\n",
    "print(f\"   Result:       {status}\")\n",
    "\n",
    "# Time-shift test\n",
    "print(\"\\n2. Time-Shift Test:\")\n",
    "tshift = time_shift_test(\n",
    "    all_features[sym], all_features[sym][\"daily_close\"],\n",
    "    weights, entry_threshold=threshold, hold_days=hold,\n",
    "    cost_bps=COST_BPS_FUTURES,\n",
    ")\n",
    "print(f\"   Real Sharpe: {tshift['real_sharpe']:.2f}\")\n",
    "for s, sharpe in sorted(tshift[\"shift_sharpes\"].items()):\n",
    "    marker = \"<\" if sharpe < tshift[\"real_sharpe\"] else \">\"\n",
    "    print(f\"   Shift {s:+d}: Sharpe={sharpe:.2f} {marker}\")\n",
    "status = \"PASS\" if tshift[\"beats_all\"] else \"FAIL\"\n",
    "print(f\"   Result: {status}\")\n",
    "\n",
    "# Visualise placebo distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.hist(placebo[\"perm_sharpes\"], bins=25, color=\"gray\", alpha=0.6, label=\"Permutations\")\n",
    "ax.axvline(placebo[\"real_sharpe\"], color=\"red\", linewidth=2, label=f\"Real ({placebo['real_sharpe']:.2f})\")\n",
    "ax.axvline(placebo[\"perm_p90\"], color=\"orange\", linewidth=1.5, linestyle=\"--\",\n",
    "           label=f\"90th %ile ({placebo['perm_p90']:.2f})\")\n",
    "ax.legend(); ax.set_xlabel(\"Sharpe Ratio\"); ax.set_ylabel(\"Count\")\n",
    "ax.set_title(f\"{sym} \u2014 Placebo Test Distribution\")\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca285f",
   "metadata": {},
   "source": [
    "<a id=\"28\"></a>\n",
    "## 28. Feature Importance\n",
    "\n",
    "fANOVA-style importance from Optuna + weight magnitude analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8ed1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cell 28: Feature importance analysis.\"\"\"\n",
    "\n",
    "if wf_result is not None:\n",
    "    weights = wf_result[\"weights\"]\n",
    "\n",
    "    # Weight magnitude as importance proxy\n",
    "    abs_weights = np.abs(weights)\n",
    "    total_abs = np.sum(abs_weights)\n",
    "    importance = abs_weights / total_abs if total_abs > 0 else abs_weights\n",
    "\n",
    "    # Sort by importance\n",
    "    sorted_idx = np.argsort(importance)[::-1]\n",
    "\n",
    "    print(f\"{'Rank':>4} {'Feature':<24} {'Weight':>8} {'Importance':>10}\")\n",
    "    print(\"-\" * 50)\n",
    "    for rank, idx in enumerate(sorted_idx[:15]):\n",
    "        print(f\"{rank+1:>4} {FEATURE_KEYS[idx]:<24} {weights[idx]:>+8.3f} \"\n",
    "              f\"{importance[idx]*100:>9.1f}%\")\n",
    "\n",
    "    # Bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    top_k = min(15, len(FEATURE_KEYS))\n",
    "    top_idx = sorted_idx[:top_k]\n",
    "    top_names = [FEATURE_KEYS[i] for i in top_idx]\n",
    "    top_imp = [importance[i] * 100 for i in top_idx]\n",
    "    colors = [\"crimson\" if weights[i] < 0 else \"steelblue\" for i in top_idx]\n",
    "\n",
    "    ax.barh(range(top_k), top_imp, color=colors, alpha=0.7)\n",
    "    ax.set_yticks(range(top_k))\n",
    "    ax.set_yticklabels(top_names, fontsize=8)\n",
    "    ax.set_xlabel(\"Relative Importance (%)\")\n",
    "    ax.set_title(f\"{SYMBOLS[0]} \u2014 Feature Importance (blue=positive, red=negative weight)\")\n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Walk-forward stability\n",
    "    if len(wf_result[\"fold_results\"]) > 1:\n",
    "        print(\"\\nWalk-Forward Fold Summary:\")\n",
    "        print(f\"{'Fold':>4} {'IS Sharpe':>10} {'OOS Sharpe':>11} {'IS Trades':>10} {'OOS Trades':>11}\")\n",
    "        print(\"-\" * 50)\n",
    "        for fr in wf_result[\"fold_results\"]:\n",
    "            print(f\"{fr['fold']:>4} {fr['is_sharpe']:>10.2f} {fr['oos_sharpe']:>11.2f} \"\n",
    "                  f\"{fr['is_trades']:>10} {fr['oos_trades']:>11}\")\n",
    "else:\n",
    "    print(\"No Optuna results \u2014 feature importance unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba7cb05",
   "metadata": {},
   "source": [
    "<a id=\"29\"></a>\n",
    "## 29. Summary & References\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **SANOS from nfo_1min** extends coverage to 316 days (vs 125 from bhavcopy), giving 2.5\u00d7 more calibration data\n",
    "2. **Option execution** on actual ATM CE/PE prices \u2014 no synthetic approximations\n",
    "3. **Walk-forward validation** provides honest OOS Sharpe separate from in-sample\n",
    "4. **24-feature composite signal** combines fractional calculus, Vedic mathematics, SANOS density, microstructure, and Masters indicators\n",
    "5. **Validation gates** (placebo + time-shift) test for signal robustness\n",
    "\n",
    "### Caveats\n",
    "\n",
    "- **Option liquidity**: ATM index options are liquid, but slippage may be higher than 10 bps in practice\n",
    "- **Walk-forward**: Limited folds (2-3) due to 316-day dataset; longer history would improve OOS reliability\n",
    "- **Optuna overfitting**: 24 features \u00d7 200 trials is still at risk of overfitting; feature selection could reduce dimensionality\n",
    "- **No overnight risk**: Intraday-only trades avoid overnight gap risk but miss swing opportunities\n",
    "\n",
    "### References\n",
    "\n",
    "- Buehler et al. (2026), \"SANOS: Smooth strictly Arbitrage-free Non-parametric Option Surfaces\", arXiv:2601.11209v2\n",
    "- Breeden & Litzenberger (1978), \"Prices of State-Contingent Claims Implicit in Option Prices\", J. Business 51\n",
    "- Ramanujan (1920), \"Mock Theta Functions\" (lost notebook); Andrews & Garvan (2012), Part IV\n",
    "- Madhava of Sangamagrama (~1400), Kerala School of Mathematics\n",
    "- Aryabhata (499), \"Aryabhatiya\" \u2014 Ganitapada verse 12\n",
    "- Hosking (1981), \"Fractional Differencing\", Biometrika 68\n",
    "- Kritzman et al. (2010), \"Absorption Ratio\", J. Financial Analysis\n",
    "- Masters, Timothy (2019), \"Statistically Sound Indicators for Financial Market Prediction\"\n",
    "- Laloux et al. (1999), \"Random Matrices and Financial Correlations\", Int. J. Theor. Appl. Finance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QuantLaxmi venv",
   "language": "python",
   "name": "quantlaxmi"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}