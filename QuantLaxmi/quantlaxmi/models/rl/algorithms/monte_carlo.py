"""Monte Carlo Methods for Reinforcement Learning.

Implements Chapter 11 of "Foundations of RL with Applications in Finance"
by Ashwin Rao & Tikhon Jelvis.

Monte Carlo methods learn value functions and optimal policies from complete
episodes of experience, without requiring a model of the environment.

Key ideas:
- MC Prediction: estimate V^pi by averaging returns G_t observed after visiting
  each state s under policy pi.
- MC Control: use epsilon-greedy exploration over Q(s,a) to converge to the
  optimal policy.
- GLIE (Greedy in the Limit with Infinite Exploration): epsilon decays to 0
  so that the policy converges to the greedy optimum while still exploring
  every (s,a) infinitely often.
- Off-policy MC with Importance Sampling: learn about a target policy pi
  from episodes generated by a behavior policy b.

References:
    Rao & Jelvis, Ch 11 (Monte Carlo Methods)
    Sutton & Barto, Ch 5 (Monte Carlo Methods)
"""
from __future__ import annotations

import logging
from abc import ABC, abstractmethod
from collections import defaultdict
from typing import (
    Any,
    Callable,
    Generic,
    Hashable,
    Iterable,
    Iterator,
    Mapping,
    Optional,
    Sequence,
    TypeVar,
    Union,
)

import numpy as np

# ---------------------------------------------------------------------------
# Core interface imports — graceful fallback to local definitions
# ---------------------------------------------------------------------------
try:
    from quantlaxmi.models.rl.core.markov_process import (
        MarkovDecisionProcess,
        MarkovRewardProcess,
        Policy,
        DeterministicPolicy,
        Distribution,
        Categorical,
        Gaussian,
        State,
        FiniteMarkovDecisionProcess,
    )
    from quantlaxmi.models.rl.core.function_approx import (
        FunctionApprox,
        Tabular,
        LinearFunctionApprox,
        DNNApprox,
        DNNSpec,
    )
    from quantlaxmi.models.rl.core.dynamic_programming import ValueFunction, ActionValueFunction
    from quantlaxmi.models.rl.core.utils import (
        returns,
        episodes_from_mdp,
        epsilon_greedy_policy,
        greedy_policy_from_qvf,
    )
except ImportError:
    pass  # Minimal local definitions below

logger = logging.getLogger(__name__)

S = TypeVar("S", bound=Hashable)
A = TypeVar("A", bound=Hashable)

# =====================================================================
# Minimal local protocol / ABC definitions (used when core is absent)
# =====================================================================


class _FunctionApproxBase(ABC, Generic[S]):
    """Minimal function approximation interface.

    In production this is replaced by ``core.function_approx.FunctionApprox``.
    Supports both tabular and parametric representations.
    """

    @abstractmethod
    def evaluate(self, states: Sequence[S]) -> np.ndarray:
        """Return current value estimates for a batch of states."""

    @abstractmethod
    def update(self, states: Sequence[S], targets: np.ndarray) -> "_FunctionApproxBase[S]":
        """Return a *new* approximation updated toward *targets*."""

    def __call__(self, state: S) -> float:
        return float(self.evaluate([state])[0])


class TabularApprox(_FunctionApproxBase[S]):
    """Simple tabular value function (dict-backed).

    Uses incremental mean or constant step-size update depending on
    ``learning_rate``.

    Parameters
    ----------
    learning_rate : float or None
        If None, uses sample-average (1/n) updates.
        If a float, uses constant step-size alpha.
    default_value : float
        Initial value for unseen states.
    """

    def __init__(
        self,
        learning_rate: Optional[float] = None,
        default_value: float = 0.0,
    ) -> None:
        self._values: dict[S, float] = defaultdict(lambda: default_value)
        self._counts: dict[S, int] = defaultdict(int)
        self._lr = learning_rate
        self._default = default_value

    # -- FunctionApprox interface ------------------------------------------

    def evaluate(self, states: Sequence[S]) -> np.ndarray:
        return np.array([self._values[s] for s in states], dtype=np.float64)

    def update(self, states: Sequence[S], targets: np.ndarray) -> "TabularApprox[S]":  # type: ignore[override]
        new = TabularApprox(learning_rate=self._lr, default_value=self._default)
        new._values = defaultdict(lambda: self._default, self._values)
        new._counts = defaultdict(int, self._counts)
        for s, g in zip(states, targets):
            new._counts[s] += 1
            if self._lr is not None:
                alpha = self._lr
            else:
                alpha = 1.0 / new._counts[s]
            new._values[s] += alpha * (g - new._values[s])
        return new

    def __call__(self, state: S) -> float:
        return self._values[state]

    @property
    def values(self) -> dict[S, float]:
        return dict(self._values)


# Use the real FunctionApprox if available, else fall back
try:
    FunctionApprox  # type: ignore[used-before-def]
except NameError:
    FunctionApprox = _FunctionApproxBase  # type: ignore[misc,assignment]
    Tabular = TabularApprox  # type: ignore[misc,assignment]

# ---------------------------------------------------------------------------
# Utility helpers
# ---------------------------------------------------------------------------


def _compute_returns(rewards: Sequence[float], gamma: float) -> list[float]:
    """Compute discounted returns G_t = sum_{k=0}^{T-t-1} gamma^k * r_{t+k+1}.

    This implements the standard backward accumulation for efficiency:
        G_{T-1} = r_T
        G_t     = r_{t+1} + gamma * G_{t+1}

    Parameters
    ----------
    rewards : sequence of float
        Reward at each time step.  ``rewards[t]`` is the reward received
        *after* visiting state s_t (i.e. r_{t+1} in Sutton-Barto notation,
        but indexed from 0).
    gamma : float
        Discount factor in [0, 1].

    Returns
    -------
    list[float]
        G_t for t = 0, ..., T-1.
    """
    T = len(rewards)
    G: list[float] = [0.0] * T
    running = 0.0
    for t in reversed(range(T)):
        running = rewards[t] + gamma * running
        G[t] = running
    return G


# =====================================================================
# MC Prediction (Ch 11.1)
# =====================================================================

__all__ = [
    "mc_prediction",
    "mc_control",
    "glie_mc_control",
    "importance_sampling_mc",
    "TabularApprox",
]


def mc_prediction(
    mrp_episodes: Iterable[Iterable[tuple[S, float]]],
    gamma: float,
    approx: Any | None = None,
    tolerance: float = 1e-6,
    first_visit: bool = True,
    learning_rate: float | None = None,
) -> Any:
    """Monte Carlo Prediction — estimate V^pi from sampled episodes.

    For each episode, compute
        G_t = sum_{k=0}^{T-t-1} gamma^k * r_{t+k+1}
    then update V(s) toward G_t.

    If ``first_visit=True`` (default), only the *first* occurrence of each
    state in an episode is used.  Otherwise every visit contributes an update
    (every-visit MC).

    Convergence (Theorem 11.1 in Rao & Jelvis):
        Under mild conditions, both first-visit and every-visit MC converge to
        V^pi(s) = E_pi[G_t | S_t = s] as the number of episodes -> infinity.

    Parameters
    ----------
    mrp_episodes : iterable of episodes
        Each episode is an iterable of ``(state, reward)`` pairs.
    gamma : float
        Discount factor.
    approx : FunctionApprox[S] or None
        Value function approximation.  If None, a ``TabularApprox`` is created.
    tolerance : float
        Convergence threshold (max absolute change across all states in a pass).
    first_visit : bool
        Whether to use first-visit (True) or every-visit (False) MC.
    learning_rate : float or None
        Step size for the approximator.  None uses sample-average.

    Returns
    -------
    FunctionApprox[S]
        The learned value function approximation.
    """
    if approx is None:
        approx = TabularApprox(learning_rate=learning_rate)

    for episode in mrp_episodes:
        episode_list = list(episode)
        if not episode_list:
            continue

        states = [s for s, _ in episode_list]
        rewards = [r for _, r in episode_list]
        G = _compute_returns(rewards, gamma)

        if first_visit:
            visited: set[S] = set()
            update_states: list[S] = []
            update_targets: list[float] = []
            for t, s in enumerate(states):
                if s not in visited:
                    visited.add(s)
                    update_states.append(s)
                    update_targets.append(G[t])
        else:
            update_states = states
            update_targets = G

        approx = approx.update(update_states, np.array(update_targets, dtype=np.float64))

    return approx


# =====================================================================
# MC Control with epsilon-greedy (Ch 11.2)
# =====================================================================


def _epsilon_greedy_action(
    q_approx: Any,
    state: S,
    actions: Sequence[A],
    epsilon: float,
    rng: np.random.Generator,
) -> A:
    """Select an action using epsilon-greedy w.r.t. Q(state, .)."""
    if rng.random() < epsilon:
        return actions[rng.integers(len(actions))]
    q_values = np.array(
        [q_approx((state, a)) for a in actions], dtype=np.float64
    )
    # Break ties randomly
    max_q = q_values.max()
    best_actions = [a for a, q in zip(actions, q_values) if np.isclose(q, max_q)]
    return rng.choice(best_actions)  # type: ignore[return-value]


def _generate_episode(
    mdp_step: Callable[[S, A], tuple[S, float, bool]],
    start_state: S,
    actions: Sequence[A],
    q_approx: Any,
    epsilon: float,
    rng: np.random.Generator,
    max_steps: int = 10000,
) -> list[tuple[S, A, float]]:
    """Generate one episode using epsilon-greedy policy derived from Q."""
    episode: list[tuple[S, A, float]] = []
    state = start_state
    for _ in range(max_steps):
        action = _epsilon_greedy_action(q_approx, state, actions, epsilon, rng)
        next_state, reward, done = mdp_step(state, action)
        episode.append((state, action, reward))
        if done:
            break
        state = next_state
    return episode


def mc_control(
    mdp_step: Callable[[S, A], tuple[S, float, bool]],
    start_states: Sequence[S] | Callable[[], S],
    actions: Sequence[A],
    gamma: float,
    approx: Any | None = None,
    num_episodes: int = 10_000,
    epsilon: float = 0.1,
    first_visit: bool = True,
    learning_rate: float | None = None,
    seed: int = 42,
) -> tuple[Any, Callable[[S], A]]:
    """Monte Carlo Control with epsilon-greedy exploration (Ch 11.2).

    Algorithm outline (every episode):
        1. Generate episode using epsilon-greedy policy derived from Q.
        2. For each (s, a) pair in the episode (first-visit or every-visit):
               G_t = discounted return from that point onward
               Q(s, a) <- update toward G_t
        3. The epsilon-greedy policy is implicitly improved because Q improves.

    Convergence (Ch 11.2):
        With exploring starts or epsilon-soft policies, MC control converges
        to the optimal action-value function Q* (and hence optimal policy)
        under standard conditions.

    Parameters
    ----------
    mdp_step : callable (state, action) -> (next_state, reward, done)
        Environment step function.
    start_states : sequence of states or callable returning a start state
        Used to sample the initial state of each episode.
    actions : sequence of A
        The action space (assumed finite and constant).
    gamma : float
        Discount factor.
    approx : FunctionApprox or None
        Q-function approximation.  If None, a ``TabularApprox`` is created.
    num_episodes : int
        Number of episodes to run.
    epsilon : float
        Exploration rate for epsilon-greedy.
    first_visit : bool
        First-visit (True) or every-visit (False) MC.
    learning_rate : float or None
        Step size; None uses sample-average.
    seed : int
        Random seed.

    Returns
    -------
    (q_approx, greedy_policy)
        The learned Q-function and the derived greedy policy function.
    """
    rng = np.random.default_rng(seed)
    if approx is None:
        approx = TabularApprox(learning_rate=learning_rate)

    for ep_idx in range(1, num_episodes + 1):
        # Sample start state
        if callable(start_states) and not isinstance(start_states, (list, tuple)):
            s0 = start_states()
        else:
            s0 = start_states[rng.integers(len(start_states))]  # type: ignore[index]

        episode = _generate_episode(mdp_step, s0, actions, approx, epsilon, rng)
        if not episode:
            continue

        # Compute returns
        rewards = [r for _, _, r in episode]
        G = _compute_returns(rewards, gamma)

        if first_visit:
            visited: set[tuple[S, A]] = set()
            update_keys: list[tuple[S, A]] = []
            update_targets: list[float] = []
            for t, (s, a, _) in enumerate(episode):
                sa = (s, a)
                if sa not in visited:
                    visited.add(sa)
                    update_keys.append(sa)
                    update_targets.append(G[t])
        else:
            update_keys = [(s, a) for s, a, _ in episode]
            update_targets = G

        approx = approx.update(update_keys, np.array(update_targets, dtype=np.float64))

        if ep_idx % max(1, num_episodes // 10) == 0:
            logger.debug("MC Control: episode %d / %d", ep_idx, num_episodes)

    # Derive greedy policy
    def greedy_policy(state: S) -> A:
        q_vals = np.array([approx((state, a)) for a in actions], dtype=np.float64)
        return actions[int(np.argmax(q_vals))]

    return approx, greedy_policy


# =====================================================================
# GLIE MC Control (Ch 11.2)
# =====================================================================


def glie_mc_control(
    mdp_step: Callable[[S, A], tuple[S, float, bool]],
    start_states: Sequence[S] | Callable[[], S],
    actions: Sequence[A],
    gamma: float,
    approx: Any | None = None,
    num_episodes: int = 10_000,
    epsilon_schedule: Callable[[int], float] | None = None,
    learning_rate: float | None = None,
    seed: int = 42,
) -> tuple[Any, Callable[[S], A]]:
    """GLIE (Greedy in the Limit with Infinite Exploration) MC Control.

    Identical to ``mc_control`` except that epsilon decays according to a
    schedule, guaranteeing convergence to the optimal policy.

    GLIE conditions (Def 11.2 in Rao & Jelvis):
        1. All state-action pairs are explored infinitely often.
        2. The policy converges to the greedy policy.

    The default schedule epsilon_k = 1/k satisfies both conditions.

    Theorem 11.3 (Rao & Jelvis):
        GLIE Monte Carlo control converges to the optimal action-value
        function, Q(s,a) -> Q*(s,a) for all s, a.

    Parameters
    ----------
    mdp_step : callable
        Environment step function.
    start_states : sequence or callable
        Start state source.
    actions : sequence
        Finite action space.
    gamma : float
        Discount factor.
    approx : FunctionApprox or None
        Q-function approximation.
    num_episodes : int
        Total episodes.
    epsilon_schedule : callable(int) -> float or None
        Maps episode number k (1-indexed) to epsilon_k.
        Default: 1/k.
    learning_rate : float or None
        Step size.
    seed : int
        Random seed.

    Returns
    -------
    (q_approx, greedy_policy)
    """
    rng = np.random.default_rng(seed)
    if approx is None:
        approx = TabularApprox(learning_rate=learning_rate)
    if epsilon_schedule is None:
        epsilon_schedule = lambda k: 1.0 / k

    for ep_idx in range(1, num_episodes + 1):
        eps = epsilon_schedule(ep_idx)

        if callable(start_states) and not isinstance(start_states, (list, tuple)):
            s0 = start_states()
        else:
            s0 = start_states[rng.integers(len(start_states))]  # type: ignore[index]

        episode = _generate_episode(mdp_step, s0, actions, approx, eps, rng)
        if not episode:
            continue

        rewards = [r for _, _, r in episode]
        G = _compute_returns(rewards, gamma)

        visited: set[tuple[S, A]] = set()
        update_keys: list[tuple[S, A]] = []
        update_targets: list[float] = []
        for t, (s, a, _) in enumerate(episode):
            sa = (s, a)
            if sa not in visited:
                visited.add(sa)
                update_keys.append(sa)
                update_targets.append(G[t])

        approx = approx.update(update_keys, np.array(update_targets, dtype=np.float64))

    def greedy_policy(state: S) -> A:
        q_vals = np.array([approx((state, a)) for a in actions], dtype=np.float64)
        return actions[int(np.argmax(q_vals))]

    return approx, greedy_policy


# =====================================================================
# Off-policy MC with Importance Sampling (Ch 11)
# =====================================================================


def importance_sampling_mc(
    episodes: Iterable[Iterable[tuple[S, A, float]]],
    target_prob: Callable[[S, A], float],
    behavior_prob: Callable[[S, A], float],
    gamma: float,
    approx: Any | None = None,
    weighted: bool = True,
    learning_rate: float | None = None,
) -> Any:
    """Off-policy Monte Carlo with importance sampling (Ch 11).

    Evaluates the value function V^pi for a *target* policy pi using episodes
    generated under a *behavior* policy b.

    The importance sampling ratio for a trajectory from time t to T-1:

        rho_{t:T-1} = prod_{k=t}^{T-1} pi(A_k | S_k) / b(A_k | S_k)

    Ordinary importance sampling (``weighted=False``):
        V(s) = (1/N(s)) * sum rho * G_t

    Weighted importance sampling (``weighted=True``, lower variance):
        V(s) = sum(rho * G_t) / sum(rho)

    Bias-variance trade-off (Ch 11):
        Ordinary IS is unbiased but high variance.
        Weighted IS is biased (bias -> 0) but dramatically lower variance.
        In practice, weighted IS is almost always preferred.

    Parameters
    ----------
    episodes : iterable of episodes
        Each episode is an iterable of ``(state, action, reward)`` triples.
    target_prob : callable (state, action) -> float
        pi(a|s) — target policy probability.
    behavior_prob : callable (state, action) -> float
        b(a|s) — behavior policy probability.
    gamma : float
        Discount factor.
    approx : FunctionApprox or None
        Value function approximation.
    weighted : bool
        If True, use weighted importance sampling (recommended).
    learning_rate : float or None
        Step size.

    Returns
    -------
    FunctionApprox[S]
        Learned value function under the target policy.
    """
    # For weighted IS we need per-state cumulative weights
    cumulative_weights: dict[S, float] = defaultdict(float)
    state_values: dict[S, float] = defaultdict(float)
    state_counts: dict[S, int] = defaultdict(int)

    for episode in episodes:
        episode_list = list(episode)
        if not episode_list:
            continue

        states = [s for s, _, _ in episode_list]
        actions_taken = [a for _, a, _ in episode_list]
        rewards = [r for _, _, r in episode_list]
        T = len(episode_list)

        G = _compute_returns(rewards, gamma)

        # Compute importance sampling ratios incrementally from the end
        # rho_{t:T-1} = prod_{k=t}^{T-1} pi(A_k|S_k) / b(A_k|S_k)
        # We build them backward for efficiency
        rho = np.ones(T, dtype=np.float64)
        running_rho = 1.0
        for k in reversed(range(T)):
            pi_prob = target_prob(states[k], actions_taken[k])
            b_prob = behavior_prob(states[k], actions_taken[k])
            if b_prob < 1e-15:
                # Behavior policy has near-zero probability — skip
                running_rho = 0.0
            else:
                running_rho *= pi_prob / b_prob
            rho[k] = running_rho
            # Reset running product at each step since we want rho_{t:T-1}
        # Actually, need to recompute: rho[t] = prod_{k=t}^{T-1} ...
        for t in range(T):
            r = 1.0
            for k in range(t, T):
                b_prob = behavior_prob(states[k], actions_taken[k])
                if b_prob < 1e-15:
                    r = 0.0
                    break
                r *= target_prob(states[k], actions_taken[k]) / b_prob
            rho[t] = r

        visited: set[S] = set()
        for t in range(T):
            s = states[t]
            if s in visited:
                continue
            visited.add(s)

            if weighted:
                cumulative_weights[s] += rho[t]
                if cumulative_weights[s] > 0:
                    state_values[s] += (
                        rho[t] / cumulative_weights[s]
                    ) * (G[t] - state_values[s])
            else:
                state_counts[s] += 1
                state_values[s] += (1.0 / state_counts[s]) * (
                    rho[t] * G[t] - state_values[s]
                )

    # Build a TabularApprox with the computed values
    result = TabularApprox(learning_rate=learning_rate)
    for s, v in state_values.items():
        result._values[s] = v
    return result
